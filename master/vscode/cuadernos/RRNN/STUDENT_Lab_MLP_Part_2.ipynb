{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aY-Da2gyh2M"
   },
   "source": [
    "# Lab: Regularizing MLPs\n",
    "\n",
    "------------------------------------------------------\n",
    "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "\n",
    "In this lab, you'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world. You can see a sample below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "k_AQa3bSyh2P"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/3200/1*QQVbuP2SEasB0XAmvjW0AA.jpeg\" width=\"400\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url= \"https://miro.medium.com/max/3200/1*QQVbuP2SEasB0XAmvjW0AA.jpeg\", width=400, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaCSXk9gyh2Q"
   },
   "source": [
    "Our goal is to build a neural network that can take one of these images and predict the digit in the image. Unlike the MNIST case, for this problem you will notice that the model **easily overfits**, so addressing this issue is an important problem here. To do so, we will experiment with early stopping and dropout.\n",
    "\n",
    "Note: a big part of the following material is a personal wrap-up of [Facebook's Deep Learning Course in Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188). So all credit goes for them!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Yv92zsRpyh2Q"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  #To get figures with high quality!\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFKYAGMcyh2R"
   },
   "source": [
    "## Part I. Download FMNIST with `torchvision`\n",
    "\n",
    "The code below will download the MNIST dataset, then create training and test datasets for us. It is mostly the same code we used to download MNIST in the previous part of the Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lLA0Ferdyh2R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:21<00:00, 1255597.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\\train-images-idx3-ubyte.gz to C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 902492.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:04<00:00, 993643.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 5163146.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:\\Users\\Horiz/.pytorch/F_MNIST_data/FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R8t5xkTyh2S"
   },
   "source": [
    "Lets take a look to the mini-batch size and plot a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5cEeTRgnyh2S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)   #To iterate through the dataset\n",
    "\n",
    "images, labels = next(dataiter)\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqzjIaQTyh2T"
   },
   "source": [
    "This is what one of the images looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C0wuIFwPyh2T"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f28d180250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 413,
       "width": 416
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().reshape([28,28]), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTJ9Xt9-yh2U"
   },
   "source": [
    "## Part II. Visualize overfiting\n",
    "\n",
    "> **Exercise**: Train a Neural Network with four layers, hidden dimmensions 256, 128 and 64 neurons. Use ReLU activation functions, and a log-Softmax output layer. To do so, complete the following steps: \n",
    "> - Create a class defining the NN model \n",
    "> - Extend the class to incorporate a training method. **Call it trainloop instead of train**. The reason will be clear later.\n",
    "> - Train the model for 30 epochs and evaluate train/test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DaL7UlAJyh2U"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,dimx,hidden1,hidden2,hidden3,nlabels): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.output1 = nn.Linear(dimx,hidden1)\n",
    "        \n",
    "        self.output2 = nn.Linear(hidden1,hidden2)\n",
    "        \n",
    "        self.output3 = nn.Linear(hidden2,hidden3)\n",
    "\n",
    "        self.output4 = nn.Linear(hidden3, nlabels) # Como nos piden 4 capas, añadimos una nueva respecto al MLP de la práctica anterior\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.output1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output4(x)\n",
    "        x = self.logsoftmax(x) # A la salida se le aplica la logsoftmax para la clasificación multiclase\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XZEOZL5ayh2U"
   },
   "outputs": [],
   "source": [
    "class MLP_extended(MLP):\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    def __init__(self,dimx,hidden1,hidden2,hidden3,nlabels,epochs=100,lr=0.001):\n",
    "        \n",
    "        super().__init__(dimx,hidden1,hidden2,hidden3,nlabels)  #To initialize `MLP`!\n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        \n",
    "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()               # NEW w.r.t Lab 1\n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "    def trainloop(self,trainloader):\n",
    "        \n",
    "        # Optimization Loop\n",
    "        \n",
    "        for e in range(int(self.epochs)):\n",
    "            \n",
    "            # Random data permutation at each epoch\n",
    "            \n",
    "            running_loss = 0.\n",
    "            \n",
    "            for images, labels in trainloader:              # NEW w.r.t Lab 1\n",
    "        \n",
    "                self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
    "            \n",
    "                out = self.forward(images.view(images.shape[0], -1))\n",
    "\n",
    "                #Your code here (multiple lines)\n",
    "                \n",
    "                loss = self.criterion(out, labels)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "            \n",
    "            self.loss_during_training.append(running_loss/len(trainloader))\n",
    "\n",
    "            if(e % 1 == 0): # Every 1 epochs\n",
    "\n",
    "                print(\"Training loss after %d epochs: %f\" \n",
    "                      %(e,self.loss_during_training[-1]))\n",
    "    \n",
    "    def acurracy_study(self,loader,modelo):\n",
    "        \n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for images,labels in loader:\n",
    "        \n",
    "                logprobs = modelo.forward(images.view(images.shape[0], -1)) # We use a log-softmax, so what we get are log-probabilities\n",
    "        \n",
    "                top_p, top_class = logprobs.topk(1, dim=1)\n",
    "                equals = (top_class == labels.view(images.shape[0], 1))\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        #print(\"Test Accuracy %f\" %(accuracy/len(loader)))\n",
    "\n",
    "        return ((accuracy/len(loader)).numpy().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "De8794MHyh2V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after 0 epochs: 0.520610\n",
      "Training loss after 1 epochs: 0.377992\n",
      "Training loss after 2 epochs: 0.338552\n",
      "Training loss after 3 epochs: 0.311487\n",
      "Training loss after 4 epochs: 0.294612\n",
      "Training loss after 5 epochs: 0.277065\n",
      "Training loss after 6 epochs: 0.262233\n",
      "Training loss after 7 epochs: 0.251089\n",
      "Training loss after 8 epochs: 0.240219\n",
      "Training loss after 9 epochs: 0.228937\n",
      "Training loss after 10 epochs: 0.220172\n",
      "Training loss after 11 epochs: 0.210175\n",
      "Training loss after 12 epochs: 0.201386\n",
      "Training loss after 13 epochs: 0.194298\n",
      "Training loss after 14 epochs: 0.184970\n",
      "Training loss after 15 epochs: 0.175770\n",
      "Training loss after 16 epochs: 0.171946\n",
      "Training loss after 17 epochs: 0.162351\n",
      "Training loss after 18 epochs: 0.156452\n",
      "Training loss after 19 epochs: 0.152420\n",
      "Training loss after 20 epochs: 0.145211\n",
      "Training loss after 21 epochs: 0.141349\n",
      "Training loss after 22 epochs: 0.133610\n",
      "Training loss after 23 epochs: 0.130822\n",
      "Training loss after 24 epochs: 0.124444\n",
      "Training loss after 25 epochs: 0.122865\n",
      "Training loss after 26 epochs: 0.115854\n",
      "Training loss after 27 epochs: 0.116951\n",
      "Training loss after 28 epochs: 0.110588\n",
      "Training loss after 29 epochs: 0.107656\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "my_MLP = MLP_extended(dimx=784, hidden1=256, hidden2=128, hidden3=64, nlabels=10, epochs=30, lr=0.001 )\n",
    "my_MLP.trainloop(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KIAYu-_yh2V"
   },
   "source": [
    "In light of the train/test performance, certainly the model is performing significantly better in the train set than in the test set. This is a sign of overfitting. For an early detection of overfitting, we will make use of a **validation set** that we will use to visualize the evolution of the loss function during training. \n",
    "\n",
    "With the following code we split the train set into one training set (45k images) and a validation set (15k images). We do a naive splitting assuming that the data is randomized. **Keep in mind that in general you can do something smarter than this like K-Fold cross validation**, but here we keep it simple.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zx577h-dyh2V"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "validloader = copy.deepcopy(trainloader)  # Creates a copy of the object \n",
    "\n",
    "#We take the first 45k images for training\n",
    "trainloader.dataset.data = trainloader.dataset.data[:45000,:,:]\n",
    "trainloader.dataset.targets = trainloader.dataset.targets[:45000]\n",
    "\n",
    "#And the rest for validation\n",
    "validloader.dataset.data = validloader.dataset.data[45000:,:,:]\n",
    "validloader.dataset.targets = validloader.dataset.targets[45000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7LvYiacyh2V"
   },
   "source": [
    "> **Exercise**: Modify your code class above so that, during training, everytime an epoch is finished you compute the loss function over the validation set. You must store these values into a list name as `valid_loss_during_training`. When performing this step, do not forget to turn off gradients by using `with torch.no_grad()`.\n",
    ">\n",
    ">Then, repeat the training (30 epochs) and plot the train/validation loss along epochs. Compute the final train/validation/test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62Tyzz_6yh2W"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWp14rKO2WjE"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzW6xPst2Y8i"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt8UDqPGyh2W"
   },
   "source": [
    "If we look at the training and validation losses as we train the network, we can see a phenomenon known as overfitting. \n",
    "\n",
    "The network learns the training set better and better, resulting in lower training losses. However, it starts having problems generalizing to data outside the training set leading to the validation loss increasing. The ultimate goal of any deep learning model is to make predictions on new data, so we should strive to get the lowest validation loss possible. One option is to use the version of the model with the lowest validation loss, here the one around 8-10 training epochs. This strategy is called *early-stopping*. In practice, you'd save the model frequently as you're training then later choose the model with the lowest validation loss. **Note that with early stopping we are using the validation set to select the appropiate number of epochs.**\n",
    "\n",
    "> **Exercise:** According to your results, re-train the model again for the right number of epochs (just before the validation loss starts to grow). Compare the train, validation and test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXDpsEQvyh2W"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FNn6pJiyh2Z"
   },
   "source": [
    "## Part III. Using Dropout Regularization\n",
    "\n",
    "The most common method to reduce overfitting (outside of early-stopping) is *dropout*, where we randomly drop input units. This forces the network to share information between weights, increasing its ability to generalize to new data. Adding dropout in PyTorch is straightforward using the [`nn.Dropout`](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout) module.\n",
    "\n",
    "The following code corresponds to a 2 layer NN where we use dropout in the intermediate hidden space:\n",
    "\n",
    "\n",
    "```python\n",
    "class MLP_dropout(nn.Module):\n",
    "    def __init__(self,dimx,hidden1,nlabels): #Nlabels will be 10 in our case\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output1 = nn.Linear(dimx,hidden1)  \n",
    "        \n",
    "        self.output2 = nn.Linear(hidden1,nlabels)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)  \n",
    "        \n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.output1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output2(x)\n",
    "        x = self.logsoftmax(x) #YOUR CODE HERE\n",
    "        return x\n",
    "\n",
    "```\n",
    "\n",
    "During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we're using the network to make predictions. To do this, you use `self.eval()`. This sets the model to evaluation mode where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with `self.train()` (**This is why we cannot call our training method `train` anymore**). In general, the pattern for the validation loop will look like this, where you turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode.\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    self.eval()\n",
    "    \n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "\n",
    "# set model back to train mode\n",
    "self.train()\n",
    "```\n",
    "\n",
    "> **Exercise:** \n",
    "> - Create a new NN class that modifies the previous one by incorporating a dropout step with `p=0.2`  after every ReLU non-linearity is applied. \n",
    "> - Modify the extended class to set `model.eval()` when appropiate (do not forget to go back to `model.train()`)\n",
    "> - For this new model, plot the evolution of the training and validation losses. Compare with the case with no dropout. Discuss the results. Is early stopping still required? If so, when you should stop training? Compare the train, validation and test performance.\n",
    "> - Repeat the experiments for a dropout probability of `p=0.1` and `p=0.3`. Which value provides the best validation performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Noc3aaRGyh2a"
   },
   "outputs": [],
   "source": [
    "class MLPdrop(nn.Module):\n",
    "    \n",
    "    #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jr6-R7TFyh2a"
   },
   "outputs": [],
   "source": [
    "class MLPdrop_extended(MLPdrop):\n",
    "    \n",
    "    #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prZlddvpyh2b"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGAS84KFyh2b"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABvyfTgbyh2b"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmKQYauNyh2b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STUDENT_Lab_2_Part_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
