{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1oNpnQH5djzIh8tPM_SMCO2W_6vgtygje","timestamp":1668155406859}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Text Vectorization I**\n","\n","---\n","### Natural Language Processing\n","Date: Nov 11, 2022\n","\n","Authors: Jerónimo Arenas-García (jarenas@ing.uc3m.es), Lorena Calvo-Bartolomé (lcalvo@pa.uc3m.es), Jesús Cid-Suero (jcid@ing.uc3m.es)\n","\n","Version 1.0\n","\n","---\n","\n","Our goal here is to provide a basic overview of the following aspects:\n","\n","\n","*   NLP preprocessing\n","*   Document BoW and TF-IDF representation\n","*   Utilization of the latter to solve a Text Classification task\n","\n","\n"],"metadata":{"id":"iPj3TQkUZihZ"}},{"cell_type":"code","metadata":{"id":"wIEhJkSaWW8Z","executionInfo":{"status":"ok","timestamp":1668181564830,"user_tz":-60,"elapsed":883,"user":{"displayName":"LORENA CALVO BARTOLOME","userId":"01140638486705301902"}}},"source":["# Common imports \n","import os\n","import numpy as np\n","import pandas as pd\n","from termcolor import colored\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Figures plotted inside the notebook\n","%matplotlib inline \n","# High quality figures\n","%config InlineBackend.figure_format = 'retina' \n","# Figures style\n","plt.style.use('seaborn-whitegrid')\n","sns.set_style(\"darkgrid\")\n","sns.color_palette(\"deep\")\n","# Figues size\n","plt.rcParams['figure.figsize'] = [8, 6]"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"s33bt1hoFviC","executionInfo":{"status":"ok","timestamp":1668181567185,"user_tz":-60,"elapsed":377,"user":{"displayName":"LORENA CALVO BARTOLOME","userId":"01140638486705301902"}}},"source":["# To wrap long text lines\n","from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-LnZYfyY9EX","colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"b48a4c19-6fd1-410d-b2d7-368d02748a0d","executionInfo":{"status":"ok","timestamp":1668181576350,"user_tz":-60,"elapsed":225,"user":{"displayName":"LORENA CALVO BARTOLOME","userId":"01140638486705301902"}}},"source":["# For fancy table Display\n","%load_ext google.colab.data_table"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","source":["We are going to save all the files in this notebook generated into our Drive. For doing so, you must fill the variable ``path_to_folder`` with your Drive's folder in which you want to save the files. Please, note that the home path to your Drive is given by:\n","\n","``\n","/content/drive/My Drive/\n","``\n","\n","Hence, if you want to save this lab's results into a folder named ``NLP_IA``, then the variable ``path_to_folder`` for you should look as follows:\n","\n","``\n","path_to_folder = '/content/drive/My Drive/NLP_IA' \n","``"],"metadata":{"id":"_sRa3IcDaH66"}},{"cell_type":"code","source":["path_to_folder = '/content/drive/My Drive/NLP_IA'  # UPDATE THIS ACCORDING TO WHERE YOU WANT TO SAVE THE FILES!!!!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"Q2HyVfyOaAH6","outputId":"1f107eae-d451-4554-ae00-4cf7bcd1eaef","executionInfo":{"status":"ok","timestamp":1668181718323,"user_tz":-60,"elapsed":696,"user":{"displayName":"LORENA CALVO BARTOLOME","userId":"01140638486705301902"}}},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')\n","\n","# Change to assignment directory\n","os.chdir(path_to_folder) "],"metadata":{"id":"oI-dH_8vW4Su","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1668181743347,"user_tz":-60,"elapsed":23895,"user":{"displayName":"LORENA CALVO BARTOLOME","userId":"01140638486705301902"}},"outputId":"df785046-671d-4b56-928f-4856181dab35"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## **1. Data preparation**\n","---\n"],"metadata":{"id":"myiD58wpbymQ"}},{"cell_type":"markdown","source":["### *1.1. Data loading*\n","\n","The first step to start working with text vectorization is downloading the dataset with which we will work. Here we will be using the **IMDB Dataset of 50K Movie Reviews**, which contains 50K movie reviews for natural language processing or Text analytics.\n"],"metadata":{"id":"nZ3x5vfLclE_"}},{"cell_type":"code","source":["try:\n","  import opendatasets as od\n","except ModuleNotFoundError:\n","  %pip install opendatasets\n","  import opendatasets as od"],"metadata":{"id":"P2G-hmU3b24N","colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"status":"ok","timestamp":1668181906683,"user_tz":-60,"elapsed":4769,"user":{"displayName":"LORENA CALVO BARTOLOME","userId":"01140638486705301902"}},"outputId":"a1f86bd0-f1b7-4bda-bbe5-603b3d94872e"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting opendatasets\n","  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.64.1)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (6.1.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2022.9.24)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n","Installing collected packages: opendatasets\n","Successfully installed opendatasets-0.1.22\n"]}]},{"cell_type":"markdown","source":["The IMDB dataset can be downloaded from **Kaggle**. For doing so, you must create an account at [kaggle.com](https://www.kaggle.com/). \n","\n","Once you have your account, go to ``Your profile`` and select ``Edit Public Profile``. If you scroll down in this view, you will see a button named ``Create New API Token``. By clicking it will automatically download a file ``kaggle.json`` containing your **username** and **key**. You will need them for executing the next cell.\n","\n","You just need to execute the following cell once, since it will store the dataset file in the drive folder you have specified above."],"metadata":{"id":"PudNbhBg-uAi"}},{"cell_type":"code","source":["# TODO: Comment this cell after executing it the first time\n","od.download(\"https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"uAkQE-SP5_pK","outputId":"0c1b019f-dfa0-4477-fe59-4f3b4d0863ac","executionInfo":{"status":"ok","timestamp":1668181938863,"user_tz":-60,"elapsed":247,"user":{"displayName":"LORENA CALVO BARTOLOME","userId":"01140638486705301902"}}},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Skipping, found downloaded files in \"./imdb-dataset-of-50k-movie-reviews\" (use force=True to force download)\n"]}]},{"cell_type":"markdown","source":["Let's save the dataset as dataframe.\n","\n","This dataset is oriented toward binary sentence classification, i.e., the prediction of whether each of the reviews in the dataset is positive or negative using either classification or deep learning algorithms.\n","\n","Hence, we have two columns in our dataframe: the ``review`` column contains the textual information and the ``sentiment`` column contains the output labels. Here we will be working first on the ``review`` column for applying distinct types of text vectorization, and finally, we will utilize the ``sentiment`` column to carry out the sentiment analysis task.\n","\n","To accelerate the process, we will be only using a third of the reviews contained in the dataset."],"metadata":{"id":"FSWQtKh4_b4v"}},{"cell_type":"code","source":["corpus_df = pd.read_csv('imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n","corpus_df = corpus_df.sample(frac=0.3, replace=True, random_state=1)\n","print(len(corpus_df))\n","corpus_df.head()"],"metadata":{"id":"dvre6LFf8puM","colab":{"base_uri":"https://localhost:8080/","height":875},"executionInfo":{"status":"ok","timestamp":1668182157268,"user_tz":-60,"elapsed":2255,"user":{"displayName":"LORENA CALVO BARTOLOME","userId":"01140638486705301902"}},"outputId":"eeb31827-8e45-4344-cc86-2ebc88ef0b5d"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["15000\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                  review sentiment\n","33003  Set in Paris in the year 1910, a retired old r...  positive\n","12172  Basic structure of a story: Beginning, Middle,...  negative\n","5192   An odd, willfully skewed biopic of Dyan Thomas...  negative\n","32511  Okay, you have:<br /><br />Penelope Keith as M...  negative\n","43723  The larger-than-life figures of Wyatt Earp and...  positive"],"text/html":["\n","  <div id=\"df-a6db6576-b1cd-4fef-9d65-9538dd5a3329\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>33003</th>\n","      <td>Set in Paris in the year 1910, a retired old r...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>12172</th>\n","      <td>Basic structure of a story: Beginning, Middle,...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>5192</th>\n","      <td>An odd, willfully skewed biopic of Dyan Thomas...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>32511</th>\n","      <td>Okay, you have:&lt;br /&gt;&lt;br /&gt;Penelope Keith as M...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>43723</th>\n","      <td>The larger-than-life figures of Wyatt Earp and...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6db6576-b1cd-4fef-9d65-9538dd5a3329')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a6db6576-b1cd-4fef-9d65-9538dd5a3329 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a6db6576-b1cd-4fef-9d65-9538dd5a3329');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"application/vnd.google.colaboratory.module+javascript":"\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a8bd4d5e58f96183/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 33003,\n            'f': \"33003\",\n        },\n\"Set in Paris in the year 1910, a retired old rich opera singer decides to give her fortune away to her beautiful cat Duchess ( voiced by Eva Gabor) and her kittens, but the jealous butler Edgar comes up with a plan as he kidnaps the cats and leaves them in the countryside. Luckily for them with the help of a streetwise and independent tomcat named Thomas O'Malley ( voiced by Phil Harris) helps them get home especially meeting some of his good friends like the swinging' Scat Cat ( voiced by Scatman Crothers) and try to foil Edgar's plans.<br /><br />Very entertaining and edgy post-Walt Disney's death animated movie with a couple of nice jazzy tunes like the memorable \\\"Everybody wants to be a cat\\\", good voice acting and some terrific animation for it's time even in these times of computer animation. Not one of the greatest Disney animated movies but a cult Disney animated fave and one of the few gems of it's day that works well, highly recommended.\",\n\"positive\"],\n [{\n            'v': 12172,\n            'f': \"12172\",\n        },\n\"Basic structure of a story: Beginning, Middle, End.<br /><br />Sometimes this structure is played with, and we get Memento or Irreversible and the story plays backwards. Sometimes it's just not linear, a la Pulp Fiction. Regardless, they all have a beginning, middle and end.<br /><br />This is the first film I have ever seen that doesn't have an end.<br /><br />Beginning: Girl's best friend is expelled.<br /><br />Middle: Girl needs to cope without best friend.<br /><br />End: Non existent.<br /><br />Not that having an end would've saved this film, but at least it would have been complete.<br /><br />It's an exercise in apathy; we get a party-mix of characters, and they all turn out to be duds. Boring, vain, vapid and pallid imitations of people.<br /><br />And here's the action within this film: NOTHING HAPPENS. Nothing at all happens. Mischa Barton tries to talk with a plummy English accent, Dominique Swain whines a lot and Brad Renfro receives a blow job from some old guy. End of movie.<br /><br />By the time the credits rolled, I had a horrible feeling that many prisoners must feel: periods of time, those precious minutes of our life, have just been wasted.<br /><br />The only passable point (and that is a very emphatic ONLY) is Brad Renfro. He acts well. Lacey Chabert I tend to like, but no luck here. Due to good work in other films, I will forgive Mischa Barton this travesty, but I hope all cast members were slapped in the face for their involvement.<br /><br />Please, I implore you. Avoid. Don't fool yourself into thinking \\\"I'll make up my own mind\\\". My sister told me to never see this, and I ignored her, wanting to make up my own mind. That was a bad decision.<br /><br />I have never hated a film. There are many I don't like, but I have never hated a film. Until I saw this.\",\n\"negative\"],\n [{\n            'v': 5192,\n            'f': \"5192\",\n        },\n\"An odd, willfully skewed biopic of Dyan Thomas in which we hear little more than a dozen lines of his poetry. Instead we have to endure a raw character expos\\u00e9e seen through the prism of his proto-bigamous relationship with wife (Sienna Miller) and childhood love (Keira Knightley). Matthew Rhys plays Thomas with sufficient charm to inoculate us against his otherwise repellent self-interest and Cillian Murphy makes up the persistently tense lovetet.<br /><br />The film never seems to decide on where it's going. There's no arc so much as a viaduct from one end of the war to the other. Maybury seems much more interested in his two female leads (who wouldn't!?) than in the man who brings them together and then divides them. Miller is the choice of the two (I found Knightley competent at best but then I have never found her sympathetic) but they both offer dreadfully inconsistent Welsh accents. Other funny decisions include too much for the inconsequential character of William (Murphy), arty production (eg double crossfades) that is neither impressionist nor symbolic and the old chestnut act of period footage which doesn't blend. 4/10\",\n\"negative\"],\n [{\n            'v': 32511,\n            'f': \"32511\",\n        },\n\"Okay, you have:<br /><br />Penelope Keith as Miss Herringbone-Tweed, B.B.E. (Backbone of England.) She's killed off in the first scene - that's right, folks; this show has no backbone!<br /><br />Peter O'Toole as Ol' Colonel Cricket from The First War and now the emblazered Lord of the Manor.<br /><br />Joanna Lumley as the ensweatered Lady of the Manor, 20 years younger than the colonel and 20 years past her own prime but still glamourous (Brit spelling, not mine) enough to have a toy-boy on the side. It's alright, they have Col. Cricket's full knowledge and consent (they guy even comes 'round for Christmas!) Still, she's considerate of the colonel enough to have said toy-boy her own age (what a gal!)<br /><br />David McCallum as said toy-boy, equally as pointlessly glamourous as his squeeze. Pilcher couldn't come up with any cover for him within the story, so she gave him a hush-hush job at the Circus.<br /><br />and finally:<br /><br />Susan Hampshire as Miss Polonia Teacups, Venerable Headmistress of the Venerable Girls' Boarding-School, serving tea in her office with a dash of deep, poignant advice for life in the outside world just before graduation. Her best bit of advice: \\\"I've only been to Nancherrow (the local Stately Home of England) once. I thought it was very beautiful but, somehow, not part of the real world.\\\" Well, we can't say they didn't warn us.<br /><br />Ah, Susan - time was, your character would have been running the whole show. They don't write 'em like that any more. Our loss, not yours.<br /><br />So - with a cast and setting like this, you have the re-makings of \\\"Brideshead Revisited,\\\" right?<br /><br />Wrong! They took these 1-dimensional supporting roles because they paid so well. After all, acting is one of the oldest temp-jobs there is (YOU name another!)<br /><br />First warning sign: lots and lots of backlighting. They get around it by shooting outdoors - \\\"hey, it's just the sunlight!\\\"<br /><br />Second warning sign: Leading Lady cries a lot. When not crying, her eyes are moist. That's the law of romance novels: Leading Lady is \\\"dewy-eyed.\\\"<br /><br />Henceforth, Leading Lady shall be known as L.L.<br /><br />Third warning sign: L.L. actually has stars in her eyes when she's in love. Still, I'll give Emily Mortimer an award just for having to act with that spotlight in her eyes (I wonder . did they use contacts?)<br /><br />And lastly, fourth warning sign: no on-screen female character is \\\"Mrs.\\\" She's either \\\"Miss\\\" or \\\"Lady.\\\"<br /><br />When all was said and done, I still couldn't tell you who was pursuing whom and why. I couldn't even tell you what was said and done.<br /><br />To sum up: they all live through World War II without anything happening to them at all.<br /><br />OK, at the end, L.L. finds she's lost her parents to the Japanese prison camps and baby sis comes home catatonic. Meanwhile (there's always a \\\"meanwhile,\\\") some young guy L.L. had a crush on (when, I don't know) comes home from some wartime tough spot and is found living on the street by Lady of the Manor (must be some street if SHE's going to find him there.) Both war casualties are whisked away to recover at Nancherrow (SOMEBODY has to be \\\"whisked away\\\" SOMEWHERE in these romance stories!)<br /><br />Great drama.\",\n\"negative\"],\n [{\n            'v': 43723,\n            'f': \"43723\",\n        },\n\"The larger-than-life figures of Wyatt Earp and Bat Masterson, and the specters of George Armstrong Custer and Sitting Bull, loom over director Anthony Mann's hugely entertaining first western with James Stewart. Although Stewart's quest to avenge his father's murder is the primary story, Winchester '73 is really an ensemble piece, with the eponymous, one-in-a-thousand firearm passing through the hands of many colorful owners, including a wry trader (John McIntire, especially great) and outlaw Dan Duryea, who's even more despicable than usual. The film's conflation of fiction and history produces a breezy pace and an ambivalent tone brilliantly in step with Mann's pared-down, compositionally rigorous film-making. His themes of psychological unrest and past dictating present faintly underlie this tall tale of good and bad men chasing after a fabled gun, but they starkly emerge in a vignette about a husband's cowardice and failed attempt at atonement, and are defined in Stewart's conversations with sidekick Millard Mitchell. Mann's use of environment is what sets him apart from other filmmakers of westerns. Instead of gazing at vistas from afar, he incorporates them into the drama as characters that redirect, complicate, or evoke the human characters' goals. Just as mountains, caves, and rapids had to be accounted for in The Naked Spur, here a gunfight occurs amidst the loose rocks and boulders of a small mountain, a physical obstruction that fatalistically determines the roles of victor and victim between two equally skilled sharpshooters. (I would be remiss in not recognizing cinematographer William H. Daniels's contribution, particularly his superlative day-for-night, open-range photography.) Not merely an adept outdoorsman, Mann presents an equally vivid picture of Wyatt Earp-patrolled Dodge City, primarily through scaled, multiple-plane staging. The shooting contest does not depend on brisk camera shifts or twitchy cuts for effect because Mann instinctively knows where to place the camera and how to move it to display the greatest density of information in a given shot. Nor does he care to spell out the plot in dialogue, relying on the actors' eyes or a well-chosen image to convey the stakes. One scene in particular serves to explain his attitude: Mitchell's telling of Stewart's motivation to Shelley Winters, which is interrupted by the climactic gunfight that soon enough reveals all. Light without feeling insubstantial, intense without being overbearing, Winchester '73 seems more modern than its contemporaries and is a joy to behold.\",\n\"positive\"]],\n        columns: [[\"number\", \"index\"], [\"string\", \"review\"], [\"string\", \"sentiment\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["### *1.2. Preprocessing*\n","\n","Before continuing with the vectorization task, we should structure and clean the text so that we only keep the information that allows us to capture the semantic content of the corpus. This will improve the result of our embeddings.\n","\n","For this purpose, we will apply the following three steps, which are typical of any NLP processing task:\n","\n","1.   Text Wrangling\n","2.   Tokenization\n","3.   Homogenization\n","4.   Cleaning\n","\n","For the next steps, we will be using some methods available from:\n","\n","*   [Natural Language Toolkit](https://www.nltk.org/)\n","*   [Beautiful Soup](https://pypi.org/project/beautifulsoup4/)\n","*   [Contractions](https://pypi.org/project/contractions/)\n","*   [re — Regular expression operations](https://docs.python.org/3/library/re.html)"],"metadata":{"id":"8vMI2q90cnLY"}},{"cell_type":"code","source":["import re\n","import nltk\n","\n","def check_nltk_packages():\n","  packages = ['punkt','stopwords','omw-1.4','wordnet']\n","\n","  for package in packages:\n","    try:\n","      nltk.data.find('tokenizers/' + package)\n","    except LookupError:\n","      nltk.download(package)\n","check_nltk_packages()\n","\n","try:\n","  import lxml\n","except ModuleNotFoundError:\n","  %pip install lxml\n","\n","try:\n","  import contractions\n","except ModuleNotFoundError:\n","  %pip install contractions\n","  import contractions\n","\n","from bs4 import BeautifulSoup\n","import re"],"metadata":{"id":"1dq2D5uzfHaR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### *1.2.1. Text Wrangling*\n","\n","If we inspect the reviews, we can see that they contain many HTML tags and some URLs that we do not want to keep for our text vectorization task since they don't add much value for understanding and analyzing the text. \n","\n","Additionally, there are many English contractions ('ll, 're) that we would like to transform into their base form (will, are) to later help with the standardization process."],"metadata":{"id":"7DKEcMr5aMcu"}},{"cell_type":"markdown","source":["##### **Exercise 1**\n","\n","Complete the function ``wrangle_text`` that performs the text wrangling task. For doing so:\n","\n","*   Make use of the library ``BeautifulSoup`` with the parser ``\"lxml\"`` to get rid of all HTML tags.\n","*   Use the function ``re.sub`` to remove all URLs in text. To this function, we need to provide a **regular expression**, i.e., a special sequence of characters that help us match or find other strings or sets of strings, using a specialized syntax held in a pattern, and a string to replace the occurrences of the regular expression found. Typically, we would use define the regular expressions as raw strings in the form r'expression'.\n","You can identify URLs using the pattern ``r'https://\\S+|www\\.\\S+'``.\n","* Use the method ``fix`` from the ``contractions`` library to expand the contractions.\n","\n","Apply the ``wrangle_text`` function into the first positive ``review`` in the corpus and save the result into a variable named ``wrangled_review``. Print the review before and after making the text wrangling.\n"],"metadata":{"id":"RUpFZpQBE9Cl"}},{"cell_type":"code","source":["#<SOL>\n","def wrangle_text(text):\n","  # TODO: Implement\n","#</SOL>\n","\n","print(colored('\\n============= First review in corpus =============', 'blue'))\n","\n","#<SOL>\n","\n","#</SOL>\n","\n","print(colored('\\n============= After wrangling result =============', 'blue'))\n","\n","#<SOL>\n","\n","#</SOL>"],"metadata":{"id":"8s7hUSGrzKVm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### *1.2.2. Tokenization*\n","\n","Tokenization is the process of segmenting a text into words,\n","referred to as **tokens**. This procedure will often also break off punctuation symbols (commas, periods, etc.), phrases, and other possible meaningful elements from the text, such as separate tokens. The list of tokens resulting from tokenization becomes the input for the homogenization stage. \n","\n","The [NLTK Tokenizer Package](https://www.nltk.org/api/nltk.tokenize.html) offers several functions to perform tokenization operations on any text string. Here we will be using the ``wordpunct_tokenize`` function, which allows the separation of punctuation marks. Since sometimes we will be interested in performing the tokenization at the sentence level, we can combine ``wordpunct_tokenize`` and ``sent_tokenize``."],"metadata":{"id":"cL978MLjLTey"}},{"cell_type":"markdown","source":["##### **Exercise 2**\n","\n","* Tokenize the ``wrangled_review`` at the word level using the ``wordpunct_tokenize`` function. Save the tokenized review in a variable named ``review_tokens``.\n","* Tokenize the the ``wrangled_review`` at the sentence level using the combination of ``wordpunct_tokenize`` and  ``sent_tokenize`` functions. Save the tokenized review in a variable named ``review_tokens_sent``."],"metadata":{"id":"vFEwe3wSRCbk"}},{"cell_type":"code","source":["from nltk.tokenize import wordpunct_tokenize\n","from nltk.tokenize import sent_tokenize\n","\n","\n","print(colored('\\n============= First review in corpus =============', 'blue'))\n","print(wrangled_review)\n","\n","#<SOL>\n","\n","#</SOL>\n","\n","print(colored('\\n============= First review (tokens) =============', 'blue'))\n","print(review_tokens)\n","\n","print(colored('\\n============= First review (tokens sent level) =============', 'blue'))\n","print(review_tokens_sent)"],"metadata":{"id":"uuKWO0k8R_g1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### *1.2.3. Homogenization*\n","\n","Homogenization is the process that aims to collapse all semantically equivalent words into a unique representative one. The homogenization process comes from multiple words sharing the same lexeme. For example, ``develop``, ``development``, ``developing``, ``developed``, ``developer``, ``developmental``, and ``developmentally``, are set of words that share the same lexeme or root and, therefore, have a certain relationship of meaning.\n","\n","To homogenize the set of tokens obtained in the previous stage, the following steps need to be performed:\n","\n","\n","1.   **Lower-cased of the tokens**\n","2.   **Elimination of non-alphanumeric characters**, like periods, question marks, and exclamation points.\n","4.   **Word normalization (Stemming/Lemmatization)**, i.e., removing word terminations to preserve the root of the words and ignore grammatical information.\n","\n","Let's see how to apply each of these steps to the previously selected review."],"metadata":{"id":"lEPiUz14Q3Nq"}},{"cell_type":"markdown","source":["##### **Exercise 3**\n","\n","Perform the following two transformations to ``review_tokens``:\n","\n","1.   Convert the tokens to lowercase. Use the ``.lower()`` method.\n","2.   Remove non-alphanumeric tokens. You can detect them using the ``.isalnum()`` method.\n","\n","Save the result in a variable named ``review_tokens_filtered``."],"metadata":{"id":"LTNUuBAYVB_j"}},{"cell_type":"code","source":["#<SOL>\n","\n","#</SOL>\n","\n","print(review_tokens_filtered[0:30])"],"metadata":{"id":"UZkNRe82SeXS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["At this point, we can choose between applying simple stemming or using lemmatization. We will try both to test their differences."],"metadata":{"id":"z8eTKMNJezce"}},{"cell_type":"code","source":["from nltk.stem import SnowballStemmer\n","from nltk.stem import WordNetLemmatizer\n","\n","stemmer = SnowballStemmer('english')\n","wnl = WordNetLemmatizer()\n","\n","stemmed_review = [stemmer.stem(el) for el in review_tokens_filtered]\n","print(colored('\\n============= Stemmed review  =============', 'blue'))\n","print(stemmed_review)\n","\n","lemmatized_review = [wnl.lemmatize(el) for el in review_tokens_filtered]\n","print(colored('\\n============= Lemmatized review  =============', 'blue'))\n","print(lemmatized_review)"],"metadata":{"id":"XL-2IqBugfyp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One of the advantages of the lemmatizer method is that the result of lemmatization is still a true word, which is more advisable for the presentation of text processing results and lemmatization. Yet, it does not remove grammatical differences (e.g., is\" or \"our\" are preserved and not replaced by the infinitive \"be\")\n","\n","In the following, we will use lemmatization."],"metadata":{"id":"zLxnHkJXg9NK"}},{"cell_type":"markdown","source":["#### *1.2.4. Cleaning*\n","\n","The third step consists of removing those words that are very common in language and do not carry out useful semantic content (articles, pronouns, etc.). For it, we will use the list of stopwords provided by NLTK."],"metadata":{"id":"xNenHd3WhQ6H"}},{"cell_type":"markdown","source":["##### **Exercise 4**\n","\n","Clean ``lemmatized_review`` by removing all tokens in the stopwords list ``stopwords_en``. Save the result in a variable named ``clean_review``.\n"],"metadata":{"id":"gM-Vb069mPKW"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stopwords_en = stopwords.words('english')\n","\n","#<SOL>\n","\n","#</SOL>\n","\n","print(colored('\\n============= Lemmatized review  =============', 'blue'))\n","print(lemmatized_review)\n","print(colored('\\n============= Clean lemmatized review  =============', 'blue'))\n","print(clean_review)"],"metadata":{"id":"05ccOVT5hXOD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Exercise 5**\n","\n","Complete the function ``prepare_data`` that performs all steps seen above (i.e., from text wrangling to cleaning). \n","\n","Use the ``apply`` function to perform the transformation into all the ``review`` columns of the ``corpus_df`` dataframe and save the result in a new column named ``clean_review``."],"metadata":{"id":"ZXYrwEkumQ2J"}},{"cell_type":"code","source":["#<SOL>\n","def prepare_data(text):\n","# TODO: Implement\n","#</SOL>"],"metadata":{"id":"48tQOLVAoam4","colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"ffea1308-92b2-436c-fe0a-21e3ceb76160"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","source":["## **2. Basic Vectorization techniques**\n","---\n","\n"],"metadata":{"id":"UABnUm07adtq"}},{"cell_type":"markdown","source":["In the following, we are going to be working with Gensim. \n","\n","Gensim is a Python library intended for NLP practitioners. It provides a variety of methods for working with documents in textual format and carrying out semantic analysis tasks such as topic modeling or semantic comparison between documents. For this reason, Gensim is also widely used in Information Retrieval (IR) tasks.\n","\n","Gensim is Open Source and is entirely programmed in Python, so it is easy to modify the code if necessary. The source code is hosted on the [Github development repository](https://github.com/RaRe-Technologies/gensim\n",").\n","\n","Despite being fully developed in Python, Gensim makes extensive use of the Numpy and Scipy libraries that provide highly efficient implementations of certain matrix transformations and mathematical calculations, so Gensim is quite fast. For this reason, Gensim has been adopted by a large number of companies as a core component of complex NLP systems. Gensim is available for use in the main Cloud Computing platforms (AWS, Azure, Google, etc)."],"metadata":{"id":"HEJxig0v6o_v"}},{"cell_type":"markdown","source":["### *2.1. Gensim corpus*\n","\n","When working with Gensim we need to manage collections of documents. In Gensim, a **document** is simply a list of tokens corresponding to a Python string, while a **corpus** is a collection of documents. The simplest way we can work with a corpus is to create a list of documents (i.e., a list of lists of tokens).\n","\n","```\n","# This is a Gensim document\n","doc = ['Any', 'string', 'you', 'want', 'to', 'work', 'with']\n","\n","# This is a Gensim corpus\n","corpus = [doc, 'A second document just to have more than one'.split()]\n","```"],"metadata":{"id":"EkHtR_dE7Y8-"}},{"cell_type":"markdown","source":["##### **Exercise 6**\n","\n","Generate a corpus to be used in this tutorial. Save it in a variable named ``corpus``."],"metadata":{"id":"cM66pQlr71t1"}},{"cell_type":"code","source":["#<SOL>\n","\n","#</SOL>\n","\n","print(colored('Number of documents in corpus: '+str(len(corpus)), 'green'))\n","print(colored('\\n============= First review =============', 'blue'))\n","print(corpus[0])"],"metadata":{"id":"Wf1RyXdK70x7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Exercise 7**\n","\n","Calculate the average number of tokens per review and plot the histogram of the number of tokens per review."],"metadata":{"id":"iynIQANV9oEx"}},{"cell_type":"code","source":["#<SOL>\n","\n","#</SOL>"],"metadata":{"id":"71-74RBU95DK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *2.2. N-grams detection*\n","\n","Gensim N-gram detection is purely based on the detection of tokens that appear next to each other with high frequency. Gensim `Phraser` can be parameterized to allow some intermediate tokens which are normally considered as links tokens in the English language. However, since we have already carried out lemmatization and stopword removal we can make use of a very simple use of method.\n","\n","Two parameters are necessary:\n","   - `min_count`: Minimum length for N-grams\n","   - `threshold`: Minimum scoring for accepting N-grams. Higher values imply that fewer N-grams are accepted. The threshold is applied on a scoring function that depends on the frequency of the detected N-grams, as well as on the number of isolated occurrences of the component tokens."],"metadata":{"id":"ylvFDqQtSM0h"}},{"cell_type":"code","source":["from gensim.models.phrases import Phrases\n","\n","phrase_model = Phrases(corpus, min_count=2, threshold=20)\n","\n","print(colored('\\n============= First review in corpus =============', 'blue'))\n","print(corpus[0])\n","corpus = [el for el in phrase_model[corpus]] # We populate corpus again\n","print(colored('\\n============= First review after N-gram replacement =============', 'blue'))\n","print(corpus[0])"],"metadata":{"id":"ctWMZLaTSXlE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's save our clean reviews in a text file for later use."],"metadata":{"id":"K-lNvsmz9TGP"}},{"cell_type":"code","source":["corpus_df['clean_review'] = corpus\n","\n","with open(\"imdb_lemmas_clean.txt\", 'w', encoding='utf-8') as fout:\n","  for el in corpus_df['clean_review'].values.tolist():\n","    fout.write(' '.join(el) + '\\n')"],"metadata":{"id":"NU1Yv1Kq9ShB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To be able to work with the corpus, we need to vectorize all its documents. To do so, there are two steps we need to carry out:\n","\n","1. Calculate the dictionary\n","2. Transform the documents using the dictionary"],"metadata":{"id":"r7AKCo62_WzY"}},{"cell_type":"markdown","source":["### *2.3. Gensim dictionary*\n","\n","As a first step for vectorizing documents, we need to create a dictionary containing all tokens in our text corpus and assign an integer identifier to each one of them.\n","\n","The following code fragment generates such a dictionary and shows the first tokens in the dictionary. "],"metadata":{"id":"yqNJKR0a_1vG"}},{"cell_type":"code","source":["from gensim.corpora import Dictionary\n","\n","# Create dictionary of tokens\n","D = Dictionary(corpus)\n","n_tokens = len(D)\n","\n","print('The positive dictionary contains', n_tokens, 'terms')\n","print('First terms in the dictionary:')\n","for n in range(10):\n","    print(str(n), ':', D[n])"],"metadata":{"id":"cubEHlmKAQgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Saved\n","207 words\n","The dictionary object contains several attributes and methods that can be useful for carrying out some cleaning tasks. You can check the available methods using \n","\n","```\n","dir(D)\n","```\n","\n","Some of the most useful methods that we will use are:\n","\n","   - ```add_documents```: updates the dictionary processing new documents\n","\n","   - ```merge_with```: merges two dictionaries\n","\n","   - ```save```, ```save_as_text```, ```load```, ```load_from_text```: can be used to give persistence to the dictionary and reading a previously calculated dictionary\n","\n","   - ```id2token```: This is a Python dictionary for the mapping tokenid (a number) -> token (text representation). You can check that ```D[n]``` is equivalent to ```D.id2token[n]```\n","\n","   - ```token2id```: A Python dictionary for the reverse mapping token -> tokenid\n","\n","   - ```items```, ```keys```, ```values```, ```iteritems```, ```iterkeys```, ```itervalues```: Can be used to obtain al items (tokenid, token), all tokenids, or all token texts, or to iterate over them.\n","\n","   - ```dfs```: A Python dictionary for the mapping tokenid -> Number of documents where the token appears\n","\n","   - ```filter_tokens```, ```filter_extremes```, ```filter_n_most_frequent```: are used to remove elements from the dictionary, and ```compactify```is used to reassign tokenids to tokens for a more efficient representation.\n","\n","   - ```doc2bow```: converts a document into its Bag of Words Representation\n","\n","   - ```doc2idx```: transforms a document into a sequence of the tokenids of the words of the document"],"metadata":{"id":"GjXFSoBaBz36"}},{"cell_type":"markdown","source":["##### **Exercise 8**\n","\n","1. Obtain a dataframe with 2 columns: `token` and `ndocs`, corresponding to the text of each token and the number of documents where the token appears\n","\n","2. Sort the dataframe according to column `ndocs`. \n","\n","3. How many tokens appear in exactly one document? Remove them from the dataframe.\n","\n","4. What are the most and less common tokens in the dictionary in terms of document occurrence?\n","\n","3. Plot a histogram of the number of token appearances"],"metadata":{"id":"F1E2Vq7ZCt21"}},{"cell_type":"code","source":["#<SOL>\n","\n","#</SOL>"],"metadata":{"id":"IlmfLWJsDDik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will filter out terms that appear in too few or too many of the documents in the dataset. This makes sense because:\n","\n","   - terms that appear in most documents are probably not very informative in the general context of a particular corpus\n","   - terms that appear in a very reduced number of documents are not useful to find repetitive patterns across documents. In fact, in many cases, we find that many of the words that are eliminated for this reason can be typos."],"metadata":{"id":"AUU9rKrkDTdL"}},{"cell_type":"code","source":["no_below = 4 #Minimum number of documents to keep a term in the dictionary\n","no_above = .80 #Maximum proportion of documents in which a term can appear to be kept in the dictionary\n","\n","D.filter_extremes(no_below=no_below,no_above=no_above)\n","n_tokens = len(D)\n","\n","print('The dictionary contains', n_tokens, 'terms')"],"metadata":{"id":"C_UcE8geDSuT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can check dictionary size has been considerably reduced with respect to the original vocabulary."],"metadata":{"id":"7_hNAvi8DaNv"}},{"cell_type":"code","source":["corpus_def = []\n","for sent in corpus:\n","  aux = [token for token in sent if token in D.token2id.keys()]\n","  corpus_def.append(aux)\n","\n","corpus_df['clean_review'] = corpus_def"],"metadata":{"id":"xpdMDkV6Qrox"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *2.3. Bag-Of-Words (BoW)*\n","\n","Next, let us create a numerical version of our corpus using the `doc2bow` method. In general, `D.doc2bow(token_list)` transforms any list of tokens into a list of tuples `(token_id, n)`, one per each token in `token_list`, where `token_id` is the token identifier (according to dictionary `D`) and `n` is the number of occurrences of such a token in `token_list`. "],"metadata":{"id":"9rISd3VD_-Yi"}},{"cell_type":"code","source":["reviews_bow = [D.doc2bow(doc) for doc in corpus]\n","\n","n_review = 1000\n","print(colored('============= Review (lemmas) =============', 'blue'))\n","print(' '.join(corpus[n_review]))\n","\n","print(colored('\\n============= Sparse vector representation =============', 'blue'))\n","print(reviews_bow[n_review])\n","\n","print(colored('\\n============= Word counts for the review =============', 'blue'))\n","print(list(map(lambda x: (D[x[0]], x[1]), reviews_bow[n_review])))"],"metadata":{"id":"FJdsQvFbDpht"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that we can interpret each element of corpus_bow as a `sparse_vector`. For example, a list of tuples \n","\n","    [(0, 1), (3, 3), (5,2)] \n","\n","for a dictionary of 10 elements can be represented as a vector, where any tuple `(id, n)` states that position `id` must take value `n`. The rest of the positions must be zero.\n","\n","    [1, 0, 0, 3, 0, 2, 0, 0, 0, 0]\n","\n","As a summary, we have obtained the following variables that will be relevant for the next sections:\n","\n","   * `D`: A Gensim dictionary. Term strings can be accessed using numeric identifiers. For instance, `D[0]` contains the string corresponding to the first position in the BoW representation.\n","   * `mycorpus_bow`: BoW corpus. A list containing an entry per project in the dataset, and consisting of the (sparse) BoW representation for the abstract of that project."],"metadata":{"id":"UQZkdCPiEL1A"}},{"cell_type":"markdown","source":["### *2.4. TF-IDF vectorization*\n","\n","Gensim TFIDF representation of a document is computed as\n","\n","$$x_{ij} = \\text{freq}_{ij} \\log_2 \\frac{\\# docs}{\\# docs_j}$$\n","\n","where: \n","\n","   - $x_{ij}$ is the component of the TFIDF representation of document $i$ corresponding to term $j$\n","   - $\\text{freq}_{ij}$ is the frequency of term $j$ in a document $i$ (i.e., number of occurrences divided by the number of tokens)\n","   - $\\# docs$ is the total number of documents in the corpus\n","   - ${\\# docs_j}$ is the number of documents in the corpus containing term $j$\n","\n","In this way, terms that appear in fewer documents get emphasized over common terms appearing in many documents.\n","\n","Gensim offers the possibility to change the *term frequency* and *inverse document frequency* calculation terms, but we will keep the defaults.\n","\n","Note that, contrary to the Bag of Words (BoW) representation, the TFIDF representation does not depend just on the tokens of each document, but gets affected by the whole corpus through the IDF factor.\n","\n","Gensim considers TFIDF as a model on its own and deals with it similarly to what is done with other models. Creating a TFIDF model is very simple"],"metadata":{"id":"6_RZWSiM_-kD"}},{"cell_type":"code","source":["from gensim.models import TfidfModel\n","\n","tfidf = TfidfModel(reviews_bow)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"5FXpTZ1REXqB","outputId":"5a0cd6f5-17df-421f-8f54-cbdaa2108b66"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","source":["A **TFIDF model cannot be updated** adding more documents. Otherwise, we would lose consistency, i.e., the TFIDF representation for a particular document would change before and after the TFIDF model gets updated."],"metadata":{"id":"iN_o_jSWEeEc"}},{"cell_type":"markdown","source":["From now on, `tfidf` can be used to convert any vector from the old representation (bow integer counts) to the new one (TFIDF real-valued weights), or to apply a transformation to a whole corpus"],"metadata":{"id":"ffXsDP8xEhZd"}},{"cell_type":"code","source":["reviews_tfidf = tfidf[reviews_bow]\n","n_project = 1000\n","print(colored('============= TFIDF representation for the project =============', 'blue'))\n","print(reviews_tfidf[n_review])\n","\n","print(colored('\\n============= TFIDF applying the transformation only to the document =============', 'blue'))\n","print(tfidf[reviews_bow[n_review]])"],"metadata":{"id":"G_Hwm8PfEkU8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As for BOW, TFIDF provides a sparse document representation."],"metadata":{"id":"-WX-jNJpEwLS"}},{"cell_type":"markdown","source":["### *2.5. Memory efficient computation*"],"metadata":{"id":"H87nLNNLE8b6"}},{"cell_type":"markdown","source":["In the previous examples, the construction of the dictionary and the transformation of the corpus to BoW or TFIDF format required that said corpus of documents be available in a list in the execution environment, and therefore required it to be stored in RAM. For a small corpus, this is not a problem, but it can be an important limitation when dealing with a large corpus with millions or tens of millions of documents. These corpora are becoming more and more common in certain applications (consider Wikipedia entries, user opinions on large e-commerce platforms, processing of medical records, etc.).\n","\n","One of the advantages of Gensim is that its implementation makes it easy to work with a corpus of these sizes. As explained in the Gensim documentation:\n","\n","> Note that the corpus above resides fully in memory, as a plain Python list. In this simple example, it doesn’t matter much, but just to make things clear, let’s assume there are millions of documents in the corpus. Storing all of them in RAM won’t do. Instead, let’s assume the documents are stored in a file on disk, one document per line. Gensim only requires that a corpus must be able to return one document vector at a time.\n","\n",">The full power of Gensim comes from the fact that a corpus doesn’t have to be a list, a NumPy array, a Pandas dataframe, or whatever. Gensim accepts any object that, when iterated over, successively yields documents."],"metadata":{"id":"bDw_YwGsGXxV"}},{"cell_type":"markdown","source":["The next fragment of code illustrates how the dictionary can be created from a corpus stored in a text file. You just need to create an iterator that returns a document at each iteration and keeps adding documents to the dictionary. Note that during the execution of the code, only one document is kept in memory at every iteration."],"metadata":{"id":"nhqeH_cTGaUO"}},{"cell_type":"code","source":["class IterableCorpus_fromfile:\n","    def __init__(self, filename):\n","        self.__filename = filename\n","    def __iter__(self):\n","        for line in open(self.__filename):\n","            # assume there's one document per line, tokens separated by whitespace\n","            yield line.lower()\n","\n","MyIterCorpus = IterableCorpus_fromfile('imdb_lemmas_clean.txt')\n","newD = Dictionary()\n","for doc in MyIterCorpus:\n","  newD.add_documents([doc.split()])\n","no_below = 4 # Minimum number of documents to keep a term in the dictionary\n","no_above = .80 # Maximum proportion of documents in which a term can appear to be kept in the dictionary\n","newD.filter_extremes(no_below=no_below,no_above=no_above)"],"metadata":{"id":"vpbGsYThFPs1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The code above can be further simplified if the iterator already carries out the tokenization of each document. In that case, the dictionary can be created with a simple command:"],"metadata":{"id":"yKIzbxUXFU4P"}},{"cell_type":"code","source":["class IterableCorpus_fromfile:\n","    def __init__(self, filename):\n","        self.__filename = filename\n","    def __iter__(self):\n","        for line in open(self.__filename):\n","            # assume there's one document per line, tokens separated by whitespace\n","            yield line.lower().split()\n","\n","MyIterCorpus = IterableCorpus_fromfile('imdb_lemmas_clean.txt')\n","newD = Dictionary(MyIterCorpus)\n","no_below = 4 # Minimum number of documents to keep a term in the dictionary\n","no_above = .80 # Maximum proportion of documents in which a term can appear to be kept in the dictionary\n","newD.filter_extremes(no_below=no_below,no_above=no_above)\n","\n","print('Number of documents processed:', newD.num_docs)\n","print('Number of elements in dictionary:', len(newD))"],"metadata":{"id":"rPFQok6ME8Am"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *2.7. Compatibility with Numpy and Scipy*\n"],"metadata":{"id":"jI5R6rRtGIEA"}},{"cell_type":"markdown","source":["Gensim contains efficient functions to convert Gensim Corpus (BoW, TFIDF) to Numpy dense matrices or Scipy Sparse Matrices. This can be useful, e.g., if we wish to use the vectorial representation of a Gensim corpus to train a classification or regression model using sklearn.\n","\n","Similarly, we also have functions to convert Numpy or Scipy matrices into Gensim representation.\n","\n","More information on the available utilities can be found in the [Gensim API matutils documentation](https://radimrehurek.com/gensim/matutils.html).\n"],"metadata":{"id":"aL6Ztp3ZGT99"}},{"cell_type":"markdown","source":["Sklearn also includes functions for tokenization and vectorization of documents. Specifically, it has the functions:\n","* [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) which implements both tokenization and word count (BoW) in a single class.\n","* [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) which is responsible for obtaining the TF-IDF representation from a BoW representation.\n","\n","* [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) which is equivalent to using `CountVectorizer()` followed by `TfidfTransformer()`."],"metadata":{"id":"HgLdeQAhG4Y9"}},{"cell_type":"markdown","source":["## **3. Sentiment Analysis with BoW and TF-IDF representations**\n","---"],"metadata":{"id":"VoWITECrGhus"}},{"cell_type":"markdown","source":["Let's start by loading the problems labels."],"metadata":{"id":"VnFHjJ6qbZuc"}},{"cell_type":"code","source":["def get_binary_label(sentiment):\n","  return 1 if sentiment == \"positive\" else 0\n","\n","corpus_df['binary_sentiment'] = corpus_df['sentiment'].apply(get_binary_label)\n","\n","Y = corpus_df['binary_sentiment'].values\n","print(Y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wcbywHiDbdu-","outputId":"3c681a18-c386-4e93-c29c-725921e6ba24"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[1 0 0 ... 0 1 1]\n"]}]},{"cell_type":"markdown","source":["And save all the changes we have made in ``corpus_df`` for later use."],"metadata":{"id":"sNCsJPzsj9mc"}},{"cell_type":"code","source":["import pickle\n","\n","def pickler(file: str, ob):\n","    \"\"\"Pickle object to file\"\"\"\n","    with open(file, 'wb') as f:\n","        pickle.dump(ob, f)\n","    return 0\n","\n","pickler(\"corpus_df_imbdb.pickle\",corpus_df)"],"metadata":{"id":"8rWYfb-r_JXH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since we have carried out the vectorization with Gensim,  we have to convert our vector representation into NumPy arrays so we can use Sklearn's classifiers. To do this, Gensim includes two functions: [corpus2dense](https://tedboy.github.io/nlps/generated/generated/gensim.matutils.corpus2dense.html), [corpus2csc](https://tedboy.github.io/nlps/generated/generated/gensim.matutils.corpus2csc.html). In general, when dealing with huge corpora, we will be interested in managing the sparse form of the data to save on computational costs.\n"],"metadata":{"id":"AXBeXFgYcTeE"}},{"cell_type":"code","source":["from gensim.matutils import corpus2dense, corpus2csc\n","\n","n_tokens = len(D)\n","num_docs = len(reviews_bow)\n","\n","# Convert BoW representacion\n","corpus_bow_dense = corpus2dense(reviews_bow, num_terms=n_tokens, num_docs=num_docs).T\n","corpus_bow_sparse = corpus2csc(reviews_bow, num_terms=n_tokens, num_docs=num_docs).T\n","\n","# Convert TFIDF representacion\n","corpus_tfidf_dense = corpus2dense(reviews_tfidf, num_terms=n_tokens, num_docs=num_docs).T\n","corpus_tfidf_sparse = corpus2csc(reviews_tfidf, num_terms=n_tokens, num_docs=num_docs).T"],"metadata":{"id":"vGrhOlDIc7oc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Exercise 9**\n","\n","Train an SVM classifier with the BoW representation of the IMDB dataset. Use the Sklearn function ``train_test_split`` to split the BOW representation of the reviews with a $70/30$ ratio each and a random state of $42$. Find the best hyperparameters for the SVM (``C`` and ``kernel``) via cross-validation with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). Evaluate the performance of the classifier based on the [R2 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)."],"metadata":{"id":"RDkvoVRsmdak"}},{"cell_type":"code","source":["# <SOL>\n","\n","# </SOL>"],"metadata":{"id":"FxRQWMoq-bSq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Exercise 10**\n","\n","Mimic the steps from Exercise 9 to train an SVM classifier with the TF-IDF representation."],"metadata":{"id":"4RN0zvaZMqZL"}},{"cell_type":"code","source":["# <SOL>\n","\n","# </SOL>"],"metadata":{"id":"qwZn_P8rdLOk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","In this first laboratory, we have covered the necessary preprocessing steps that need to be applied to a text corpus previous to its vectorization using several state-of-the-art Python libraries. We have then seen how to obtain BoW and TFIDF representations based on the Gensim library and how to use them for a Sentiment Analysis problem. \n","\n","While we will see that Bag-of-Words and TF-IDF as they neither capture the context of words nor allow for similarity comparison, it is still important to know how they work and how to use them, since they still provide quite good results in some tasks, as we have seen in this notebook. \n","\n","To finish, it is also important that you get confident with the Genism library as you will be using it a lot in this course!\n"],"metadata":{"id":"LriaFb8wbEyP"}}]}