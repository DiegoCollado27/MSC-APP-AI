{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PRÁCTICA 1 - PROCESAMIENTO DE LENGUAJE NATURAL - MASTER EN INTELIGENCIA ARTIFICIAL APLICADA\n",
        "\n",
        "# JOSÉ LORENTE LÓPEZ - DNI: 48842308Z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPj3TQkUZihZ"
      },
      "source": [
        "# **Text Vectorization I**\n",
        "\n",
        "---\n",
        "### Natural Language Processing\n",
        "Date: Nov 11, 2022\n",
        "\n",
        "Authors: Jerónimo Arenas-García (jarenas@ing.uc3m.es), Lorena Calvo-Bartolomé (lcalvo@pa.uc3m.es), Jesús Cid-Suero (jcid@ing.uc3m.es)\n",
        "\n",
        "Version 1.0\n",
        "\n",
        "---\n",
        "\n",
        "Our goal here is to provide a basic overview of the following aspects:\n",
        "\n",
        "\n",
        "*   NLP preprocessing\n",
        "*   Document BoW and TF-IDF representation\n",
        "*   Utilization of the latter to solve a Text Classification task\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Introducimos librerías necesarias para el desarrollo de la práctica\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import contractions\n",
        "from nltk.tokenize import wordpunct_tokenize, sent_tokenize, word_tokenize\n",
        "import colored\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from matplotlib import pyplot as plt\n",
        "from gensim.models.phrases import Phrases\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s33bt1hoFviC"
      },
      "outputs": [],
      "source": [
        "# To wrap long text lines\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sRa3IcDaH66"
      },
      "source": [
        "We are going to save all the files in this notebook generated into our Drive. For doing so, you must fill the variable ``path_to_folder`` with your Drive's folder in which you want to save the files. Please, note that the home path to your Drive is given by:\n",
        "\n",
        "``\n",
        "/content/drive/My Drive/\n",
        "``\n",
        "\n",
        "Hence, if you want to save this lab's results into a folder named ``NLP_IA``, then the variable ``path_to_folder`` for you should look as follows:\n",
        "\n",
        "``\n",
        "path_to_folder = '/content/drive/My Drive/NLP_IA' \n",
        "``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Q2HyVfyOaAH6",
        "outputId": "34643139-5a82-4951-9b6a-23ac75fed388"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Dirección a la que mandaremos la base de datos a utilizar\n",
        "\n",
        "path_to_folder = 'C:/Users/José/Desktop/Master - CIII/1ºCuatrimestre/2ºSemicuatrimestre/Códigos - Python/Procesamiento del Lenguaje Natural/Lab1/data_lab1_1'  # UPDATE THIS ACCORDING TO WHERE YOU WANT TO SAVE THE FILES!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myiD58wpbymQ"
      },
      "source": [
        "## **1. Data preparation**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ3x5vfLclE_"
      },
      "source": [
        "### *1.1. Data loading*\n",
        "\n",
        "The first step to start working with text vectorization is downloading the dataset with which we will work. Here we will be using the **IMDB Dataset of 50K Movie Reviews**, which contains 50K movie reviews for natural language processing or Text analytics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "P2G-hmU3b24N",
        "outputId": "83063a88-b228-4d80-d526-082830876bcb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Importamos (o, en caso de no tenerlo instalamos) una librería asociada a un conjunto de datasets abiertos\n",
        "\n",
        "try:\n",
        "  import opendatasets as od\n",
        "except ModuleNotFoundError:\n",
        "  %pip install opendatasets\n",
        "  import opendatasets as od"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PudNbhBg-uAi"
      },
      "source": [
        "The IMDB dataset can be downloaded from **Kaggle**. For doing so, you must create an account at [kaggle.com](https://www.kaggle.com/). \n",
        "\n",
        "Once you have your account, go to ``Your profile`` and select ``Edit Public Profile``. If you scroll down in this view, you will see a button named ``Create New API Token``. By clicking it will automatically download a file ``kaggle.json`` containing your **username** and **key**. You will need them for executing the next cell.\n",
        "\n",
        "You just need to execute the following cell once, since it will store the dataset file in the drive folder you have specified above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "uAkQE-SP5_pK",
        "outputId": "839a6707-59c0-4dbf-bc06-2be869c6c143"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping, found downloaded files in \".\\imdb-dataset-of-50k-movie-reviews\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "# Descargaremos este dataset que cuenta con 50k reviews de películas donde cada una va a acompañada de una etiqueta (que corresponde al sentimiento transmitido por la review; en este caso la clasificación es binaria sentimiento: positivo o negativo)\n",
        "\n",
        "# TODO: Comment this cell after executing it the first time\n",
        "od.download(\"https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSWQtKh4_b4v"
      },
      "source": [
        "Let's save the dataset as dataframe.\n",
        "\n",
        "This dataset is oriented toward binary sentence classification, i.e., the prediction of whether each of the reviews in the dataset is positive or negative using either classification or deep learning algorithms.\n",
        "\n",
        "Hence, we have two columns in our dataframe: the ``review`` column contains the textual information and the ``sentiment`` column contains the output labels. Here we will be working first on the ``review`` column for applying distinct types of text vectorization, and finally, we will utilize the ``sentiment`` column to carry out the sentiment analysis task.\n",
        "\n",
        "To accelerate the process, we will be only using a third of the reviews contained in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1106
        },
        "id": "dvre6LFf8puM",
        "outputId": "7587bd08-506a-49d1-e480-6ec83e4fc4b3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15000\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33003</th>\n",
              "      <td>Set in Paris in the year 1910, a retired old r...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12172</th>\n",
              "      <td>Basic structure of a story: Beginning, Middle,...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5192</th>\n",
              "      <td>An odd, willfully skewed biopic of Dyan Thomas...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32511</th>\n",
              "      <td>Okay, you have:&lt;br /&gt;&lt;br /&gt;Penelope Keith as M...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43723</th>\n",
              "      <td>The larger-than-life figures of Wyatt Earp and...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "33003  Set in Paris in the year 1910, a retired old r...  positive\n",
              "12172  Basic structure of a story: Beginning, Middle,...  negative\n",
              "5192   An odd, willfully skewed biopic of Dyan Thomas...  negative\n",
              "32511  Okay, you have:<br /><br />Penelope Keith as M...  negative\n",
              "43723  The larger-than-life figures of Wyatt Earp and...  positive"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Introducimos el dataset a un dataframe y nos quedamnos (de forma aleatoria) con un 30% de las muestras. Pasamos de 50k reviews a 15k con el fin de crear un código computacionalmente más óptimo (aunque, claro está, mejor funcionará cuantas más muestras introduzcamos)\n",
        "\n",
        "corpus_df = pd.read_csv('imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
        "corpus_df = corpus_df.sample(frac=0.3, replace=True, random_state=1)\n",
        "print(len(corpus_df))\n",
        "corpus_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vMI2q90cnLY"
      },
      "source": [
        "### *1.2. Preprocessing*\n",
        "\n",
        "Before continuing with the vectorization task, we should structure and clean the text so that we only keep the information that allows us to capture the semantic content of the corpus. This will improve the result of our embeddings.\n",
        "\n",
        "For this purpose, we will apply the following three steps, which are typical of any NLP processing task:\n",
        "\n",
        "1.   Text Wrangling\n",
        "2.   Tokenization\n",
        "3.   Homogenization\n",
        "4.   Cleaning\n",
        "\n",
        "For the next steps, we will be using some methods available from:\n",
        "\n",
        "*   [Natural Language Toolkit](https://www.nltk.org/)\n",
        "*   [Beautiful Soup](https://pypi.org/project/beautifulsoup4/)\n",
        "*   [Contractions](https://pypi.org/project/contractions/)\n",
        "*   [re — Regular expression operations](https://docs.python.org/3/library/re.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de la tearea de vectorización, es fundamental realizar una limpieza del texto con el fin de quedarnos solo con información que proporciona contenido del corpus. Esto mejorará notablemente nuestros resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "1dq2D5uzfHaR",
        "outputId": "7de7fd0a-7ba9-43f2-f4f3-0762d70a053f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\José\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\José\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\José\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Importamos todas las librerías asociadas al preprocesado de los datos del dataset\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "def check_nltk_packages():\n",
        "  packages = ['punkt','stopwords','omw-1.4','wordnet']\n",
        "\n",
        "  for package in packages:\n",
        "    try:\n",
        "      nltk.data.find('tokenizers/' + package)\n",
        "    except LookupError:\n",
        "      nltk.download(package)\n",
        "check_nltk_packages()\n",
        "\n",
        "try:\n",
        "  import lxml\n",
        "except ModuleNotFoundError:\n",
        "  %pip install lxml\n",
        "\n",
        "try:\n",
        "  import contractions\n",
        "except ModuleNotFoundError:\n",
        "  %pip install contractions\n",
        "  import contractions\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DKEcMr5aMcu"
      },
      "source": [
        "#### *1.2.1. Text Wrangling*\n",
        "\n",
        "If we inspect the reviews, we can see that they contain many HTML tags and some URLs that we do not want to keep for our text vectorization task since they don't add much value for understanding and analyzing the text. \n",
        "\n",
        "Additionally, there are many English contractions ('ll, 're) that we would like to transform into their base form (will, are) to later help with the standardization process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"Text wrangling\" consistirá en eliminar tanto las etiquetas HTML, URLs y sustituir contracciones de palabras en inglés por su palabra al completo (ya que sino, para el programa, se tomarán como dos palabras diferentes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUpFZpQBE9Cl"
      },
      "source": [
        "##### **Exercise 1**\n",
        "\n",
        "Complete the function ``wrangle_text`` that performs the text wrangling task. For doing so:\n",
        "\n",
        "*   Make use of the library ``BeautifulSoup`` with the parser ``\"lxml\"`` to get rid of all HTML tags.\n",
        "*   Use the function ``re.sub`` to remove all URLs in text. To this function, we need to provide a **regular expression**, i.e., a special sequence of characters that help us match or find other strings or sets of strings, using a specialized syntax held in a pattern, and a string to replace the occurrences of the regular expression found. Typically, we would use define the regular expressions as raw strings in the form r'expression'.\n",
        "You can identify URLs using the pattern ``r'https://\\S+|www\\.\\S+'``.\n",
        "* Use the method ``fix`` from the ``contractions`` library to expand the contractions.\n",
        "\n",
        "Apply the ``wrangle_text`` function into the first positive ``review`` in the corpus and save the result into a variable named ``wrangled_review``. Print the review before and after making the text wrangling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con wrangle_text eliminaremos las HTML tags, direcciones URL y extenderemos las contracciones de palabras para que no se tomen como palabras diferentes a las no contraidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8s7hUSGrzKVm"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#<SOL>\n",
        "def wrangle_text(texto):\n",
        "  # TODO: Implement\n",
        "#</SOL>\n",
        "\n",
        "  #<SOL>\n",
        "  texto = str(texto)\n",
        "  texto = BeautifulSoup(texto, \"lxml\").text\n",
        "  texto = re.sub(r'http\\S+', '', str(texto))\n",
        "  texto = contractions.fix(texto)\n",
        "  #</SOL>\n",
        "\n",
        "#<SOL>\n",
        "  return texto\n",
        "#</SOL>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pp = corpus_df['review'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\anaconda\\envs\\ids\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "for i in range (15000):\n",
        "    pp[i] = wrangle_text(pp[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "corpus_df['review'] = pp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'Basic structure of a story: Beginning, Middle, End.Sometimes this structure is played with, and we get Memento or Irreversible and the story plays backwards. Sometimes it is just not linear, a la Pulp Fiction. Regardless, they all have a beginning, middle and end.This is the first film I have ever seen that does not have an end.Beginning: Girl\\'s best friend is expelled.Middle: Girl needs to cope without best friend.End: Non existent.Not that having an end would have saved this film, but at least it would have been complete.It is an exercise in apathy; we get a party-mix of characters, and they all turn out to be duds. Boring, vain, vapid and pallid imitations of people.And here is the action within this film: NOTHING HAPPENS. Nothing at all happens. Mischa Barton tries to talk with a plummy English accent, Dominique Swain whines a lot and Brad Renfro receives a blow job from some old guy. End of movie.By the time the credits rolled, I had a horrible feeling that many prisoners must feel: periods of time, those precious minutes of our life, have just been wasted.The only passable point (and that is a very emphatic ONLY) is Brad Renfro. He acts well. Lacey Chabert I tend to like, but no luck here. Due to good work in other films, I will forgive Mischa Barton this travesty, but I hope all cast members were slapped in the face for their involvement.Please, I implore you. Avoid. Do not fool yourself into thinking \"I will make up my own mind\". My sister told me to never see this, and I ignored her, wanting to make up my own mind. That was a bad decision.I have never hated a film. There are many I do not like, but I have never hated a film. Until I saw this.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vemos en un ejemplo como ha funcionado correctamos el uso de esta función\n",
        "pp[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL978MLjLTey"
      },
      "source": [
        "#### *1.2.2. Tokenization*\n",
        "\n",
        "Tokenization is the process of segmenting a text into words,\n",
        "referred to as **tokens**. This procedure will often also break off punctuation symbols (commas, periods, etc.), phrases, and other possible meaningful elements from the text, such as separate tokens. The list of tokens resulting from tokenization becomes the input for the homogenization stage. \n",
        "\n",
        "The [NLTK Tokenizer Package](https://www.nltk.org/api/nltk.tokenize.html) offers several functions to perform tokenization operations on any text string. Here we will be using the ``wordpunct_tokenize`` function, which allows the separation of punctuation marks. Since sometimes we will be interested in performing the tokenization at the sentence level, we can combine ``wordpunct_tokenize`` and ``sent_tokenize``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cada review la separaremos en sus tokens, es decir en las palabras que la componen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFEwe3wSRCbk"
      },
      "source": [
        "##### **Exercise 2**\n",
        "\n",
        "* Tokenize the ``wrangled_review`` at the word level using the ``wordpunct_tokenize`` function. Save the tokenized review in a variable named ``review_tokens``.\n",
        "* Tokenize the the ``wrangled_review`` at the sentence level using the combination of ``wordpunct_tokenize`` and  ``sent_tokenize`` functions. Save the tokenized review in a variable named ``review_tokens_sent``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uuKWO0k8R_g1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def tokenize(texto):\n",
        "#<SOL>\n",
        "    texto = str(texto)\n",
        "    review_tokens = wordpunct_tokenize(texto)\n",
        "    review_tokens_sent = sent_tokenize(texto)\n",
        "    return review_tokens, review_tokens_sent\n",
        "#</SOL>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Veamos si funciona la función haciendo la word/sentece tokenize de una review cualquiera\n",
        "\n",
        "review_tokens = wordpunct_tokenize(pp[1])\n",
        "review_tokens_sent = sent_tokenize(pp[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['Basic',\n",
              " 'structure',\n",
              " 'of',\n",
              " 'a',\n",
              " 'story',\n",
              " ':',\n",
              " 'Beginning',\n",
              " ',',\n",
              " 'Middle',\n",
              " ',',\n",
              " 'End',\n",
              " '.',\n",
              " 'Sometimes',\n",
              " 'this',\n",
              " 'structure',\n",
              " 'is',\n",
              " 'played',\n",
              " 'with',\n",
              " ',',\n",
              " 'and',\n",
              " 'we',\n",
              " 'get',\n",
              " 'Memento',\n",
              " 'or',\n",
              " 'Irreversible',\n",
              " 'and',\n",
              " 'the',\n",
              " 'story',\n",
              " 'plays',\n",
              " 'backwards',\n",
              " '.',\n",
              " 'Sometimes',\n",
              " 'it',\n",
              " 'is',\n",
              " 'just',\n",
              " 'not',\n",
              " 'linear',\n",
              " ',',\n",
              " 'a',\n",
              " 'la',\n",
              " 'Pulp',\n",
              " 'Fiction',\n",
              " '.',\n",
              " 'Regardless',\n",
              " ',',\n",
              " 'they',\n",
              " 'all',\n",
              " 'have',\n",
              " 'a',\n",
              " 'beginning',\n",
              " ',',\n",
              " 'middle',\n",
              " 'and',\n",
              " 'end',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'film',\n",
              " 'I',\n",
              " 'have',\n",
              " 'ever',\n",
              " 'seen',\n",
              " 'that',\n",
              " 'does',\n",
              " 'not',\n",
              " 'have',\n",
              " 'an',\n",
              " 'end',\n",
              " '.',\n",
              " 'Beginning',\n",
              " ':',\n",
              " 'Girl',\n",
              " \"'\",\n",
              " 's',\n",
              " 'best',\n",
              " 'friend',\n",
              " 'is',\n",
              " 'expelled',\n",
              " '.',\n",
              " 'Middle',\n",
              " ':',\n",
              " 'Girl',\n",
              " 'needs',\n",
              " 'to',\n",
              " 'cope',\n",
              " 'without',\n",
              " 'best',\n",
              " 'friend',\n",
              " '.',\n",
              " 'End',\n",
              " ':',\n",
              " 'Non',\n",
              " 'existent',\n",
              " '.',\n",
              " 'Not',\n",
              " 'that',\n",
              " 'having',\n",
              " 'an',\n",
              " 'end',\n",
              " 'would',\n",
              " 'have',\n",
              " 'saved',\n",
              " 'this',\n",
              " 'film',\n",
              " ',',\n",
              " 'but',\n",
              " 'at',\n",
              " 'least',\n",
              " 'it',\n",
              " 'would',\n",
              " 'have',\n",
              " 'been',\n",
              " 'complete',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'an',\n",
              " 'exercise',\n",
              " 'in',\n",
              " 'apathy',\n",
              " ';',\n",
              " 'we',\n",
              " 'get',\n",
              " 'a',\n",
              " 'party',\n",
              " '-',\n",
              " 'mix',\n",
              " 'of',\n",
              " 'characters',\n",
              " ',',\n",
              " 'and',\n",
              " 'they',\n",
              " 'all',\n",
              " 'turn',\n",
              " 'out',\n",
              " 'to',\n",
              " 'be',\n",
              " 'duds',\n",
              " '.',\n",
              " 'Boring',\n",
              " ',',\n",
              " 'vain',\n",
              " ',',\n",
              " 'vapid',\n",
              " 'and',\n",
              " 'pallid',\n",
              " 'imitations',\n",
              " 'of',\n",
              " 'people',\n",
              " '.',\n",
              " 'And',\n",
              " 'here',\n",
              " 'is',\n",
              " 'the',\n",
              " 'action',\n",
              " 'within',\n",
              " 'this',\n",
              " 'film',\n",
              " ':',\n",
              " 'NOTHING',\n",
              " 'HAPPENS',\n",
              " '.',\n",
              " 'Nothing',\n",
              " 'at',\n",
              " 'all',\n",
              " 'happens',\n",
              " '.',\n",
              " 'Mischa',\n",
              " 'Barton',\n",
              " 'tries',\n",
              " 'to',\n",
              " 'talk',\n",
              " 'with',\n",
              " 'a',\n",
              " 'plummy',\n",
              " 'English',\n",
              " 'accent',\n",
              " ',',\n",
              " 'Dominique',\n",
              " 'Swain',\n",
              " 'whines',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'and',\n",
              " 'Brad',\n",
              " 'Renfro',\n",
              " 'receives',\n",
              " 'a',\n",
              " 'blow',\n",
              " 'job',\n",
              " 'from',\n",
              " 'some',\n",
              " 'old',\n",
              " 'guy',\n",
              " '.',\n",
              " 'End',\n",
              " 'of',\n",
              " 'movie',\n",
              " '.',\n",
              " 'By',\n",
              " 'the',\n",
              " 'time',\n",
              " 'the',\n",
              " 'credits',\n",
              " 'rolled',\n",
              " ',',\n",
              " 'I',\n",
              " 'had',\n",
              " 'a',\n",
              " 'horrible',\n",
              " 'feeling',\n",
              " 'that',\n",
              " 'many',\n",
              " 'prisoners',\n",
              " 'must',\n",
              " 'feel',\n",
              " ':',\n",
              " 'periods',\n",
              " 'of',\n",
              " 'time',\n",
              " ',',\n",
              " 'those',\n",
              " 'precious',\n",
              " 'minutes',\n",
              " 'of',\n",
              " 'our',\n",
              " 'life',\n",
              " ',',\n",
              " 'have',\n",
              " 'just',\n",
              " 'been',\n",
              " 'wasted',\n",
              " '.',\n",
              " 'The',\n",
              " 'only',\n",
              " 'passable',\n",
              " 'point',\n",
              " '(',\n",
              " 'and',\n",
              " 'that',\n",
              " 'is',\n",
              " 'a',\n",
              " 'very',\n",
              " 'emphatic',\n",
              " 'ONLY',\n",
              " ')',\n",
              " 'is',\n",
              " 'Brad',\n",
              " 'Renfro',\n",
              " '.',\n",
              " 'He',\n",
              " 'acts',\n",
              " 'well',\n",
              " '.',\n",
              " 'Lacey',\n",
              " 'Chabert',\n",
              " 'I',\n",
              " 'tend',\n",
              " 'to',\n",
              " 'like',\n",
              " ',',\n",
              " 'but',\n",
              " 'no',\n",
              " 'luck',\n",
              " 'here',\n",
              " '.',\n",
              " 'Due',\n",
              " 'to',\n",
              " 'good',\n",
              " 'work',\n",
              " 'in',\n",
              " 'other',\n",
              " 'films',\n",
              " ',',\n",
              " 'I',\n",
              " 'will',\n",
              " 'forgive',\n",
              " 'Mischa',\n",
              " 'Barton',\n",
              " 'this',\n",
              " 'travesty',\n",
              " ',',\n",
              " 'but',\n",
              " 'I',\n",
              " 'hope',\n",
              " 'all',\n",
              " 'cast',\n",
              " 'members',\n",
              " 'were',\n",
              " 'slapped',\n",
              " 'in',\n",
              " 'the',\n",
              " 'face',\n",
              " 'for',\n",
              " 'their',\n",
              " 'involvement',\n",
              " '.',\n",
              " 'Please',\n",
              " ',',\n",
              " 'I',\n",
              " 'implore',\n",
              " 'you',\n",
              " '.',\n",
              " 'Avoid',\n",
              " '.',\n",
              " 'Do',\n",
              " 'not',\n",
              " 'fool',\n",
              " 'yourself',\n",
              " 'into',\n",
              " 'thinking',\n",
              " '\"',\n",
              " 'I',\n",
              " 'will',\n",
              " 'make',\n",
              " 'up',\n",
              " 'my',\n",
              " 'own',\n",
              " 'mind',\n",
              " '\".',\n",
              " 'My',\n",
              " 'sister',\n",
              " 'told',\n",
              " 'me',\n",
              " 'to',\n",
              " 'never',\n",
              " 'see',\n",
              " 'this',\n",
              " ',',\n",
              " 'and',\n",
              " 'I',\n",
              " 'ignored',\n",
              " 'her',\n",
              " ',',\n",
              " 'wanting',\n",
              " 'to',\n",
              " 'make',\n",
              " 'up',\n",
              " 'my',\n",
              " 'own',\n",
              " 'mind',\n",
              " '.',\n",
              " 'That',\n",
              " 'was',\n",
              " 'a',\n",
              " 'bad',\n",
              " 'decision',\n",
              " '.',\n",
              " 'I',\n",
              " 'have',\n",
              " 'never',\n",
              " 'hated',\n",
              " 'a',\n",
              " 'film',\n",
              " '.',\n",
              " 'There',\n",
              " 'are',\n",
              " 'many',\n",
              " 'I',\n",
              " 'do',\n",
              " 'not',\n",
              " 'like',\n",
              " ',',\n",
              " 'but',\n",
              " 'I',\n",
              " 'have',\n",
              " 'never',\n",
              " 'hated',\n",
              " 'a',\n",
              " 'film',\n",
              " '.',\n",
              " 'Until',\n",
              " 'I',\n",
              " 'saw',\n",
              " 'this',\n",
              " '.']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vemos como la tokenizacion por palabras funciona correctamente\n",
        "\n",
        "review_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['Basic structure of a story: Beginning, Middle, End.Sometimes this structure is played with, and we get Memento or Irreversible and the story plays backwards.',\n",
              " 'Sometimes it is just not linear, a la Pulp Fiction.',\n",
              " \"Regardless, they all have a beginning, middle and end.This is the first film I have ever seen that does not have an end.Beginning: Girl's best friend is expelled.Middle: Girl needs to cope without best friend.End: Non existent.Not that having an end would have saved this film, but at least it would have been complete.It is an exercise in apathy; we get a party-mix of characters, and they all turn out to be duds.\",\n",
              " 'Boring, vain, vapid and pallid imitations of people.And here is the action within this film: NOTHING HAPPENS.',\n",
              " 'Nothing at all happens.',\n",
              " 'Mischa Barton tries to talk with a plummy English accent, Dominique Swain whines a lot and Brad Renfro receives a blow job from some old guy.',\n",
              " 'End of movie.By the time the credits rolled, I had a horrible feeling that many prisoners must feel: periods of time, those precious minutes of our life, have just been wasted.The only passable point (and that is a very emphatic ONLY) is Brad Renfro.',\n",
              " 'He acts well.',\n",
              " 'Lacey Chabert I tend to like, but no luck here.',\n",
              " 'Due to good work in other films, I will forgive Mischa Barton this travesty, but I hope all cast members were slapped in the face for their involvement.Please, I implore you.',\n",
              " 'Avoid.',\n",
              " 'Do not fool yourself into thinking \"I will make up my own mind\".',\n",
              " 'My sister told me to never see this, and I ignored her, wanting to make up my own mind.',\n",
              " 'That was a bad decision.I have never hated a film.',\n",
              " 'There are many I do not like, but I have never hated a film.',\n",
              " 'Until I saw this.']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vemos como la tokenizacion por frases funciona correctamente\n",
        "\n",
        "review_tokens_sent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEPiUz14Q3Nq"
      },
      "source": [
        "#### *1.2.3. Homogenization*\n",
        "\n",
        "Homogenization is the process that aims to collapse all semantically equivalent words into a unique representative one. The homogenization process comes from multiple words sharing the same lexeme. For example, ``develop``, ``development``, ``developing``, ``developed``, ``developer``, ``developmental``, and ``developmentally``, are set of words that share the same lexeme or root and, therefore, have a certain relationship of meaning.\n",
        "\n",
        "To homogenize the set of tokens obtained in the previous stage, the following steps need to be performed:\n",
        "\n",
        "\n",
        "1.   **Lower-cased of the tokens**\n",
        "2.   **Elimination of non-alphanumeric characters**, like periods, question marks, and exclamation points.\n",
        "4.   **Word normalization (Stemming/Lemmatization)**, i.e., removing word terminations to preserve the root of the words and ignore grammatical information.\n",
        "\n",
        "Let's see how to apply each of these steps to the previously selected review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCq3LFWtayBR"
      },
      "source": [
        "Buscamos que cada forma verbal, adverbio... todo se resuma en su forma raiz para que no signifiquen cosas diferentes (ya que aportan el mismo significado léxico a la frase). A su vez, pondremos todos los tokens en minúsculas (para que, por ejemplo Junio y junio signifiquen, como debe ser, lo mismo) y eliminaremos todos los carácteres no alfanuméricos (signos de puntuación, @, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTNUuBAYVB_j"
      },
      "source": [
        "##### **Exercise 3**\n",
        "\n",
        "Perform the following two transformations to ``review_tokens``:\n",
        "\n",
        "1.   Convert the tokens to lowercase. Use the ``.lower()`` method.\n",
        "2.   Remove non-alphanumeric tokens. You can detect them using the ``.isalnum()`` method.\n",
        "\n",
        "Save the result in a variable named ``review_tokens_filtered``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UZkNRe82SeXS"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['basic', 'structure', 'of', 'a', 'story', 'beginning', 'middle', 'end', 'sometimes', 'this', 'structure', 'is', 'played', 'with', 'and', 'we', 'get', 'memento', 'or', 'irreversible', 'and', 'the', 'story', 'plays', 'backwards', 'sometimes', 'it', 'is', 'just', 'not']\n"
          ]
        }
      ],
      "source": [
        "#<SOL>\n",
        "\n",
        "# Con la siguiente función pasamos todo a minúsculas y eliminamos los caracteres no alfanuméricos\n",
        "\n",
        "def Homogenization(texto):\n",
        "    \n",
        "    ayuda = []\n",
        "    \n",
        "    for i in range (len(texto)):\n",
        "        ayuda.append(texto[i].lower())\n",
        "\n",
        "    extern = []\n",
        "    review_tokens_filtered = []\n",
        "\n",
        "    for i in range (len(ayuda)):\n",
        "        extern.append(ayuda[i].isalnum())\n",
        "\n",
        "    for j in range (len(extern)):\n",
        "        if(extern[j] == True):\n",
        "            review_tokens_filtered.append(ayuda[j])\n",
        "\n",
        "    return review_tokens_filtered\n",
        "#</SOL>\n",
        "\n",
        "review_tokens_filtered = Homogenization(review_tokens)\n",
        "\n",
        "print(review_tokens_filtered[0:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8eTKMNJezce"
      },
      "source": [
        "At this point, we can choose between applying simple stemming or using lemmatization. We will try both to test their differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XL-2IqBugfyp"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============= Stemmed review  =============\n",
            "['basic', 'structur', 'of', 'a', 'stori', 'begin', 'middl', 'end', 'sometim', 'this', 'structur', 'is', 'play', 'with', 'and', 'we', 'get', 'memento', 'or', 'irrevers', 'and', 'the', 'stori', 'play', 'backward', 'sometim', 'it', 'is', 'just', 'not', 'linear', 'a', 'la', 'pulp', 'fiction', 'regardless', 'they', 'all', 'have', 'a', 'begin', 'middl', 'and', 'end', 'this', 'is', 'the', 'first', 'film', 'i', 'have', 'ever', 'seen', 'that', 'doe', 'not', 'have', 'an', 'end', 'begin', 'girl', 's', 'best', 'friend', 'is', 'expel', 'middl', 'girl', 'need', 'to', 'cope', 'without', 'best', 'friend', 'end', 'non', 'exist', 'not', 'that', 'have', 'an', 'end', 'would', 'have', 'save', 'this', 'film', 'but', 'at', 'least', 'it', 'would', 'have', 'been', 'complet', 'it', 'is', 'an', 'exercis', 'in', 'apathi', 'we', 'get', 'a', 'parti', 'mix', 'of', 'charact', 'and', 'they', 'all', 'turn', 'out', 'to', 'be', 'dud', 'bore', 'vain', 'vapid', 'and', 'pallid', 'imit', 'of', 'peopl', 'and', 'here', 'is', 'the', 'action', 'within', 'this', 'film', 'noth', 'happen', 'noth', 'at', 'all', 'happen', 'mischa', 'barton', 'tri', 'to', 'talk', 'with', 'a', 'plummi', 'english', 'accent', 'dominiqu', 'swain', 'whine', 'a', 'lot', 'and', 'brad', 'renfro', 'receiv', 'a', 'blow', 'job', 'from', 'some', 'old', 'guy', 'end', 'of', 'movi', 'by', 'the', 'time', 'the', 'credit', 'roll', 'i', 'had', 'a', 'horribl', 'feel', 'that', 'mani', 'prison', 'must', 'feel', 'period', 'of', 'time', 'those', 'precious', 'minut', 'of', 'our', 'life', 'have', 'just', 'been', 'wast', 'the', 'onli', 'passabl', 'point', 'and', 'that', 'is', 'a', 'veri', 'emphat', 'onli', 'is', 'brad', 'renfro', 'he', 'act', 'well', 'lacey', 'chabert', 'i', 'tend', 'to', 'like', 'but', 'no', 'luck', 'here', 'due', 'to', 'good', 'work', 'in', 'other', 'film', 'i', 'will', 'forgiv', 'mischa', 'barton', 'this', 'travesti', 'but', 'i', 'hope', 'all', 'cast', 'member', 'were', 'slap', 'in', 'the', 'face', 'for', 'their', 'involv', 'pleas', 'i', 'implor', 'you', 'avoid', 'do', 'not', 'fool', 'yourself', 'into', 'think', 'i', 'will', 'make', 'up', 'my', 'own', 'mind', 'my', 'sister', 'told', 'me', 'to', 'never', 'see', 'this', 'and', 'i', 'ignor', 'her', 'want', 'to', 'make', 'up', 'my', 'own', 'mind', 'that', 'was', 'a', 'bad', 'decis', 'i', 'have', 'never', 'hate', 'a', 'film', 'there', 'are', 'mani', 'i', 'do', 'not', 'like', 'but', 'i', 'have', 'never', 'hate', 'a', 'film', 'until', 'i', 'saw', 'this']\n",
            "\n",
            "============= Lemmatized review  =============\n",
            "['basic', 'structure', 'of', 'a', 'story', 'beginning', 'middle', 'end', 'sometimes', 'this', 'structure', 'is', 'played', 'with', 'and', 'we', 'get', 'memento', 'or', 'irreversible', 'and', 'the', 'story', 'play', 'backwards', 'sometimes', 'it', 'is', 'just', 'not', 'linear', 'a', 'la', 'pulp', 'fiction', 'regardless', 'they', 'all', 'have', 'a', 'beginning', 'middle', 'and', 'end', 'this', 'is', 'the', 'first', 'film', 'i', 'have', 'ever', 'seen', 'that', 'doe', 'not', 'have', 'an', 'end', 'beginning', 'girl', 's', 'best', 'friend', 'is', 'expelled', 'middle', 'girl', 'need', 'to', 'cope', 'without', 'best', 'friend', 'end', 'non', 'existent', 'not', 'that', 'having', 'an', 'end', 'would', 'have', 'saved', 'this', 'film', 'but', 'at', 'least', 'it', 'would', 'have', 'been', 'complete', 'it', 'is', 'an', 'exercise', 'in', 'apathy', 'we', 'get', 'a', 'party', 'mix', 'of', 'character', 'and', 'they', 'all', 'turn', 'out', 'to', 'be', 'dud', 'boring', 'vain', 'vapid', 'and', 'pallid', 'imitation', 'of', 'people', 'and', 'here', 'is', 'the', 'action', 'within', 'this', 'film', 'nothing', 'happens', 'nothing', 'at', 'all', 'happens', 'mischa', 'barton', 'try', 'to', 'talk', 'with', 'a', 'plummy', 'english', 'accent', 'dominique', 'swain', 'whine', 'a', 'lot', 'and', 'brad', 'renfro', 'receives', 'a', 'blow', 'job', 'from', 'some', 'old', 'guy', 'end', 'of', 'movie', 'by', 'the', 'time', 'the', 'credit', 'rolled', 'i', 'had', 'a', 'horrible', 'feeling', 'that', 'many', 'prisoner', 'must', 'feel', 'period', 'of', 'time', 'those', 'precious', 'minute', 'of', 'our', 'life', 'have', 'just', 'been', 'wasted', 'the', 'only', 'passable', 'point', 'and', 'that', 'is', 'a', 'very', 'emphatic', 'only', 'is', 'brad', 'renfro', 'he', 'act', 'well', 'lacey', 'chabert', 'i', 'tend', 'to', 'like', 'but', 'no', 'luck', 'here', 'due', 'to', 'good', 'work', 'in', 'other', 'film', 'i', 'will', 'forgive', 'mischa', 'barton', 'this', 'travesty', 'but', 'i', 'hope', 'all', 'cast', 'member', 'were', 'slapped', 'in', 'the', 'face', 'for', 'their', 'involvement', 'please', 'i', 'implore', 'you', 'avoid', 'do', 'not', 'fool', 'yourself', 'into', 'thinking', 'i', 'will', 'make', 'up', 'my', 'own', 'mind', 'my', 'sister', 'told', 'me', 'to', 'never', 'see', 'this', 'and', 'i', 'ignored', 'her', 'wanting', 'to', 'make', 'up', 'my', 'own', 'mind', 'that', 'wa', 'a', 'bad', 'decision', 'i', 'have', 'never', 'hated', 'a', 'film', 'there', 'are', 'many', 'i', 'do', 'not', 'like', 'but', 'i', 'have', 'never', 'hated', 'a', 'film', 'until', 'i', 'saw', 'this']\n"
          ]
        }
      ],
      "source": [
        "# Realizamos la lemmatization (de dos formas diferentes) y vemos como funciona correctamente\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "stemmed_review = [stemmer.stem(el) for el in review_tokens_filtered]\n",
        "print('\\n============= Stemmed review  =============')\n",
        "print(stemmed_review)\n",
        "\n",
        "lemmatized_review = [wnl.lemmatize(el) for el in review_tokens_filtered]\n",
        "print('\\n============= Lemmatized review  =============')\n",
        "print(lemmatized_review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLxnHkJXg9NK"
      },
      "source": [
        "One of the advantages of the lemmatizer method is that the result of lemmatization is still a true word, which is more advisable for the presentation of text processing results and lemmatization. Yet, it does not remove grammatical differences (e.g., is\" or \"our\" are preserved and not replaced by the infinitive \"be\")\n",
        "\n",
        "In the following, we will use lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNenHd3WhQ6H"
      },
      "source": [
        "#### *1.2.4. Cleaning*\n",
        "\n",
        "The third step consists of removing those words that are very common in language and do not carry out useful semantic content (articles, pronouns, etc.). For it, we will use the list of stopwords provided by NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D97PrbOfbUlu"
      },
      "source": [
        "Eliminamos todo token que no proporcione significado a las frases (artículos, pronombres,...) para ello usaremos la lista de \"stopwords\" en inglés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM-Vb069mPKW"
      },
      "source": [
        "##### **Exercise 4**\n",
        "\n",
        "Clean ``lemmatized_review`` by removing all tokens in the stopwords list ``stopwords_en``. Save the result in a variable named ``clean_review``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "05ccOVT5hXOD"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============= Lemmatized review  =============\n",
            "['basic', 'structure', 'of', 'a', 'story', 'beginning', 'middle', 'end', 'sometimes', 'this', 'structure', 'is', 'played', 'with', 'and', 'we', 'get', 'memento', 'or', 'irreversible', 'and', 'the', 'story', 'play', 'backwards', 'sometimes', 'it', 'is', 'just', 'not', 'linear', 'a', 'la', 'pulp', 'fiction', 'regardless', 'they', 'all', 'have', 'a', 'beginning', 'middle', 'and', 'end', 'this', 'is', 'the', 'first', 'film', 'i', 'have', 'ever', 'seen', 'that', 'doe', 'not', 'have', 'an', 'end', 'beginning', 'girl', 's', 'best', 'friend', 'is', 'expelled', 'middle', 'girl', 'need', 'to', 'cope', 'without', 'best', 'friend', 'end', 'non', 'existent', 'not', 'that', 'having', 'an', 'end', 'would', 'have', 'saved', 'this', 'film', 'but', 'at', 'least', 'it', 'would', 'have', 'been', 'complete', 'it', 'is', 'an', 'exercise', 'in', 'apathy', 'we', 'get', 'a', 'party', 'mix', 'of', 'character', 'and', 'they', 'all', 'turn', 'out', 'to', 'be', 'dud', 'boring', 'vain', 'vapid', 'and', 'pallid', 'imitation', 'of', 'people', 'and', 'here', 'is', 'the', 'action', 'within', 'this', 'film', 'nothing', 'happens', 'nothing', 'at', 'all', 'happens', 'mischa', 'barton', 'try', 'to', 'talk', 'with', 'a', 'plummy', 'english', 'accent', 'dominique', 'swain', 'whine', 'a', 'lot', 'and', 'brad', 'renfro', 'receives', 'a', 'blow', 'job', 'from', 'some', 'old', 'guy', 'end', 'of', 'movie', 'by', 'the', 'time', 'the', 'credit', 'rolled', 'i', 'had', 'a', 'horrible', 'feeling', 'that', 'many', 'prisoner', 'must', 'feel', 'period', 'of', 'time', 'those', 'precious', 'minute', 'of', 'our', 'life', 'have', 'just', 'been', 'wasted', 'the', 'only', 'passable', 'point', 'and', 'that', 'is', 'a', 'very', 'emphatic', 'only', 'is', 'brad', 'renfro', 'he', 'act', 'well', 'lacey', 'chabert', 'i', 'tend', 'to', 'like', 'but', 'no', 'luck', 'here', 'due', 'to', 'good', 'work', 'in', 'other', 'film', 'i', 'will', 'forgive', 'mischa', 'barton', 'this', 'travesty', 'but', 'i', 'hope', 'all', 'cast', 'member', 'were', 'slapped', 'in', 'the', 'face', 'for', 'their', 'involvement', 'please', 'i', 'implore', 'you', 'avoid', 'do', 'not', 'fool', 'yourself', 'into', 'thinking', 'i', 'will', 'make', 'up', 'my', 'own', 'mind', 'my', 'sister', 'told', 'me', 'to', 'never', 'see', 'this', 'and', 'i', 'ignored', 'her', 'wanting', 'to', 'make', 'up', 'my', 'own', 'mind', 'that', 'wa', 'a', 'bad', 'decision', 'i', 'have', 'never', 'hated', 'a', 'film', 'there', 'are', 'many', 'i', 'do', 'not', 'like', 'but', 'i', 'have', 'never', 'hated', 'a', 'film', 'until', 'i', 'saw', 'this']\n",
            "\n",
            "============= Clean lemmatized review  =============\n",
            "['basic', 'structure', 'story', 'beginning', 'middle', 'end', 'sometimes', 'structure', 'played', 'get', 'memento', 'irreversible', 'story', 'play', 'backwards', 'sometimes', 'linear', 'la', 'pulp', 'fiction', 'regardless', 'beginning', 'middle', 'end', 'first', 'film', 'ever', 'seen', 'doe', 'end', 'beginning', 'girl', 'best', 'friend', 'expelled', 'middle', 'girl', 'need', 'cope', 'without', 'best', 'friend', 'end', 'non', 'existent', 'end', 'would', 'saved', 'film', 'least', 'would', 'complete', 'exercise', 'apathy', 'get', 'party', 'mix', 'character', 'turn', 'dud', 'boring', 'vain', 'vapid', 'pallid', 'imitation', 'people', 'action', 'within', 'film', 'nothing', 'happens', 'nothing', 'happens', 'mischa', 'barton', 'try', 'talk', 'plummy', 'english', 'accent', 'dominique', 'swain', 'whine', 'lot', 'brad', 'renfro', 'receives', 'blow', 'job', 'old', 'guy', 'end', 'movie', 'time', 'credit', 'rolled', 'horrible', 'feeling', 'many', 'prisoner', 'must', 'feel', 'period', 'time', 'precious', 'minute', 'life', 'wasted', 'passable', 'point', 'emphatic', 'brad', 'renfro', 'act', 'well', 'lacey', 'chabert', 'tend', 'like', 'luck', 'due', 'good', 'work', 'film', 'forgive', 'mischa', 'barton', 'travesty', 'hope', 'cast', 'member', 'slapped', 'face', 'involvement', 'please', 'implore', 'avoid', 'fool', 'thinking', 'make', 'mind', 'sister', 'told', 'never', 'see', 'ignored', 'wanting', 'make', 'mind', 'wa', 'bad', 'decision', 'never', 'hated', 'film', 'many', 'like', 'never', 'hated', 'film', 'saw']\n"
          ]
        }
      ],
      "source": [
        "def cleaning(texto):\n",
        "\n",
        "\n",
        "    stopwords_en = stopwords.words('english')\n",
        "\n",
        "    #<SOL>\n",
        "\n",
        "    filtered_sentence = []\n",
        "\n",
        "    for i in texto:\n",
        "        if i not in stopwords_en:\n",
        "            filtered_sentence.append(i)\n",
        "\n",
        "    clean_review = filtered_sentence\n",
        "    return clean_review\n",
        "    #</SOL>\n",
        "\n",
        "# Comprobemos que ha funcionado correctamente\n",
        "\n",
        "clean_review = cleaning(lemmatized_review)\n",
        "\n",
        "print('\\n============= Lemmatized review  =============')\n",
        "print(lemmatized_review)\n",
        "print('\\n============= Clean lemmatized review  =============')\n",
        "print(clean_review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXYrwEkumQ2J"
      },
      "source": [
        "##### **Exercise 5**\n",
        "\n",
        "Complete the function ``prepare_data`` that performs all steps seen above (i.e., from text wrangling to cleaning). \n",
        "\n",
        "Use the ``apply`` function to perform the transformation into all the ``review`` columns of the ``corpus_df`` dataframe and save the result in a new column named ``clean_review``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introducimos todo el preprocesado de los datos en una función para realizarlo directamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "48tQOLVAoam4",
        "outputId": "ffea1308-92b2-436c-fe0a-21e3ceb76160"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#<SOL>\n",
        "def prepare_data(text):\n",
        "# TODO: Implement\n",
        "\n",
        "    text = wrangle_text(text)\n",
        "    tokenize_text, tokenize_sentence = tokenize(text)\n",
        "    tokenize_text_filtered = Homogenization(tokenize_text)\n",
        "    lemmatized_review = [wnl.lemmatize(el) for el in tokenize_text_filtered]\n",
        "    clean_review = cleaning(lemmatized_review)\n",
        "    \n",
        "    return clean_review\n",
        "#</SOL>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Cambiamos la columna 'review' de corpus por los mismos datos tras el preprocesado de los mismos\n",
        "\n",
        "pp = corpus_df['review'].values\n",
        "\n",
        "for i in range (15000):\n",
        "    pp[i] = prepare_data(pp[i])\n",
        "\n",
        "corpus_df['review'] = pp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33003</th>\n",
              "      <td>[set, paris, year, 1910, retired, old, rich, o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12172</th>\n",
              "      <td>[basic, structure, story, beginning, middle, e...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5192</th>\n",
              "      <td>[odd, willfully, skewed, biopic, dyan, thomas,...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32511</th>\n",
              "      <td>[okay, penelope, keith, miss, herringbone, twe...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43723</th>\n",
              "      <td>[larger, life, figure, wyatt, earp, bat, maste...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16645</th>\n",
              "      <td>[6, 10, acting, great, good, acting, 4, 10, di...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14615</th>\n",
              "      <td>[perhaps, biggest, waste, production, time, mo...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36865</th>\n",
              "      <td>[hilarious, would, sworn, ed, wood, wrote, ter...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20865</th>\n",
              "      <td>[unsung, quiet, gem, tell, true, story, pow, e...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36511</th>\n",
              "      <td>[spent, many, sleepless, night, watching, 2001...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "33003  [set, paris, year, 1910, retired, old, rich, o...  positive\n",
              "12172  [basic, structure, story, beginning, middle, e...  negative\n",
              "5192   [odd, willfully, skewed, biopic, dyan, thomas,...  negative\n",
              "32511  [okay, penelope, keith, miss, herringbone, twe...  negative\n",
              "43723  [larger, life, figure, wyatt, earp, bat, maste...  positive\n",
              "...                                                  ...       ...\n",
              "16645  [6, 10, acting, great, good, acting, 4, 10, di...  negative\n",
              "14615  [perhaps, biggest, waste, production, time, mo...  negative\n",
              "36865  [hilarious, would, sworn, ed, wood, wrote, ter...  negative\n",
              "20865  [unsung, quiet, gem, tell, true, story, pow, e...  positive\n",
              "36511  [spent, many, sleepless, night, watching, 2001...  positive\n",
              "\n",
              "[15000 rows x 2 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Veamos como ha funcionado correctamente\n",
        "\n",
        "corpus_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['set',\n",
              " 'paris',\n",
              " 'year',\n",
              " '1910',\n",
              " 'retired',\n",
              " 'old',\n",
              " 'rich',\n",
              " 'opera',\n",
              " 'singer',\n",
              " 'decides',\n",
              " 'give',\n",
              " 'fortune',\n",
              " 'away',\n",
              " 'beautiful',\n",
              " 'cat',\n",
              " 'duchess',\n",
              " 'voiced',\n",
              " 'eva',\n",
              " 'gabor',\n",
              " 'kitten',\n",
              " 'jealous',\n",
              " 'butler',\n",
              " 'edgar',\n",
              " 'come',\n",
              " 'plan',\n",
              " 'kidnaps',\n",
              " 'cat',\n",
              " 'leaf',\n",
              " 'countryside',\n",
              " 'luckily',\n",
              " 'help',\n",
              " 'streetwise',\n",
              " 'independent',\n",
              " 'tomcat',\n",
              " 'named',\n",
              " 'thomas',\n",
              " 'malley',\n",
              " 'voiced',\n",
              " 'phil',\n",
              " 'harris',\n",
              " 'help',\n",
              " 'get',\n",
              " 'home',\n",
              " 'especially',\n",
              " 'meeting',\n",
              " 'good',\n",
              " 'friend',\n",
              " 'like',\n",
              " 'swinging',\n",
              " 'scat',\n",
              " 'cat',\n",
              " 'voiced',\n",
              " 'scatman',\n",
              " 'crothers',\n",
              " 'try',\n",
              " 'foil',\n",
              " 'edgar',\n",
              " 'plan',\n",
              " 'entertaining',\n",
              " 'edgy',\n",
              " 'post',\n",
              " 'walt',\n",
              " 'disney',\n",
              " 'death',\n",
              " 'animated',\n",
              " 'movie',\n",
              " 'couple',\n",
              " 'nice',\n",
              " 'jazzy',\n",
              " 'tune',\n",
              " 'like',\n",
              " 'memorable',\n",
              " 'everybody',\n",
              " 'want',\n",
              " 'cat',\n",
              " 'good',\n",
              " 'voice',\n",
              " 'acting',\n",
              " 'terrific',\n",
              " 'animation',\n",
              " 'time',\n",
              " 'even',\n",
              " 'time',\n",
              " 'computer',\n",
              " 'animation',\n",
              " 'one',\n",
              " 'greatest',\n",
              " 'disney',\n",
              " 'animated',\n",
              " 'movie',\n",
              " 'cult',\n",
              " 'disney',\n",
              " 'animated',\n",
              " 'fave',\n",
              " 'one',\n",
              " 'gem',\n",
              " 'day',\n",
              " 'work',\n",
              " 'well',\n",
              " 'highly',\n",
              " 'recommended']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Y como podemos extraer una review cualquiera\n",
        "\n",
        "corpus_df['review'].values[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UABnUm07adtq"
      },
      "source": [
        "## **2. Basic Vectorization techniques**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEJxig0v6o_v"
      },
      "source": [
        "In the following, we are going to be working with Gensim. \n",
        "\n",
        "Gensim is a Python library intended for NLP practitioners. It provides a variety of methods for working with documents in textual format and carrying out semantic analysis tasks such as topic modeling or semantic comparison between documents. For this reason, Gensim is also widely used in Information Retrieval (IR) tasks.\n",
        "\n",
        "Gensim is Open Source and is entirely programmed in Python, so it is easy to modify the code if necessary. The source code is hosted on the [Github development repository](https://github.com/RaRe-Technologies/gensim\n",
        ").\n",
        "\n",
        "Despite being fully developed in Python, Gensim makes extensive use of the Numpy and Scipy libraries that provide highly efficient implementations of certain matrix transformations and mathematical calculations, so Gensim is quite fast. For this reason, Gensim has been adopted by a large number of companies as a core component of complex NLP systems. Gensim is available for use in the main Cloud Computing platforms (AWS, Azure, Google, etc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkHtR_dE7Y8-"
      },
      "source": [
        "### *2.1. Gensim corpus*\n",
        "\n",
        "When working with Gensim we need to manage collections of documents. In Gensim, a **document** is simply a list of tokens corresponding to a Python string, while a **corpus** is a collection of documents. The simplest way we can work with a corpus is to create a list of documents (i.e., a list of lists of tokens).\n",
        "\n",
        "```\n",
        "# This is a Gensim document\n",
        "doc = ['Any', 'string', 'you', 'want', 'to', 'work', 'with']\n",
        "\n",
        "# This is a Gensim corpus\n",
        "corpus = [doc, 'A second document just to have more than one'.split()]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM66pQlr71t1"
      },
      "source": [
        "##### **Exercise 6**\n",
        "\n",
        "Generate a corpus to be used in this tutorial. Save it in a variable named ``corpus``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En Gensim un corpus es una lista de listas, donde cada posición de la lista \"madre\" es un documento (en nuestro caso, los documentos son las reviews en forma de tokens preprocesados).\n",
        "\n",
        "Creamos un corpus dado nuestro dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Wf1RyXdK70x7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents in corpus: 15000\n",
            "\n",
            "============= First review =============\n",
            "['set', 'paris', 'year', '1910', 'retired', 'old', 'rich', 'opera', 'singer', 'decides', 'give', 'fortune', 'away', 'beautiful', 'cat', 'duchess', 'voiced', 'eva', 'gabor', 'kitten', 'jealous', 'butler', 'edgar', 'come', 'plan', 'kidnaps', 'cat', 'leaf', 'countryside', 'luckily', 'help', 'streetwise', 'independent', 'tomcat', 'named', 'thomas', 'malley', 'voiced', 'phil', 'harris', 'help', 'get', 'home', 'especially', 'meeting', 'good', 'friend', 'like', 'swinging', 'scat', 'cat', 'voiced', 'scatman', 'crothers', 'try', 'foil', 'edgar', 'plan', 'entertaining', 'edgy', 'post', 'walt', 'disney', 'death', 'animated', 'movie', 'couple', 'nice', 'jazzy', 'tune', 'like', 'memorable', 'everybody', 'want', 'cat', 'good', 'voice', 'acting', 'terrific', 'animation', 'time', 'even', 'time', 'computer', 'animation', 'one', 'greatest', 'disney', 'animated', 'movie', 'cult', 'disney', 'animated', 'fave', 'one', 'gem', 'day', 'work', 'well', 'highly', 'recommended']\n"
          ]
        }
      ],
      "source": [
        "#<SOL>\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for i in range (15000):\n",
        "    corpus.append(corpus_df['review'].values[i])\n",
        "\n",
        "#</SOL>\n",
        "\n",
        "print('Number of documents in corpus: '+str(len(corpus)))\n",
        "print('\\n============= First review =============')\n",
        "print(corpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iynIQANV9oEx"
      },
      "source": [
        "##### **Exercise 7**\n",
        "\n",
        "Calculate the average number of tokens per review and plot the histogram of the number of tokens per review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Crearemos una serie donde se encuentre el número de tokens por review y haremos un histograma con la distribución general."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "71-74RBU95DK"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#<SOL>\n",
        "\n",
        "average_number_tokens = []\n",
        "\n",
        "for i in range(len(corpus)):\n",
        "    average_number_tokens.append(len(corpus[i]))\n",
        "\n",
        "average_number_tokens_pd = pd.Series(average_number_tokens)\n",
        "\n",
        "average_number_tokens_pd.hist(bins = 100)\n",
        "\n",
        "#</SOL>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos, por lo general, la mayoría de reviews suelen tener unos 100 tokens de media."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[101,\n",
              " 161,\n",
              " 109,\n",
              " 311,\n",
              " 250,\n",
              " 129,\n",
              " 107,\n",
              " 55,\n",
              " 405,\n",
              " 80,\n",
              " 87,\n",
              " 191,\n",
              " 212,\n",
              " 69,\n",
              " 23,\n",
              " 55,\n",
              " 56,\n",
              " 61,\n",
              " 213,\n",
              " 84,\n",
              " 38,\n",
              " 32,\n",
              " 105,\n",
              " 92,\n",
              " 219,\n",
              " 84,\n",
              " 234,\n",
              " 47,\n",
              " 96,\n",
              " 129,\n",
              " 104,\n",
              " 411,\n",
              " 146,\n",
              " 63,\n",
              " 24,\n",
              " 90,\n",
              " 62,\n",
              " 109,\n",
              " 52,\n",
              " 122,\n",
              " 170,\n",
              " 339,\n",
              " 170,\n",
              " 88,\n",
              " 46,\n",
              " 64,\n",
              " 72,\n",
              " 425,\n",
              " 118,\n",
              " 74,\n",
              " 195,\n",
              " 80,\n",
              " 73,\n",
              " 436,\n",
              " 82,\n",
              " 52,\n",
              " 102,\n",
              " 336,\n",
              " 87,\n",
              " 154,\n",
              " 76,\n",
              " 36,\n",
              " 302,\n",
              " 32,\n",
              " 47,\n",
              " 167,\n",
              " 56,\n",
              " 143,\n",
              " 25,\n",
              " 104,\n",
              " 88,\n",
              " 63,\n",
              " 52,\n",
              " 46,\n",
              " 112,\n",
              " 147,\n",
              " 170,\n",
              " 130,\n",
              " 382,\n",
              " 89,\n",
              " 110,\n",
              " 275,\n",
              " 108,\n",
              " 65,\n",
              " 20,\n",
              " 78,\n",
              " 135,\n",
              " 72,\n",
              " 116,\n",
              " 82,\n",
              " 101,\n",
              " 198,\n",
              " 180,\n",
              " 71,\n",
              " 189,\n",
              " 288,\n",
              " 79,\n",
              " 21,\n",
              " 359,\n",
              " 130,\n",
              " 99,\n",
              " 61,\n",
              " 70,\n",
              " 73,\n",
              " 77,\n",
              " 210,\n",
              " 141,\n",
              " 137,\n",
              " 197,\n",
              " 147,\n",
              " 142,\n",
              " 43,\n",
              " 35,\n",
              " 61,\n",
              " 195,\n",
              " 84,\n",
              " 33,\n",
              " 74,\n",
              " 180,\n",
              " 81,\n",
              " 93,\n",
              " 240,\n",
              " 100,\n",
              " 83,\n",
              " 64,\n",
              " 30,\n",
              " 99,\n",
              " 92,\n",
              " 150,\n",
              " 65,\n",
              " 131,\n",
              " 111,\n",
              " 59,\n",
              " 71,\n",
              " 72,\n",
              " 239,\n",
              " 150,\n",
              " 72,\n",
              " 212,\n",
              " 57,\n",
              " 112,\n",
              " 97,\n",
              " 267,\n",
              " 50,\n",
              " 62,\n",
              " 206,\n",
              " 79,\n",
              " 44,\n",
              " 103,\n",
              " 450,\n",
              " 374,\n",
              " 36,\n",
              " 109,\n",
              " 60,\n",
              " 63,\n",
              " 97,\n",
              " 179,\n",
              " 124,\n",
              " 72,\n",
              " 171,\n",
              " 145,\n",
              " 98,\n",
              " 335,\n",
              " 90,\n",
              " 105,\n",
              " 130,\n",
              " 84,\n",
              " 72,\n",
              " 91,\n",
              " 92,\n",
              " 60,\n",
              " 59,\n",
              " 121,\n",
              " 102,\n",
              " 210,\n",
              " 155,\n",
              " 162,\n",
              " 264,\n",
              " 118,\n",
              " 64,\n",
              " 476,\n",
              " 73,\n",
              " 193,\n",
              " 76,\n",
              " 120,\n",
              " 122,\n",
              " 43,\n",
              " 276,\n",
              " 242,\n",
              " 106,\n",
              " 129,\n",
              " 161,\n",
              " 126,\n",
              " 127,\n",
              " 172,\n",
              " 62,\n",
              " 25,\n",
              " 108,\n",
              " 58,\n",
              " 169,\n",
              " 155,\n",
              " 170,\n",
              " 86,\n",
              " 322,\n",
              " 243,\n",
              " 74,\n",
              " 367,\n",
              " 63,\n",
              " 107,\n",
              " 142,\n",
              " 78,\n",
              " 66,\n",
              " 162,\n",
              " 377,\n",
              " 272,\n",
              " 83,\n",
              " 61,\n",
              " 254,\n",
              " 73,\n",
              " 63,\n",
              " 114,\n",
              " 477,\n",
              " 528,\n",
              " 35,\n",
              " 27,\n",
              " 87,\n",
              " 61,\n",
              " 22,\n",
              " 67,\n",
              " 174,\n",
              " 146,\n",
              " 220,\n",
              " 85,\n",
              " 107,\n",
              " 125,\n",
              " 70,\n",
              " 67,\n",
              " 65,\n",
              " 81,\n",
              " 68,\n",
              " 57,\n",
              " 297,\n",
              " 115,\n",
              " 21,\n",
              " 68,\n",
              " 185,\n",
              " 66,\n",
              " 30,\n",
              " 123,\n",
              " 288,\n",
              " 58,\n",
              " 63,\n",
              " 98,\n",
              " 70,\n",
              " 116,\n",
              " 101,\n",
              " 123,\n",
              " 83,\n",
              " 82,\n",
              " 74,\n",
              " 110,\n",
              " 78,\n",
              " 62,\n",
              " 32,\n",
              " 111,\n",
              " 68,\n",
              " 184,\n",
              " 73,\n",
              " 59,\n",
              " 73,\n",
              " 72,\n",
              " 62,\n",
              " 64,\n",
              " 73,\n",
              " 29,\n",
              " 22,\n",
              " 155,\n",
              " 182,\n",
              " 63,\n",
              " 212,\n",
              " 96,\n",
              " 143,\n",
              " 75,\n",
              " 85,\n",
              " 44,\n",
              " 38,\n",
              " 100,\n",
              " 412,\n",
              " 95,\n",
              " 38,\n",
              " 151,\n",
              " 57,\n",
              " 242,\n",
              " 89,\n",
              " 107,\n",
              " 65,\n",
              " 96,\n",
              " 178,\n",
              " 197,\n",
              " 121,\n",
              " 140,\n",
              " 142,\n",
              " 110,\n",
              " 64,\n",
              " 91,\n",
              " 343,\n",
              " 130,\n",
              " 79,\n",
              " 190,\n",
              " 57,\n",
              " 84,\n",
              " 87,\n",
              " 71,\n",
              " 54,\n",
              " 61,\n",
              " 54,\n",
              " 80,\n",
              " 67,\n",
              " 303,\n",
              " 168,\n",
              " 71,\n",
              " 62,\n",
              " 69,\n",
              " 98,\n",
              " 98,\n",
              " 138,\n",
              " 106,\n",
              " 112,\n",
              " 63,\n",
              " 77,\n",
              " 137,\n",
              " 128,\n",
              " 155,\n",
              " 255,\n",
              " 62,\n",
              " 320,\n",
              " 151,\n",
              " 115,\n",
              " 60,\n",
              " 117,\n",
              " 97,\n",
              " 46,\n",
              " 92,\n",
              " 104,\n",
              " 67,\n",
              " 191,\n",
              " 223,\n",
              " 157,\n",
              " 30,\n",
              " 67,\n",
              " 308,\n",
              " 75,\n",
              " 94,\n",
              " 40,\n",
              " 66,\n",
              " 74,\n",
              " 76,\n",
              " 63,\n",
              " 65,\n",
              " 245,\n",
              " 58,\n",
              " 186,\n",
              " 109,\n",
              " 129,\n",
              " 75,\n",
              " 65,\n",
              " 228,\n",
              " 380,\n",
              " 29,\n",
              " 122,\n",
              " 77,\n",
              " 353,\n",
              " 366,\n",
              " 61,\n",
              " 135,\n",
              " 81,\n",
              " 31,\n",
              " 18,\n",
              " 150,\n",
              " 87,\n",
              " 88,\n",
              " 57,\n",
              " 82,\n",
              " 192,\n",
              " 49,\n",
              " 91,\n",
              " 129,\n",
              " 47,\n",
              " 472,\n",
              " 157,\n",
              " 84,\n",
              " 275,\n",
              " 185,\n",
              " 66,\n",
              " 69,\n",
              " 79,\n",
              " 75,\n",
              " 59,\n",
              " 370,\n",
              " 72,\n",
              " 155,\n",
              " 98,\n",
              " 82,\n",
              " 92,\n",
              " 290,\n",
              " 71,\n",
              " 71,\n",
              " 142,\n",
              " 118,\n",
              " 38,\n",
              " 40,\n",
              " 102,\n",
              " 206,\n",
              " 264,\n",
              " 72,\n",
              " 71,\n",
              " 88,\n",
              " 64,\n",
              " 87,\n",
              " 335,\n",
              " 125,\n",
              " 69,\n",
              " 200,\n",
              " 84,\n",
              " 64,\n",
              " 71,\n",
              " 100,\n",
              " 73,\n",
              " 155,\n",
              " 111,\n",
              " 157,\n",
              " 189,\n",
              " 30,\n",
              " 285,\n",
              " 47,\n",
              " 53,\n",
              " 56,\n",
              " 93,\n",
              " 78,\n",
              " 114,\n",
              " 154,\n",
              " 32,\n",
              " 472,\n",
              " 147,\n",
              " 69,\n",
              " 409,\n",
              " 30,\n",
              " 182,\n",
              " 81,\n",
              " 111,\n",
              " 63,\n",
              " 73,\n",
              " 94,\n",
              " 74,\n",
              " 86,\n",
              " 110,\n",
              " 117,\n",
              " 23,\n",
              " 91,\n",
              " 94,\n",
              " 93,\n",
              " 72,\n",
              " 74,\n",
              " 518,\n",
              " 80,\n",
              " 109,\n",
              " 80,\n",
              " 139,\n",
              " 86,\n",
              " 14,\n",
              " 79,\n",
              " 56,\n",
              " 95,\n",
              " 51,\n",
              " 181,\n",
              " 98,\n",
              " 23,\n",
              " 145,\n",
              " 84,\n",
              " 136,\n",
              " 155,\n",
              " 397,\n",
              " 67,\n",
              " 228,\n",
              " 174,\n",
              " 120,\n",
              " 69,\n",
              " 21,\n",
              " 152,\n",
              " 120,\n",
              " 56,\n",
              " 208,\n",
              " 50,\n",
              " 136,\n",
              " 63,\n",
              " 244,\n",
              " 68,\n",
              " 104,\n",
              " 304,\n",
              " 96,\n",
              " 123,\n",
              " 42,\n",
              " 139,\n",
              " 194,\n",
              " 277,\n",
              " 61,\n",
              " 61,\n",
              " 89,\n",
              " 72,\n",
              " 70,\n",
              " 84,\n",
              " 151,\n",
              " 210,\n",
              " 75,\n",
              " 245,\n",
              " 69,\n",
              " 450,\n",
              " 52,\n",
              " 64,\n",
              " 62,\n",
              " 63,\n",
              " 115,\n",
              " 149,\n",
              " 62,\n",
              " 66,\n",
              " 128,\n",
              " 266,\n",
              " 54,\n",
              " 43,\n",
              " 139,\n",
              " 45,\n",
              " 109,\n",
              " 79,\n",
              " 59,\n",
              " 41,\n",
              " 312,\n",
              " 268,\n",
              " 212,\n",
              " 93,\n",
              " 69,\n",
              " 55,\n",
              " 179,\n",
              " 87,\n",
              " 295,\n",
              " 74,\n",
              " 74,\n",
              " 92,\n",
              " 243,\n",
              " 110,\n",
              " 96,\n",
              " 76,\n",
              " 113,\n",
              " 27,\n",
              " 137,\n",
              " 70,\n",
              " 112,\n",
              " 204,\n",
              " 59,\n",
              " 59,\n",
              " 210,\n",
              " 152,\n",
              " 110,\n",
              " 66,\n",
              " 137,\n",
              " 530,\n",
              " 58,\n",
              " 105,\n",
              " 73,\n",
              " 95,\n",
              " 99,\n",
              " 78,\n",
              " 57,\n",
              " 103,\n",
              " 48,\n",
              " 102,\n",
              " 123,\n",
              " 322,\n",
              " 193,\n",
              " 76,\n",
              " 194,\n",
              " 52,\n",
              " 92,\n",
              " 94,\n",
              " 63,\n",
              " 27,\n",
              " 35,\n",
              " 65,\n",
              " 72,\n",
              " 79,\n",
              " 58,\n",
              " 128,\n",
              " 435,\n",
              " 197,\n",
              " 157,\n",
              " 58,\n",
              " 300,\n",
              " 70,\n",
              " 508,\n",
              " 86,\n",
              " 128,\n",
              " 96,\n",
              " 199,\n",
              " 144,\n",
              " 200,\n",
              " 31,\n",
              " 198,\n",
              " 88,\n",
              " 109,\n",
              " 116,\n",
              " 120,\n",
              " 93,\n",
              " 66,\n",
              " 92,\n",
              " 44,\n",
              " 53,\n",
              " 73,\n",
              " 41,\n",
              " 60,\n",
              " 89,\n",
              " 307,\n",
              " 55,\n",
              " 39,\n",
              " 39,\n",
              " 133,\n",
              " 69,\n",
              " 68,\n",
              " 82,\n",
              " 91,\n",
              " 129,\n",
              " 389,\n",
              " 97,\n",
              " 534,\n",
              " 69,\n",
              " 69,\n",
              " 29,\n",
              " 132,\n",
              " 143,\n",
              " 100,\n",
              " 67,\n",
              " 162,\n",
              " 75,\n",
              " 154,\n",
              " 34,\n",
              " 95,\n",
              " 266,\n",
              " 87,\n",
              " 77,\n",
              " 45,\n",
              " 324,\n",
              " 76,\n",
              " 114,\n",
              " 303,\n",
              " 143,\n",
              " 30,\n",
              " 258,\n",
              " 78,\n",
              " 136,\n",
              " 68,\n",
              " 32,\n",
              " 29,\n",
              " 168,\n",
              " 144,\n",
              " 308,\n",
              " 153,\n",
              " 55,\n",
              " 68,\n",
              " 112,\n",
              " 66,\n",
              " 72,\n",
              " 69,\n",
              " 61,\n",
              " 79,\n",
              " 316,\n",
              " 101,\n",
              " 166,\n",
              " 83,\n",
              " 131,\n",
              " 173,\n",
              " 91,\n",
              " 225,\n",
              " 223,\n",
              " 74,\n",
              " 139,\n",
              " 40,\n",
              " 41,\n",
              " 143,\n",
              " 81,\n",
              " 77,\n",
              " 96,\n",
              " 37,\n",
              " 49,\n",
              " 223,\n",
              " 63,\n",
              " 58,\n",
              " 92,\n",
              " 62,\n",
              " 105,\n",
              " 73,\n",
              " 37,\n",
              " 121,\n",
              " 86,\n",
              " 108,\n",
              " 204,\n",
              " 58,\n",
              " 138,\n",
              " 29,\n",
              " 32,\n",
              " 48,\n",
              " 41,\n",
              " 100,\n",
              " 157,\n",
              " 113,\n",
              " 231,\n",
              " 46,\n",
              " 58,\n",
              " 170,\n",
              " 85,\n",
              " 178,\n",
              " 82,\n",
              " 60,\n",
              " 73,\n",
              " 80,\n",
              " 87,\n",
              " 75,\n",
              " 32,\n",
              " 116,\n",
              " 133,\n",
              " 172,\n",
              " 177,\n",
              " 204,\n",
              " 323,\n",
              " 67,\n",
              " 171,\n",
              " 131,\n",
              " 219,\n",
              " 357,\n",
              " 58,\n",
              " 86,\n",
              " 115,\n",
              " 104,\n",
              " 92,\n",
              " 228,\n",
              " 177,\n",
              " 247,\n",
              " 79,\n",
              " 273,\n",
              " 103,\n",
              " 61,\n",
              " 588,\n",
              " 76,\n",
              " 100,\n",
              " 80,\n",
              " 64,\n",
              " 162,\n",
              " 182,\n",
              " 39,\n",
              " 191,\n",
              " 104,\n",
              " 130,\n",
              " 123,\n",
              " 85,\n",
              " 56,\n",
              " 92,\n",
              " 183,\n",
              " 144,\n",
              " 53,\n",
              " 77,\n",
              " 508,\n",
              " 24,\n",
              " 88,\n",
              " 43,\n",
              " 67,\n",
              " 91,\n",
              " 23,\n",
              " 98,\n",
              " 107,\n",
              " 138,\n",
              " 198,\n",
              " 166,\n",
              " 157,\n",
              " 107,\n",
              " 227,\n",
              " 60,\n",
              " 66,\n",
              " 270,\n",
              " 197,\n",
              " 196,\n",
              " 89,\n",
              " 133,\n",
              " 44,\n",
              " 211,\n",
              " 82,\n",
              " 97,\n",
              " 84,\n",
              " 355,\n",
              " 140,\n",
              " 137,\n",
              " 362,\n",
              " 344,\n",
              " 22,\n",
              " 61,\n",
              " 193,\n",
              " 64,\n",
              " 318,\n",
              " 191,\n",
              " 231,\n",
              " 58,\n",
              " 43,\n",
              " 401,\n",
              " 140,\n",
              " 87,\n",
              " 138,\n",
              " 77,\n",
              " 90,\n",
              " 132,\n",
              " 39,\n",
              " 47,\n",
              " 268,\n",
              " 66,\n",
              " 159,\n",
              " 43,\n",
              " 145,\n",
              " 259,\n",
              " 220,\n",
              " 67,\n",
              " 77,\n",
              " 23,\n",
              " 247,\n",
              " 92,\n",
              " 97,\n",
              " 23,\n",
              " 72,\n",
              " 55,\n",
              " 32,\n",
              " 72,\n",
              " 66,\n",
              " 118,\n",
              " 110,\n",
              " 46,\n",
              " 104,\n",
              " 25,\n",
              " 120,\n",
              " 117,\n",
              " 333,\n",
              " 109,\n",
              " 184,\n",
              " 39,\n",
              " 65,\n",
              " 127,\n",
              " 30,\n",
              " 94,\n",
              " 126,\n",
              " 53,\n",
              " 202,\n",
              " 30,\n",
              " 71,\n",
              " 22,\n",
              " 115,\n",
              " 71,\n",
              " 84,\n",
              " 150,\n",
              " 79,\n",
              " 148,\n",
              " 140,\n",
              " 40,\n",
              " 228,\n",
              " 122,\n",
              " 472,\n",
              " 82,\n",
              " 31,\n",
              " 189,\n",
              " 317,\n",
              " 69,\n",
              " 77,\n",
              " 84,\n",
              " 129,\n",
              " 91,\n",
              " 50,\n",
              " 72,\n",
              " 69,\n",
              " 66,\n",
              " 175,\n",
              " 115,\n",
              " 57,\n",
              " 82,\n",
              " 175,\n",
              " 141,\n",
              " 34,\n",
              " 169,\n",
              " 200,\n",
              " 48,\n",
              " 152,\n",
              " 265,\n",
              " 83,\n",
              " 384,\n",
              " 42,\n",
              " 78,\n",
              " 61,\n",
              " 169,\n",
              " 115,\n",
              " 36,\n",
              " 95,\n",
              " 111,\n",
              " 188,\n",
              " 91,\n",
              " 33,\n",
              " 109,\n",
              " 290,\n",
              " 386,\n",
              " 62,\n",
              " 37,\n",
              " 114,\n",
              " 73,\n",
              " 27,\n",
              " 162,\n",
              " 213,\n",
              " 157,\n",
              " 68,\n",
              " 72,\n",
              " 68,\n",
              " 78,\n",
              " 252,\n",
              " 183,\n",
              " 30,\n",
              " 67,\n",
              " 212,\n",
              " 42,\n",
              " 105,\n",
              " 34,\n",
              " 78,\n",
              " 209,\n",
              " 193,\n",
              " 61,\n",
              " 189,\n",
              " 46,\n",
              " 135,\n",
              " 92,\n",
              " 69,\n",
              " 67,\n",
              " 68,\n",
              " 79,\n",
              " 74,\n",
              " 66,\n",
              " 197,\n",
              " 67,\n",
              " 121,\n",
              " 200,\n",
              " 85,\n",
              " 142,\n",
              " 105,\n",
              " 69,\n",
              " 53,\n",
              " 71,\n",
              " 59,\n",
              " 65,\n",
              " 72,\n",
              " 230,\n",
              " 240,\n",
              " 187,\n",
              " 27,\n",
              " 100,\n",
              " 54,\n",
              " 80,\n",
              " 31,\n",
              " 54,\n",
              " 132,\n",
              " 111,\n",
              " 497,\n",
              " 199,\n",
              " 73,\n",
              " 109,\n",
              " 93,\n",
              " 60,\n",
              " 461,\n",
              " 67,\n",
              " 39,\n",
              " 71,\n",
              " 172,\n",
              " 57,\n",
              " 50,\n",
              " 467,\n",
              " 76,\n",
              " 112,\n",
              " 35,\n",
              " 70,\n",
              " 321,\n",
              " 44,\n",
              " 77,\n",
              " 190,\n",
              " 75,\n",
              " 164,\n",
              " 58,\n",
              " 366,\n",
              " 160,\n",
              " 344,\n",
              " 64,\n",
              " 61,\n",
              " 71,\n",
              " 109,\n",
              " 57,\n",
              " ...]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vemos como hemos crer\n",
        "\n",
        "average_number_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylvFDqQtSM0h"
      },
      "source": [
        "### *2.2. N-grams detection*\n",
        "\n",
        "Gensim N-gram detection is purely based on the detection of tokens that appear next to each other with high frequency. Gensim `Phraser` can be parameterized to allow some intermediate tokens which are normally considered as links tokens in the English language. However, since we have already carried out lemmatization and stopword removal we can make use of a very simple use of method.\n",
        "\n",
        "Two parameters are necessary:\n",
        "   - `min_count`: Minimum length for N-grams\n",
        "   - `threshold`: Minimum scoring for accepting N-grams. Higher values imply that fewer N-grams are accepted. The threshold is applied on a scoring function that depends on the frequency of the detected N-grams, as well as on the number of isolated occurrences of the component tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uniremos con un \"_\" todas las palabras adyacentes cuyo significado se complete al juntarse ambas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ctWMZLaTSXlE"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============= First review in corpus =============\n",
            "['set', 'paris', 'year', '1910', 'retired', 'old', 'rich', 'opera', 'singer', 'decides', 'give', 'fortune', 'away', 'beautiful', 'cat', 'duchess', 'voiced', 'eva', 'gabor', 'kitten', 'jealous', 'butler', 'edgar', 'come', 'plan', 'kidnaps', 'cat', 'leaf', 'countryside', 'luckily', 'help', 'streetwise', 'independent', 'tomcat', 'named', 'thomas', 'malley', 'voiced', 'phil', 'harris', 'help', 'get', 'home', 'especially', 'meeting', 'good', 'friend', 'like', 'swinging', 'scat', 'cat', 'voiced', 'scatman', 'crothers', 'try', 'foil', 'edgar', 'plan', 'entertaining', 'edgy', 'post', 'walt', 'disney', 'death', 'animated', 'movie', 'couple', 'nice', 'jazzy', 'tune', 'like', 'memorable', 'everybody', 'want', 'cat', 'good', 'voice', 'acting', 'terrific', 'animation', 'time', 'even', 'time', 'computer', 'animation', 'one', 'greatest', 'disney', 'animated', 'movie', 'cult', 'disney', 'animated', 'fave', 'one', 'gem', 'day', 'work', 'well', 'highly', 'recommended']\n",
            "\n",
            "============= First review after N-gram replacement =============\n",
            "['set', 'paris', 'year_1910', 'retired', 'old', 'rich', 'opera_singer', 'decides', 'give', 'fortune', 'away', 'beautiful', 'cat', 'duchess', 'voiced', 'eva_gabor', 'kitten', 'jealous', 'butler_edgar', 'come', 'plan', 'kidnaps', 'cat', 'leaf', 'countryside', 'luckily', 'help', 'streetwise', 'independent', 'tomcat', 'named', 'thomas_malley', 'voiced_phil', 'harris', 'help', 'get', 'home', 'especially', 'meeting', 'good', 'friend', 'like', 'swinging', 'scat_cat', 'voiced', 'scatman_crothers', 'try', 'foil', 'edgar', 'plan', 'entertaining', 'edgy', 'post', 'walt_disney', 'death', 'animated', 'movie', 'couple', 'nice', 'jazzy', 'tune', 'like', 'memorable', 'everybody', 'want', 'cat', 'good', 'voice', 'acting', 'terrific', 'animation', 'time', 'even', 'time', 'computer_animation', 'one', 'greatest', 'disney_animated', 'movie', 'cult', 'disney_animated', 'fave', 'one', 'gem', 'day', 'work', 'well', 'highly_recommended']\n"
          ]
        }
      ],
      "source": [
        "phrase_model = Phrases(corpus, min_count=2, threshold=20)\n",
        "\n",
        "print('\\n============= First review in corpus =============')\n",
        "print(corpus[0])\n",
        "corpus = [el for el in phrase_model[corpus]] # We populate corpus again\n",
        "print('\\n============= First review after N-gram replacement =============')\n",
        "print(corpus[0])\n",
        "\n",
        "# Veamos como lo hemos realizado exitosamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-lNvsmz9TGP"
      },
      "source": [
        "Let's save our clean reviews in a text file for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NU1Yv1Kq9ShB"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "corpus_df['clean_review'] = corpus\n",
        "\n",
        "with open(\"imdb_lemmas_clean.txt\", 'w', encoding='utf-8') as fout:\n",
        "  for el in corpus_df['clean_review'].values.tolist():\n",
        "    fout.write(' '.join(el) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7AKCo62_WzY"
      },
      "source": [
        "To be able to work with the corpus, we need to vectorize all its documents. To do so, there are two steps we need to carry out:\n",
        "\n",
        "1. Calculate the dictionary\n",
        "2. Transform the documents using the dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqNJKR0a_1vG"
      },
      "source": [
        "### *2.3. Gensim dictionary*\n",
        "\n",
        "As a first step for vectorizing documents, we need to create a dictionary containing all tokens in our text corpus and assign an integer identifier to each one of them.\n",
        "\n",
        "The following code fragment generates such a dictionary and shows the first tokens in the dictionary. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Crearemos un diccionario donde, cada token que aparece en nuestro corpus, será asignado con un valor numérico representativo del mismo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cubEHlmKAQgD"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The positive dictionary contains 72575 terms\n",
            "First terms in the dictionary:\n",
            "0 : acting\n",
            "1 : animated\n",
            "2 : animation\n",
            "3 : away\n",
            "4 : beautiful\n",
            "5 : butler_edgar\n",
            "6 : cat\n",
            "7 : come\n",
            "8 : computer_animation\n",
            "9 : countryside\n"
          ]
        }
      ],
      "source": [
        "# Create dictionary of tokens\n",
        "D = Dictionary(corpus)\n",
        "n_tokens = len(D)\n",
        "\n",
        "print('The positive dictionary contains', n_tokens, 'terms')\n",
        "print('First terms in the dictionary:')\n",
        "for n in range(10):\n",
        "    print(str(n), ':', D[n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjXFSoBaBz36"
      },
      "source": [
        "\n",
        "Saved\n",
        "207 words\n",
        "The dictionary object contains several attributes and methods that can be useful for carrying out some cleaning tasks. You can check the available methods using \n",
        "\n",
        "```\n",
        "dir(D)\n",
        "```\n",
        "\n",
        "Some of the most useful methods that we will use are:\n",
        "\n",
        "   - ```add_documents```: updates the dictionary processing new documents\n",
        "\n",
        "   - ```merge_with```: merges two dictionaries\n",
        "\n",
        "   - ```save```, ```save_as_text```, ```load```, ```load_from_text```: can be used to give persistence to the dictionary and reading a previously calculated dictionary\n",
        "\n",
        "   - ```id2token```: This is a Python dictionary for the mapping tokenid (a number) -> token (text representation). You can check that ```D[n]``` is equivalent to ```D.id2token[n]```\n",
        "\n",
        "   - ```token2id```: A Python dictionary for the reverse mapping token -> tokenid\n",
        "\n",
        "   - ```items```, ```keys```, ```values```, ```iteritems```, ```iterkeys```, ```itervalues```: Can be used to obtain al items (tokenid, token), all tokenids, or all token texts, or to iterate over them.\n",
        "\n",
        "   - ```dfs```: A Python dictionary for the mapping tokenid -> Number of documents where the token appears\n",
        "\n",
        "   - ```filter_tokens```, ```filter_extremes```, ```filter_n_most_frequent```: are used to remove elements from the dictionary, and ```compactify```is used to reassign tokenids to tokens for a more efficient representation.\n",
        "\n",
        "   - ```doc2bow```: converts a document into its Bag of Words Representation\n",
        "\n",
        "   - ```doc2idx```: transforms a document into a sequence of the tokenids of the words of the document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1E2Vq7ZCt21"
      },
      "source": [
        "##### **Exercise 8**\n",
        "\n",
        "1. Obtain a dataframe with 2 columns: `token` and `ndocs`, corresponding to the text of each token and the number of documents where the token appears\n",
        "\n",
        "2. Sort the dataframe according to column `ndocs`. \n",
        "\n",
        "3. How many tokens appear in exactly one document? Remove them from the dataframe.\n",
        "\n",
        "4. What are the most and less common tokens in the dictionary in terms of document occurrence?\n",
        "\n",
        "3. Plot a histogram of the number of token appearances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Crearemos un datafram donde tendremos, para cada token que aparezca en los docs, el número de documentos en los que aparece; eliminamos los que solo aparecen una vez y ploteamos un histograma en función de sus ocurriencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "IlmfLWJsDDik"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkEAAAGdCAYAAAAVEKdkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuRElEQVR4nO3de3SU9Z3H8U8SkkkCDOFiEim3dFEhck9KGC9d1JARc1xRdNFlaYqIC5u4huxCzRaDQF1YWkHUYFoVwh6lXPZUW4ECs0GgSAAJRLkItSvduMVJrFyG62RInv3Dk2cZE7BJhgn4e7/OyTnM7/ed5/k930nC5zzPPJkIy7IsAQAAGCayrRcAAADQFghBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjtWvrBbSl+vp6HTt2TB07dlRERERbLwcAAPwFLMvS6dOn1b17d0VGtvx8jtEh6NixY+rZs2dbLwMAALTAZ599ph49erT4+UaHoI4dO0r6qolOpzNk2w0EAtq0aZOysrIUHR0dsu2iafQ7vOh3+NDr8KLf4dWafvt8PvXs2dP+f7yljA5BDZfAnE5nyENQfHy8nE4nP0hhQL/Di36HD70OL/odXqHod2vfysIbowEAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIzUrBD333HOKiIgI+urXr589f+HCBeXm5qpr167q0KGDxo4dq+rq6qBtVFVVKTs7W/Hx8UpMTNT06dN18eLFoJotW7Zo2LBhcjgc6tu3r0pLSxutpbi4WH369FFsbKwyMjK0e/fu5hwKAAAwXLPPBN166636/PPP7a/t27fbc9OmTdO7776rNWvWaOvWrTp27Jgeeughe76urk7Z2dmqra3Vjh07tHz5cpWWlqqoqMiuOXr0qLKzs3XXXXepsrJS+fn5euKJJ7Rx40a7ZtWqVSooKNCsWbO0d+9eDR48WG63WzU1NS3tAwAAMEyzQ1C7du2UnJxsf3Xr1k2SdOrUKb3xxhtauHCh7r77bqWlpWnZsmXasWOHdu7cKUnatGmTDh06pDfffFNDhgzR6NGjNXfuXBUXF6u2tlaSVFJSopSUFL3wwgvq37+/8vLy9PDDD2vRokX2GhYuXKjJkydr4sSJSk1NVUlJieLj47V06dJQ9AQAABig2Z8d9sknn6h79+6KjY2Vy+XSvHnz1KtXL1VUVCgQCCgzM9Ou7devn3r16qXy8nKNGDFC5eXlGjhwoJKSkuwat9utqVOn6uDBgxo6dKjKy8uDttFQk5+fL0mqra1VRUWFCgsL7fnIyEhlZmaqvLz8imv3+/3y+/32Y5/PJ+mrzy8JBALNbcVlNWwrlNvE5dHv8KLf4UOvw4t+h1dr+h2q16hZISgjI0OlpaW65ZZb9Pnnn2v27Nm68847deDAAXm9XsXExCghISHoOUlJSfJ6vZIkr9cbFIAa5hvmrlTj8/l0/vx5nThxQnV1dU3WHD58+IrrnzdvnmbPnt1ofNOmTYqPj//mBjSTx+MJ+TZxefQ7vOh3+NDr8KLf4dWSfp87dy4k+25WCBo9erT970GDBikjI0O9e/fW6tWrFRcXF5IFXU2FhYUqKCiwH/t8PvXs2VNZWVkh/xR5j8ejUaNG8UnEYUC/w4t+hw+9Di/6HV6t6XfDlZzWavblsEslJCTo5ptv1h/+8AeNGjVKtbW1OnnyZNDZoOrqaiUnJ0uSkpOTG93F1XD32KU1X7+jrLq6Wk6nU3FxcYqKilJUVFSTNQ3buByHwyGHw9FoPDo6+qp8ww99frP8dRFBY3+cnx3y/eArV+t1RNPod/jQ6/Ci3+HVkn6H6vVp1d8JOnPmjP77v/9bN954o9LS0hQdHa2ysjJ7/siRI6qqqpLL5ZIkuVwu7d+/P+guLo/HI6fTqdTUVLvm0m001DRsIyYmRmlpaUE19fX1Kisrs2sAAAC+SbNC0L/8y79o69at+uMf/6gdO3bowQcfVFRUlB577DF16tRJkyZNUkFBgd577z1VVFRo4sSJcrlcGjFihCQpKytLqampmjBhgj788ENt3LhRM2fOVG5urn2GZsqUKfr00081Y8YMHT58WEuWLNHq1as1bdo0ex0FBQV67bXXtHz5cn388ceaOnWqzp49q4kTJ4awNQAA4NusWZfD/vd//1ePPfaYvvzyS91www264447tHPnTt1www2SpEWLFikyMlJjx46V3++X2+3WkiVL7OdHRUVp7dq1mjp1qlwul9q3b6+cnBzNmTPHrklJSdG6des0bdo0LV68WD169NDrr78ut9tt14wbN05ffPGFioqK5PV6NWTIEG3YsKHRm6UBAAAup1khaOXKlVecj42NVXFxsYqLiy9b07t3b61fv/6K2xk5cqT27dt3xZq8vDzl5eVdsQYAAOBy+OwwAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIzUqhA0f/58RUREKD8/3x67cOGCcnNz1bVrV3Xo0EFjx45VdXV10POqqqqUnZ2t+Ph4JSYmavr06bp48WJQzZYtWzRs2DA5HA717dtXpaWljfZfXFysPn36KDY2VhkZGdq9e3drDgcAABikxSHogw8+0M9//nMNGjQoaHzatGl69913tWbNGm3dulXHjh3TQw89ZM/X1dUpOztbtbW12rFjh5YvX67S0lIVFRXZNUePHlV2drbuuusuVVZWKj8/X0888YQ2btxo16xatUoFBQWaNWuW9u7dq8GDB8vtdqumpqalhwQAAAzSohB05swZjR8/Xq+99po6d+5sj586dUpvvPGGFi5cqLvvvltpaWlatmyZduzYoZ07d0qSNm3apEOHDunNN9/UkCFDNHr0aM2dO1fFxcWqra2VJJWUlCglJUUvvPCC+vfvr7y8PD388MNatGiRva+FCxdq8uTJmjhxolJTU1VSUqL4+HgtXbq0Nf0AAACGaNeSJ+Xm5io7O1uZmZn6yU9+Yo9XVFQoEAgoMzPTHuvXr5969eql8vJyjRgxQuXl5Ro4cKCSkpLsGrfbralTp+rgwYMaOnSoysvLg7bRUNNw2a22tlYVFRUqLCy05yMjI5WZmany8vLLrtvv98vv99uPfT6fJCkQCCgQCLSkFU1q2JYj0rrsHEKnoaf0Njzod/jQ6/Ci3+HVmn6H6jVqdghauXKl9u7dqw8++KDRnNfrVUxMjBISEoLGk5KS5PV67ZpLA1DDfMPclWp8Pp/Onz+vEydOqK6ursmaw4cPX3bt8+bN0+zZsxuNb9q0SfHx8Zd9XkvNTa9vNLZ+/fqQ7wdf8Xg8bb0Eo9Dv8KHX4UW/w6sl/T537lxI9t2sEPTZZ5/p6aeflsfjUWxsbEgWEE6FhYUqKCiwH/t8PvXs2VNZWVlyOp0h208gEJDH49GzeyLlr48ImjvwnDtk+8FXGvo9atQoRUdHt/VyvvXod/jQ6/Ci3+HVmn43XMlprWaFoIqKCtXU1GjYsGH2WF1dnbZt26ZXXnlFGzduVG1trU6ePBl0Nqi6ulrJycmSpOTk5EZ3cTXcPXZpzdfvKKuurpbT6VRcXJyioqIUFRXVZE3DNpricDjkcDgajUdHR1+Vb3h/fYT8dcEhiB+sq+dqvY5oGv0OH3odXvQ7vFrS71C9Ps16Y/Q999yj/fv3q7Ky0v5KT0/X+PHj7X9HR0errKzMfs6RI0dUVVUll8slSXK5XNq/f3/QXVwej0dOp1Opqal2zaXbaKhp2EZMTIzS0tKCaurr61VWVmbXAAAAXEmzzgR17NhRAwYMCBpr3769unbtao9PmjRJBQUF6tKli5xOp5566im5XC6NGDFCkpSVlaXU1FRNmDBBCxYskNfr1cyZM5Wbm2ufpZkyZYpeeeUVzZgxQ48//rg2b96s1atXa926dfZ+CwoKlJOTo/T0dA0fPlwvvviizp49q4kTJ7aqIQAAwAwtujvsShYtWqTIyEiNHTtWfr9fbrdbS5YsseejoqK0du1aTZ06VS6XS+3bt1dOTo7mzJlj16SkpGjdunWaNm2aFi9erB49euj111+X2/3/76cZN26cvvjiCxUVFcnr9WrIkCHasGFDozdLAwAANKXVIWjLli1Bj2NjY1VcXKzi4uLLPqd3797feJfUyJEjtW/fvivW5OXlKS8v7y9eKwAAQAM+OwwAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIzUrBL366qsaNGiQnE6nnE6nXC6Xfvvb39rzFy5cUG5urrp27aoOHTpo7Nixqq6uDtpGVVWVsrOzFR8fr8TERE2fPl0XL14MqtmyZYuGDRsmh8Ohvn37qrS0tNFaiouL1adPH8XGxiojI0O7d+9uzqEAAADDNSsE9ejRQ/Pnz1dFRYX27Nmju+++Ww888IAOHjwoSZo2bZreffddrVmzRlu3btWxY8f00EMP2c+vq6tTdna2amtrtWPHDi1fvlylpaUqKiqya44ePars7GzdddddqqysVH5+vp544glt3LjRrlm1apUKCgo0a9Ys7d27V4MHD5bb7VZNTU1r+wEAAAzRrBB0//3367777tNNN92km2++Wc8//7w6dOignTt36tSpU3rjjTe0cOFC3X333UpLS9OyZcu0Y8cO7dy5U5K0adMmHTp0SG+++aaGDBmi0aNHa+7cuSouLlZtba0kqaSkRCkpKXrhhRfUv39/5eXl6eGHH9aiRYvsdSxcuFCTJ0/WxIkTlZqaqpKSEsXHx2vp0qUhbA0AAPg2a9fSJ9bV1WnNmjU6e/asXC6XKioqFAgElJmZadf069dPvXr1Unl5uUaMGKHy8nINHDhQSUlJdo3b7dbUqVN18OBBDR06VOXl5UHbaKjJz8+XJNXW1qqiokKFhYX2fGRkpDIzM1VeXn7FNfv9fvn9fvuxz+eTJAUCAQUCgZa2opGGbTkircvOIXQaekpvw4N+hw+9Di/6HV6t6XeoXqNmh6D9+/fL5XLpwoUL6tChg95++22lpqaqsrJSMTExSkhICKpPSkqS1+uVJHm93qAA1DDfMHelGp/Pp/Pnz+vEiROqq6trsubw4cNXXPu8efM0e/bsRuObNm1SfHz8Nx98M81Nr280tn79+pDvB1/xeDxtvQSj0O/wodfhRb/DqyX9PnfuXEj23ewQdMstt6iyslKnTp3Sf/7nfyonJ0dbt24NyWKutsLCQhUUFNiPfT6fevbsqaysLDmdzpDtJxAIyOPx6Nk9kfLXRwTNHXjOHbL94CsN/R41apSio6PbejnfevQ7fOh1eNHv8GpNvxuu5LRWs0NQTEyM+vbtK0lKS0vTBx98oMWLF2vcuHGqra3VyZMng84GVVdXKzk5WZKUnJzc6C6uhrvHLq35+h1l1dXVcjqdiouLU1RUlKKiopqsadjG5TgcDjkcjkbj0dHRV+Ub3l8fIX9dcAjiB+vquVqvI5pGv8OHXocX/Q6vlvQ7VK9Pq/9OUH19vfx+v9LS0hQdHa2ysjJ77siRI6qqqpLL5ZIkuVwu7d+/P+guLo/HI6fTqdTUVLvm0m001DRsIyYmRmlpaUE19fX1Kisrs2sAAAC+SbPOBBUWFmr06NHq1auXTp8+rRUrVmjLli3auHGjOnXqpEmTJqmgoEBdunSR0+nUU089JZfLpREjRkiSsrKylJqaqgkTJmjBggXyer2aOXOmcnNz7TM0U6ZM0SuvvKIZM2bo8ccf1+bNm7V69WqtW7fOXkdBQYFycnKUnp6u4cOH68UXX9TZs2c1ceLEELYGAAB8mzUrBNXU1OgHP/iBPv/8c3Xq1EmDBg3Sxo0bNWrUKEnSokWLFBkZqbFjx8rv98vtdmvJkiX286OiorR27VpNnTpVLpdL7du3V05OjubMmWPXpKSkaN26dZo2bZoWL16sHj166PXXX5fb/f/vpRk3bpy++OILFRUVyev1asiQIdqwYUOjN0sDAABcTrNC0BtvvHHF+djYWBUXF6u4uPiyNb179/7GO6RGjhypffv2XbEmLy9PeXl5V6wBAAC4HD47DAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGalYImjdvnr73ve+pY8eOSkxM1JgxY3TkyJGgmgsXLig3N1ddu3ZVhw4dNHbsWFVXVwfVVFVVKTs7W/Hx8UpMTNT06dN18eLFoJotW7Zo2LBhcjgc6tu3r0pLSxutp7i4WH369FFsbKwyMjK0e/fu5hwOAAAwWLNC0NatW5Wbm6udO3fK4/EoEAgoKytLZ8+etWumTZumd999V2vWrNHWrVt17NgxPfTQQ/Z8XV2dsrOzVVtbqx07dmj58uUqLS1VUVGRXXP06FFlZ2frrrvuUmVlpfLz8/XEE09o48aNds2qVatUUFCgWbNmae/evRo8eLDcbrdqampa0w8AAGCIds0p3rBhQ9Dj0tJSJSYmqqKiQt///vd16tQpvfHGG1qxYoXuvvtuSdKyZcvUv39/7dy5UyNGjNCmTZt06NAh/dd//ZeSkpI0ZMgQzZ07Vz/60Y/03HPPKSYmRiUlJUpJSdELL7wgSerfv7+2b9+uRYsWye12S5IWLlyoyZMna+LEiZKkkpISrVu3TkuXLtUzzzzT6sYAAIBvt1a9J+jUqVOSpC5dukiSKioqFAgElJmZadf069dPvXr1Unl5uSSpvLxcAwcOVFJSkl3jdrvl8/l08OBBu+bSbTTUNGyjtrZWFRUVQTWRkZHKzMy0awAAAK6kWWeCLlVfX6/8/HzdfvvtGjBggCTJ6/UqJiZGCQkJQbVJSUnyer12zaUBqGG+Ye5KNT6fT+fPn9eJEydUV1fXZM3hw4cvu2a/3y+/328/9vl8kqRAIKBAIPCXHvo3atiWI9K67BxCp6Gn9DY86Hf40Ovwot/h1Zp+h+o1anEIys3N1YEDB7R9+/aQLCQc5s2bp9mzZzca37Rpk+Lj40O+v7np9Y3G1q9fH/L94Csej6etl2AU+h0+9Dq86Hd4taTf586dC8m+WxSC8vLytHbtWm3btk09evSwx5OTk1VbW6uTJ08GnQ2qrq5WcnKyXfP1u7ga7h67tObrd5RVV1fL6XQqLi5OUVFRioqKarKmYRtNKSwsVEFBgf3Y5/OpZ8+eysrKktPpbEYHriwQCMjj8ejZPZHy10cEzR14zh2y/eArDf0eNWqUoqOj23o533r0O3zodXjR7/BqTb8bruS0VrNCkGVZeuqpp/T2229ry5YtSklJCZpPS0tTdHS0ysrKNHbsWEnSkSNHVFVVJZfLJUlyuVx6/vnnVVNTo8TERElfpUCn06nU1FS75utnTDwej72NmJgYpaWlqaysTGPGjJH01eW5srIy5eXlXXb9DodDDoej0Xh0dPRV+Yb310fIXxccgvjBunqu1uuIptHv8KHX4UW/w6sl/Q7V69OsEJSbm6sVK1bo17/+tTp27Gi/h6dTp06Ki4tTp06dNGnSJBUUFKhLly5yOp166qmn5HK5NGLECElSVlaWUlNTNWHCBC1YsEBer1czZ85Ubm6uHVCmTJmiV155RTNmzNDjjz+uzZs3a/Xq1Vq3bp29loKCAuXk5Cg9PV3Dhw/Xiy++qLNnz9p3iwEAAFxJs0LQq6++KkkaOXJk0PiyZcv0wx/+UJK0aNEiRUZGauzYsfL7/XK73VqyZIldGxUVpbVr12rq1KlyuVxq3769cnJyNGfOHLsmJSVF69at07Rp07R48WL16NFDr7/+un17vCSNGzdOX3zxhYqKiuT1ejVkyBBt2LCh0ZulAQAAmtLsy2HfJDY2VsXFxSouLr5sTe/evb/xDcIjR47Uvn37rliTl5d3xctfAAAAl8NnhwEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgpGaHoG3btun+++9X9+7dFRERoXfeeSdo3rIsFRUV6cYbb1RcXJwyMzP1ySefBNUcP35c48ePl9PpVEJCgiZNmqQzZ84E1Xz00Ue68847FRsbq549e2rBggWN1rJmzRr169dPsbGxGjhwoNavX9/cwwEAAIZqdgg6e/asBg8erOLi4ibnFyxYoJdeekklJSXatWuX2rdvL7fbrQsXLtg148eP18GDB+XxeLR27Vpt27ZNTz75pD3v8/mUlZWl3r17q6KiQj/96U/13HPP6Re/+IVds2PHDj322GOaNGmS9u3bpzFjxmjMmDE6cOBAcw8JAAAYqF1znzB69GiNHj26yTnLsvTiiy9q5syZeuCBByRJ//Ef/6GkpCS98847evTRR/Xxxx9rw4YN+uCDD5Seni5Jevnll3XffffpZz/7mbp376633npLtbW1Wrp0qWJiYnTrrbeqsrJSCxcutMPS4sWLde+992r69OmSpLlz58rj8eiVV15RSUlJi5oBAADM0ewQdCVHjx6V1+tVZmamPdapUydlZGSovLxcjz76qMrLy5WQkGAHIEnKzMxUZGSkdu3apQcffFDl5eX6/ve/r5iYGLvG7Xbr3//933XixAl17txZ5eXlKigoCNq/2+1udHnuUn6/X36/337s8/kkSYFAQIFAoLWHb2vYliPSuuwcQqehp/Q2POh3+NDr8KLf4dWafofqNQppCPJ6vZKkpKSkoPGkpCR7zuv1KjExMXgR7dqpS5cuQTUpKSmNttEw17lzZ3m93ivupynz5s3T7NmzG41v2rRJ8fHxf8khNsvc9PpGY7xv6erxeDxtvQSj0O/wodfhRb/DqyX9PnfuXEj2HdIQdK0rLCwMOnvk8/nUs2dPZWVlyel0hmw/gUBAHo9Hz+6JlL8+ImjuwHPukO0HX2no96hRoxQdHd3Wy/nWo9/hQ6/Di36HV2v63XAlp7VCGoKSk5MlSdXV1brxxhvt8erqag0ZMsSuqampCXrexYsXdfz4cfv5ycnJqq6uDqppePxNNQ3zTXE4HHI4HI3Go6Ojr8o3vL8+Qv664BDED9bVc7VeRzSNfocPvQ4v+h1eLel3qF6fkP6doJSUFCUnJ6usrMwe8/l82rVrl1wulyTJ5XLp5MmTqqiosGs2b96s+vp6ZWRk2DXbtm0Luubn8Xh0yy23qHPnznbNpftpqGnYDwAAwJU0OwSdOXNGlZWVqqyslPTVm6ErKytVVVWliIgI5efn6yc/+Yl+85vfaP/+/frBD36g7t27a8yYMZKk/v37695779XkyZO1e/duvf/++8rLy9Ojjz6q7t27S5L+7u/+TjExMZo0aZIOHjyoVatWafHixUGXsp5++mlt2LBBL7zwgg4fPqznnntOe/bsUV5eXuu7AgAAvvWafTlsz549uuuuu+zHDcEkJydHpaWlmjFjhs6ePasnn3xSJ0+e1B133KENGzYoNjbWfs5bb72lvLw83XPPPYqMjNTYsWP10ksv2fOdOnXSpk2blJubq7S0NHXr1k1FRUVBf0votttu04oVKzRz5kz967/+q2666Sa98847GjBgQIsaAQAAzNLsEDRy5EhZVuNbvxtERERozpw5mjNnzmVrunTpohUrVlxxP4MGDdLvfve7K9Y88sgjeuSRR668YAAAgCbw2WEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMFK7tl6Aafo8s67R2B/nZ7fBSgAAMNt1fyaouLhYffr0UWxsrDIyMrR79+62XhIAALgOXNchaNWqVSooKNCsWbO0d+9eDR48WG63WzU1NW29NAAAcI27ri+HLVy4UJMnT9bEiRMlSSUlJVq3bp2WLl2qZ555po1X95dr6hKZxGUyAACupus2BNXW1qqiokKFhYX2WGRkpDIzM1VeXt7kc/x+v/x+v/341KlTkqTjx48rEAiEbG2BQEDnzp1Tu0Ck6uojWrydvv+yusnxXYX3tHib30YN/f7yyy8VHR3d1sv51qPf4UOvw4t+h1dr+n369GlJkmVZrVrDdRuC/vznP6uurk5JSUlB40lJSTp8+HCTz5k3b55mz57daDwlJeWqrPFq6fZCW68AAIC2d/r0aXXq1KnFz79uQ1BLFBYWqqCgwH5cX1+v48ePq2vXroqIaPkZm6/z+Xzq2bOnPvvsMzmdzpBtF02j3+FFv8OHXocX/Q6v1vTbsiydPn1a3bt3b9UartsQ1K1bN0VFRam6ujpovLq6WsnJyU0+x+FwyOFwBI0lJCRcrSXK6XTygxRG9Du86Hf40Ovwot/h1dJ+t+YMUIPr9u6wmJgYpaWlqayszB6rr69XWVmZXC5XG64MAABcD67bM0GSVFBQoJycHKWnp2v48OF68cUXdfbsWftuMQAAgMu5rkPQuHHj9MUXX6ioqEher1dDhgzRhg0bGr1ZOtwcDodmzZrV6NIbrg76HV70O3zodXjR7/C6FvodYbX2/jIAAIDr0HX7niAAAIDWIAQBAAAjEYIAAICRCEEAAMBIhKCroLi4WH369FFsbKwyMjK0e/futl7SNW3evHn63ve+p44dOyoxMVFjxozRkSNHgmouXLig3Nxcde3aVR06dNDYsWMb/aHMqqoqZWdnKz4+XomJiZo+fbouXrwYVLNlyxYNGzZMDodDffv2VWlp6dU+vGve/PnzFRERofz8fHuMfofWn/70J/393/+9unbtqri4OA0cOFB79uyx5y3LUlFRkW688UbFxcUpMzNTn3zySdA2jh8/rvHjx8vpdCohIUGTJk3SmTNngmo++ugj3XnnnYqNjVXPnj21YMGCsBzftaSurk7PPvusUlJSFBcXp7/6q7/S3Llzgz5jin633LZt23T//fere/fuioiI0DvvvBM0H87erlmzRv369VNsbKwGDhyo9evXN/+ALITUypUrrZiYGGvp0qXWwYMHrcmTJ1sJCQlWdXV1Wy/tmuV2u61ly5ZZBw4csCorK6377rvP6tWrl3XmzBm7ZsqUKVbPnj2tsrIya8+ePdaIESOs2267zZ6/ePGiNWDAACszM9Pat2+ftX79eqtbt25WYWGhXfPpp59a8fHxVkFBgXXo0CHr5ZdftqKioqwNGzaE9XivJbt377b69OljDRo0yHr66aftcfodOsePH7d69+5t/fCHP7R27dplffrpp9bGjRutP/zhD3bN/PnzrU6dOlnvvPOO9eGHH1p/8zd/Y6WkpFjnz5+3a+69915r8ODB1s6dO63f/e53Vt++fa3HHnvMnj916pSVlJRkjR8/3jpw4ID1y1/+0oqLi7N+/vOfh/V429rzzz9vde3a1Vq7dq119OhRa82aNVaHDh2sxYsX2zX0u+XWr19v/fjHP7Z+9atfWZKst99+O2g+XL19//33raioKGvBggXWoUOHrJkzZ1rR0dHW/v37m3U8hKAQGz58uJWbm2s/rqurs7p3727NmzevDVd1fampqbEkWVu3brUsy7JOnjxpRUdHW2vWrLFrPv74Y0uSVV5eblnWVz+YkZGRltfrtWteffVVy+l0Wn6/37Isy5oxY4Z16623Bu1r3LhxltvtvtqHdE06ffq0ddNNN1kej8f667/+azsE0e/Q+tGPfmTdcccdl52vr6+3kpOTrZ/+9Kf22MmTJy2Hw2H98pe/tCzLsg4dOmRJsj744AO75re//a0VERFh/elPf7Isy7KWLFlide7c2e5/w75vueWWUB/SNS07O9t6/PHHg8Yeeugha/z48ZZl0e9Q+noICmdv//Zv/9bKzs4OWk9GRob1D//wD806Bi6HhVBtba0qKiqUmZlpj0VGRiozM1Pl5eVtuLLry6lTpyRJXbp0kSRVVFQoEAgE9bVfv37q1auX3dfy8nINHDgw6A9lut1u+Xw+HTx40K65dBsNNaa+Nrm5ucrOzm7UE/odWr/5zW+Unp6uRx55RImJiRo6dKhee+01e/7o0aPyer1BverUqZMyMjKC+p2QkKD09HS7JjMzU5GRkdq1a5dd8/3vf18xMTF2jdvt1pEjR3TixImrfZjXjNtuu01lZWX6/e9/L0n68MMPtX37do0ePVoS/b6awtnbUP1+IQSF0J///GfV1dU1+ovVSUlJ8nq9bbSq60t9fb3y8/N1++23a8CAAZIkr9ermJiYRh92e2lfvV5vk31vmLtSjc/n0/nz56/G4VyzVq5cqb1792revHmN5uh3aH366ad69dVXddNNN2njxo2aOnWq/umf/knLly+X9P/9utLvDa/Xq8TExKD5du3aqUuXLs16TUzwzDPP6NFHH1W/fv0UHR2toUOHKj8/X+PHj5dEv6+mcPb2cjXN7f11/bEZ+PbJzc3VgQMHtH379rZeyrfWZ599pqeffloej0exsbFtvZxvvfr6eqWnp+vf/u3fJElDhw7VgQMHVFJSopycnDZe3bfP6tWr9dZbb2nFihW69dZbVVlZqfz8fHXv3p1+oxHOBIVQt27dFBUV1egumurqaiUnJ7fRqq4feXl5Wrt2rd577z316NHDHk9OTlZtba1OnjwZVH9pX5OTk5vse8PclWqcTqfi4uJCfTjXrIqKCtXU1GjYsGFq166d2rVrp61bt+qll15Su3btlJSURL9D6MYbb1RqamrQWP/+/VVVVSXp//t1pd8bycnJqqmpCZq/ePGijh8/3qzXxATTp0+3zwYNHDhQEyZM0LRp0+yznvT76glnby9X09zeE4JCKCYmRmlpaSorK7PH6uvrVVZWJpfL1YYru7ZZlqW8vDy9/fbb2rx5s1JSUoLm09LSFB0dHdTXI0eOqKqqyu6ry+XS/v37g364PB6PnE6n/R+Qy+UK2kZDjWmvzT333KP9+/ersrLS/kpPT9f48ePtf9Pv0Ln99tsb/cmH3//+9+rdu7ckKSUlRcnJyUG98vl82rVrV1C/T548qYqKCrtm8+bNqq+vV0ZGhl2zbds2BQIBu8bj8eiWW25R586dr9rxXWvOnTunyMjg/9qioqJUX18viX5fTeHsbch+vzTrbdT4RitXrrQcDodVWlpqHTp0yHryySethISEoLtoEGzq1KlWp06drC1btliff/65/XXu3Dm7ZsqUKVavXr2szZs3W3v27LFcLpflcrns+YZbtrOysqzKykprw4YN1g033NDkLdvTp0+3Pv74Y6u4uNjIW7abcundYZZFv0Np9+7dVrt27aznn3/e+uSTT6y33nrLio+Pt9588027Zv78+VZCQoL161//2vroo4+sBx54oMnbiocOHWrt2rXL2r59u3XTTTcF3VZ88uRJKykpyZowYYJ14MABa+XKlVZ8fPy3/pbtr8vJybG+853v2LfI/+pXv7K6detmzZgxw66h3y13+vRpa9++fda+ffssSdbChQutffv2Wf/zP/9jWVb4evv+++9b7dq1s372s59ZH3/8sTVr1ixukb9WvPzyy1avXr2smJgYa/jw4dbOnTvbeknXNElNfi1btsyuOX/+vPWP//iPVufOna34+HjrwQcftD7//POg7fzxj3+0Ro8ebcXFxVndunWz/vmf/9kKBAJBNe+99541ZMgQKyYmxvrud78btA+TfT0E0e/Qevfdd60BAwZYDofD6tevn/WLX/wiaL6+vt569tlnraSkJMvhcFj33HOPdeTIkaCaL7/80nrsscesDh06WE6n05o4caJ1+vTpoJoPP/zQuuOOOyyHw2F95zvfsebPn3/Vj+1a4/P5rKefftrq1auXFRsba333u9+1fvzjHwfdbk2/W+69995r8vd1Tk6OZVnh7e3q1autm2++2YqJibFuvfVWa926dc0+ngjLuuTPaAIAABiC9wQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYKT/A42rZ05kg+z0AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#<SOL>\n",
        "\n",
        "kk = pd.DataFrame(columns=['token','ndocs'])\n",
        "\n",
        "tokens_diccionario = []\n",
        "freq_token = []\n",
        "\n",
        "for i in range(len(D)):\n",
        "    tokens_diccionario.append(D[i]) \n",
        "    freq_token.append(D.dfs[i])\n",
        "\n",
        "kk['token'] = tokens_diccionario\n",
        "kk['ndocs'] = freq_token\n",
        "\n",
        "kk = kk.drop(kk[kk.ndocs == 1].index)\n",
        "\n",
        "minimo_frq = kk['ndocs'].min()\n",
        "min_token = kk.loc[kk['ndocs'] == minimo_frq]\n",
        "\n",
        "maximo_frq = kk['ndocs'].max()\n",
        "max_token = kk.loc[kk['ndocs'] == maximo_frq]\n",
        "\n",
        "kk['ndocs'].hist(bins = 80)\n",
        "\n",
        "#</SOL>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por lo general, suelen aparecer pocas veces en documentos diferentes; o, al menos, es su tendencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>ndocs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>acting</td>\n",
              "      <td>3081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>animated</td>\n",
              "      <td>144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>animation</td>\n",
              "      <td>294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>away</td>\n",
              "      <td>921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>beautiful</td>\n",
              "      <td>931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71636</th>\n",
              "      <td>chungking_express</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71724</th>\n",
              "      <td>improvisationally</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71759</th>\n",
              "      <td>galasso</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71981</th>\n",
              "      <td>distend</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72113</th>\n",
              "      <td>neuron</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>51313 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   token  ndocs\n",
              "0                 acting   3081\n",
              "1               animated    144\n",
              "2              animation    294\n",
              "3                   away    921\n",
              "4              beautiful    931\n",
              "...                  ...    ...\n",
              "71636  chungking_express      3\n",
              "71724  improvisationally      2\n",
              "71759            galasso      2\n",
              "71981            distend      2\n",
              "72113             neuron      2\n",
              "\n",
              "[51313 rows x 2 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Veamos como hemos creado el df bien y hemos eliminado los que aparecen solo una vez\n",
        "\n",
        "kk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUU9rKrkDTdL"
      },
      "source": [
        "Next, we will filter out terms that appear in too few or too many of the documents in the dataset. This makes sense because:\n",
        "\n",
        "   - terms that appear in most documents are probably not very informative in the general context of a particular corpus\n",
        "   - terms that appear in a very reduced number of documents are not useful to find repetitive patterns across documents. In fact, in many cases, we find that many of the words that are eliminated for this reason can be typos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Eliminamos del diccionario todo token que aparezca o muy pocas veces (menos de 4) o demasiadas (en más de un 80% de los documentos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "C_UcE8geDSuT"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The dictionary contains 29779 terms\n"
          ]
        }
      ],
      "source": [
        "no_below = 4 #Minimum number of documents to keep a term in the dictionary\n",
        "no_above = .80 #Maximum proportion of documents in which a term can appear to be kept in the dictionary\n",
        "\n",
        "D.filter_extremes(no_below=no_below,no_above=no_above)\n",
        "n_tokens = len(D)\n",
        "\n",
        "print('The dictionary contains', n_tokens, 'terms')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_hNAvi8DaNv"
      },
      "source": [
        "You can check dictionary size has been considerably reduced with respect to the original vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xpdMDkV6Qrox"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "corpus_def = []\n",
        "for sent in corpus:\n",
        "  aux = [token for token in sent if token in D.token2id.keys()]\n",
        "  corpus_def.append(aux)\n",
        "\n",
        "corpus_df['clean_review'] = corpus_def"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>clean_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33003</th>\n",
              "      <td>[set, paris, year, 1910, retired, old, rich, o...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[set, paris, retired, old, rich, opera_singer,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12172</th>\n",
              "      <td>[basic, structure, story, beginning, middle, e...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[basic, structure, story, beginning_middle, en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5192</th>\n",
              "      <td>[odd, willfully, skewed, biopic, dyan, thomas,...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[odd, willfully, skewed, biopic, dyan, thomas,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32511</th>\n",
              "      <td>[okay, penelope, keith, miss, herringbone, twe...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[okay, miss, b, b, e, backbone, england, kille...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43723</th>\n",
              "      <td>[larger, life, figure, wyatt, earp, bat, maste...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[larger_life, figure, wyatt_earp, bat, masters...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16645</th>\n",
              "      <td>[6, 10, acting, great, good, acting, 4, 10, di...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[6, 10, acting, great, good, acting, 4_10, dir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14615</th>\n",
              "      <td>[perhaps, biggest, waste, production, time, mo...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[perhaps, biggest, waste, production, time, mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36865</th>\n",
              "      <td>[hilarious, would, sworn, ed, wood, wrote, ter...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[hilarious, would, sworn, ed_wood, wrote, terr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20865</th>\n",
              "      <td>[unsung, quiet, gem, tell, true, story, pow, e...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[unsung, quiet, gem, tell, true, story, pow, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36511</th>\n",
              "      <td>[spent, many, sleepless, night, watching, 2001...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[spent, many, sleepless_night, watching, 2001,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment  \\\n",
              "33003  [set, paris, year, 1910, retired, old, rich, o...  positive   \n",
              "12172  [basic, structure, story, beginning, middle, e...  negative   \n",
              "5192   [odd, willfully, skewed, biopic, dyan, thomas,...  negative   \n",
              "32511  [okay, penelope, keith, miss, herringbone, twe...  negative   \n",
              "43723  [larger, life, figure, wyatt, earp, bat, maste...  positive   \n",
              "...                                                  ...       ...   \n",
              "16645  [6, 10, acting, great, good, acting, 4, 10, di...  negative   \n",
              "14615  [perhaps, biggest, waste, production, time, mo...  negative   \n",
              "36865  [hilarious, would, sworn, ed, wood, wrote, ter...  negative   \n",
              "20865  [unsung, quiet, gem, tell, true, story, pow, e...  positive   \n",
              "36511  [spent, many, sleepless, night, watching, 2001...  positive   \n",
              "\n",
              "                                            clean_review  \n",
              "33003  [set, paris, retired, old, rich, opera_singer,...  \n",
              "12172  [basic, structure, story, beginning_middle, en...  \n",
              "5192   [odd, willfully, skewed, biopic, dyan, thomas,...  \n",
              "32511  [okay, miss, b, b, e, backbone, england, kille...  \n",
              "43723  [larger_life, figure, wyatt_earp, bat, masters...  \n",
              "...                                                  ...  \n",
              "16645  [6, 10, acting, great, good, acting, 4_10, dir...  \n",
              "14615  [perhaps, biggest, waste, production, time, mo...  \n",
              "36865  [hilarious, would, sworn, ed_wood, wrote, terr...  \n",
              "20865  [unsung, quiet, gem, tell, true, story, pow, e...  \n",
              "36511  [spent, many, sleepless_night, watching, 2001,...  \n",
              "\n",
              "[15000 rows x 3 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Veamos que ya tenemos las reviews sin esos tokens eliminados previamente\n",
        "\n",
        "corpus_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sin embargo, los tokens no se pueden meter tal cual a un modelo de predicción sino que deben codificarse a nivel numérico. Para ello usaremos la codificación BoW y la TD-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rISd3VD_-Yi"
      },
      "source": [
        "### *2.3. Bag-Of-Words (BoW)*\n",
        "\n",
        "Next, let us create a numerical version of our corpus using the `doc2bow` method. In general, `D.doc2bow(token_list)` transforms any list of tokens into a list of tuples `(token_id, n)`, one per each token in `token_list`, where `token_id` is the token identifier (according to dictionary `D`) and `n` is the number of occurrences of such a token in `token_list`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con esto conseguimos que cada token de cada review se represente como una tupla donde aparecerá (id del token en el diccionario, número de veces que aparece el token en dicho documento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FJdsQvFbDpht"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= Review (lemmas) =============\n",
            "film really used location well amazing shot dark_disturbing film move_slowly constantly keep watching modern love worked well gold_coast film fantastic program year offering audience glimpse_australian cinema usually_neglected importantly_refreshing see australian_cinema taking cliché_aussie character story line seen done death year film would compliment_festival open_debate screening performance character well_developed cinematography fantastic interesting exploration family relationship environment\n",
            "\n",
            "============= Sparse vector representation =============\n",
            "[(13, 1), (73, 2), (89, 2), (107, 4), (153, 1), (157, 1), (170, 1), (204, 1), (206, 1), (220, 1), (224, 1), (280, 1), (446, 1), (486, 1), (518, 1), (528, 1), (662, 1), (673, 1), (770, 1), (1008, 1), (1057, 2), (1221, 1), (1225, 1), (1245, 1), (1378, 2), (1445, 1), (1553, 1), (2063, 1), (2128, 1), (2552, 1), (2590, 1), (3290, 1), (4182, 1), (4947, 1), (5673, 1), (5735, 1), (8082, 1), (11212, 1), (13478, 1), (15689, 1), (15690, 1), (15691, 1)]\n",
            "\n",
            "============= Word counts for the review =============\n",
            "[('death', 1), ('well', 2), ('character', 2), ('film', 4), ('see', 1), ('story', 1), ('would', 1), ('line', 1), ('love', 1), ('relationship', 1), ('seen', 1), ('done', 1), ('environment', 1), ('modern', 1), ('really', 1), ('shot', 1), ('keep', 1), ('performance', 1), ('family', 1), ('interesting', 1), ('year', 2), ('audience', 1), ('cinema', 1), ('amazing', 1), ('fantastic', 2), ('cinematography', 1), ('taking', 1), ('used', 1), ('watching', 1), ('constantly', 1), ('screening', 1), ('offering', 1), ('location', 1), ('worked', 1), ('move_slowly', 1), ('program', 1), ('well_developed', 1), ('australian_cinema', 1), ('exploration', 1), ('dark_disturbing', 1), ('gold_coast', 1), ('open_debate', 1)]\n"
          ]
        }
      ],
      "source": [
        "reviews_bow = [D.doc2bow(doc) for doc in corpus]\n",
        "\n",
        "n_review = 1000\n",
        "print('============= Review (lemmas) =============')\n",
        "print(' '.join(corpus[n_review]))\n",
        "\n",
        "print('\\n============= Sparse vector representation =============')\n",
        "print(reviews_bow[n_review])\n",
        "\n",
        "print('\\n============= Word counts for the review =============')\n",
        "print(list(map(lambda x: (D[x[0]], x[1]), reviews_bow[n_review])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQZkdCPiEL1A"
      },
      "source": [
        "Note that we can interpret each element of corpus_bow as a `sparse_vector`. For example, a list of tuples \n",
        "\n",
        "    [(0, 1), (3, 3), (5,2)] \n",
        "\n",
        "for a dictionary of 10 elements can be represented as a vector, where any tuple `(id, n)` states that position `id` must take value `n`. The rest of the positions must be zero.\n",
        "\n",
        "    [1, 0, 0, 3, 0, 2, 0, 0, 0, 0]\n",
        "\n",
        "As a summary, we have obtained the following variables that will be relevant for the next sections:\n",
        "\n",
        "   * `D`: A Gensim dictionary. Term strings can be accessed using numeric identifiers. For instance, `D[0]` contains the string corresponding to the first position in the BoW representation.\n",
        "   * `mycorpus_bow`: BoW corpus. A list containing an entry per project in the dataset, and consisting of the (sparse) BoW representation for the abstract of that project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "mycorpus_bow = reviews_bow y tiene, para cada review un resumen de su index en el dict y el nº de veces que aparece en ese documento ern concreto. En eso se resume según BoW la representación numérica que relaciona o da sentido a unos tokens obtenidos tras el preprocesado de una cadena de texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ya tenemos la sparse representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_RZWSiM_-kD"
      },
      "source": [
        "### *2.4. TF-IDF vectorization*\n",
        "\n",
        "Gensim TFIDF representation of a document is computed as\n",
        "\n",
        "$$x_{ij} = \\text{freq}_{ij} \\log_2 \\frac{\\# docs}{\\# docs_j}$$\n",
        "\n",
        "where: \n",
        "\n",
        "   - $x_{ij}$ is the component of the TFIDF representation of document $i$ corresponding to term $j$\n",
        "   - $\\text{freq}_{ij}$ is the frequency of term $j$ in a document $i$ (i.e., number of occurrences divided by the number of tokens)\n",
        "   - $\\# docs$ is the total number of documents in the corpus\n",
        "   - ${\\# docs_j}$ is the number of documents in the corpus containing term $j$\n",
        "\n",
        "In this way, terms that appear in fewer documents get emphasized over common terms appearing in many documents.\n",
        "\n",
        "Gensim offers the possibility to change the *term frequency* and *inverse document frequency* calculation terms, but we will keep the defaults.\n",
        "\n",
        "Note that, contrary to the Bag of Words (BoW) representation, the TFIDF representation does not depend just on the tokens of each document, but gets affected by the whole corpus through the IDF factor.\n",
        "\n",
        "Gensim considers TFIDF as a model on its own and deals with it similarly to what is done with other models. Creating a TFIDF model is very simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con esto no solo tendremos en cuenta el número de aparición del token en el documento sino que también dependerá de todo el corpus (lo que da paso a más información respecto a cada uno de los documentos en comparación con la BoW representation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5FXpTZ1REXqB",
        "outputId": "5a0cd6f5-17df-421f-8f54-cbdaa2108b66"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tfidf = TfidfModel(reviews_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN_o_jSWEeEc"
      },
      "source": [
        "A **TFIDF model cannot be updated** adding more documents. Otherwise, we would lose consistency, i.e., the TFIDF representation for a particular document would change before and after the TFIDF model gets updated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffXsDP8xEhZd"
      },
      "source": [
        "From now on, `tfidf` can be used to convert any vector from the old representation (bow integer counts) to the new one (TFIDF real-valued weights), or to apply a transformation to a whole corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos como cada token de cada documento se sustituye por una tupla (id del token en el diccionario, representación TFIDF de dicho token en dicho documento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "G_Hwm8PfEkU8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= TFIDF representation for the project =============\n",
            "[(13, 0.10303186556063204), (73, 0.09299293171819176), (89, 0.0839552297866133), (107, 0.0788119729120782), (153, 0.04035973106414201), (157, 0.041908418884884106), (170, 0.03624598163546388), (204, 0.08344137508321578), (206, 0.0644902683898919), (220, 0.11629665955805626), (224, 0.06402022569086893), (280, 0.08830442967660285), (446, 0.18512567038139738), (486, 0.13188020665640368), (518, 0.04396272965195787), (528, 0.08866615013810306), (662, 0.09615821248254511), (673, 0.07362318078192463), (770, 0.09268434362670872), (1008, 0.08185496531201722), (1057, 0.16501900817670423), (1221, 0.09091586355696611), (1225, 0.11635671875384479), (1245, 0.11494211554395115), (1378, 0.26872714851940493), (1445, 0.12577848299554983), (1553, 0.12737282268051714), (2063, 0.09759670721778727), (2128, 0.07371556188177023), (2552, 0.152782283160089), (2590, 0.1898428984287813), (3290, 0.19124380849349917), (4182, 0.14291086694400915), (4947, 0.13713659790322516), (5673, 0.2646336856308083), (5735, 0.17657178710292698), (8082, 0.20863411926350509), (11212, 0.26844622512814276), (13478, 0.20941233543234947), (15689, 0.2831182465187149), (15690, 0.29779026790928714), (15691, 0.29779026790928714)]\n",
            "\n",
            "============= TFIDF applying the transformation only to the document =============\n",
            "[(13, 0.10303186556063204), (73, 0.09299293171819176), (89, 0.0839552297866133), (107, 0.0788119729120782), (153, 0.04035973106414201), (157, 0.041908418884884106), (170, 0.03624598163546388), (204, 0.08344137508321578), (206, 0.0644902683898919), (220, 0.11629665955805626), (224, 0.06402022569086893), (280, 0.08830442967660285), (446, 0.18512567038139738), (486, 0.13188020665640368), (518, 0.04396272965195787), (528, 0.08866615013810306), (662, 0.09615821248254511), (673, 0.07362318078192463), (770, 0.09268434362670872), (1008, 0.08185496531201722), (1057, 0.16501900817670423), (1221, 0.09091586355696611), (1225, 0.11635671875384479), (1245, 0.11494211554395115), (1378, 0.26872714851940493), (1445, 0.12577848299554983), (1553, 0.12737282268051714), (2063, 0.09759670721778727), (2128, 0.07371556188177023), (2552, 0.152782283160089), (2590, 0.1898428984287813), (3290, 0.19124380849349917), (4182, 0.14291086694400915), (4947, 0.13713659790322516), (5673, 0.2646336856308083), (5735, 0.17657178710292698), (8082, 0.20863411926350509), (11212, 0.26844622512814276), (13478, 0.20941233543234947), (15689, 0.2831182465187149), (15690, 0.29779026790928714), (15691, 0.29779026790928714)]\n"
          ]
        }
      ],
      "source": [
        "reviews_tfidf = tfidf[reviews_bow]\n",
        "n_project = 1000\n",
        "print('============= TFIDF representation for the project =============')\n",
        "print(reviews_tfidf[n_review])\n",
        "\n",
        "print('\\n============= TFIDF applying the transformation only to the document =============')\n",
        "print(tfidf[reviews_bow[n_review]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WX-jNJpEwLS"
      },
      "source": [
        "As for BOW, TFIDF provides a sparse document representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H87nLNNLE8b6"
      },
      "source": [
        "### *2.5. Memory efficient computation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimización a nivel computacional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDw_YwGsGXxV"
      },
      "source": [
        "In the previous examples, the construction of the dictionary and the transformation of the corpus to BoW or TFIDF format required that said corpus of documents be available in a list in the execution environment, and therefore required it to be stored in RAM. For a small corpus, this is not a problem, but it can be an important limitation when dealing with a large corpus with millions or tens of millions of documents. These corpora are becoming more and more common in certain applications (consider Wikipedia entries, user opinions on large e-commerce platforms, processing of medical records, etc.).\n",
        "\n",
        "One of the advantages of Gensim is that its implementation makes it easy to work with a corpus of these sizes. As explained in the Gensim documentation:\n",
        "\n",
        "> Note that the corpus above resides fully in memory, as a plain Python list. In this simple example, it doesn’t matter much, but just to make things clear, let’s assume there are millions of documents in the corpus. Storing all of them in RAM won’t do. Instead, let’s assume the documents are stored in a file on disk, one document per line. Gensim only requires that a corpus must be able to return one document vector at a time.\n",
        "\n",
        ">The full power of Gensim comes from the fact that a corpus doesn’t have to be a list, a NumPy array, a Pandas dataframe, or whatever. Gensim accepts any object that, when iterated over, successively yields documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhqeH_cTGaUO"
      },
      "source": [
        "The next fragment of code illustrates how the dictionary can be created from a corpus stored in a text file. You just need to create an iterator that returns a document at each iteration and keeps adding documents to the dictionary. Note that during the execution of the code, only one document is kept in memory at every iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vpbGsYThFPs1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class IterableCorpus_fromfile:\n",
        "    def __init__(self, filename):\n",
        "        self.__filename = filename\n",
        "    def __iter__(self):\n",
        "        for line in open(self.__filename):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield line.lower()\n",
        "\n",
        "MyIterCorpus = IterableCorpus_fromfile('imdb_lemmas_clean.txt')\n",
        "newD = Dictionary()\n",
        "for doc in MyIterCorpus:\n",
        "  newD.add_documents([doc.split()])\n",
        "no_below = 4 # Minimum number of documents to keep a term in the dictionary\n",
        "no_above = .80 # Maximum proportion of documents in which a term can appear to be kept in the dictionary\n",
        "newD.filter_extremes(no_below=no_below,no_above=no_above)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKIzbxUXFU4P"
      },
      "source": [
        "The code above can be further simplified if the iterator already carries out the tokenization of each document. In that case, the dictionary can be created with a simple command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rPFQok6ME8Am"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents processed: 15000\n",
            "Number of elements in dictionary: 29779\n"
          ]
        }
      ],
      "source": [
        "class IterableCorpus_fromfile:\n",
        "    def __init__(self, filename):\n",
        "        self.__filename = filename\n",
        "    def __iter__(self):\n",
        "        for line in open(self.__filename):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield line.lower().split()\n",
        "\n",
        "MyIterCorpus = IterableCorpus_fromfile('imdb_lemmas_clean.txt')\n",
        "newD = Dictionary(MyIterCorpus)\n",
        "no_below = 4 # Minimum number of documents to keep a term in the dictionary\n",
        "no_above = .80 # Maximum proportion of documents in which a term can appear to be kept in the dictionary\n",
        "newD.filter_extremes(no_below=no_below,no_above=no_above)\n",
        "\n",
        "print('Number of documents processed:', newD.num_docs)\n",
        "print('Number of elements in dictionary:', len(newD))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI5R6rRtGIEA"
      },
      "source": [
        "### *2.7. Compatibility with Numpy and Scipy*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL6Ztp3ZGT99"
      },
      "source": [
        "Gensim contains efficient functions to convert Gensim Corpus (BoW, TFIDF) to Numpy dense matrices or Scipy Sparse Matrices. This can be useful, e.g., if we wish to use the vectorial representation of a Gensim corpus to train a classification or regression model using sklearn.\n",
        "\n",
        "Similarly, we also have functions to convert Numpy or Scipy matrices into Gensim representation.\n",
        "\n",
        "More information on the available utilities can be found in the [Gensim API matutils documentation](https://radimrehurek.com/gensim/matutils.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgLdeQAhG4Y9"
      },
      "source": [
        "Sklearn also includes functions for tokenization and vectorization of documents. Specifically, it has the functions:\n",
        "* [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) which implements both tokenization and word count (BoW) in a single class.\n",
        "* [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) which is responsible for obtaining the TF-IDF representation from a BoW representation.\n",
        "\n",
        "* [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) which is equivalent to using `CountVectorizer()` followed by `TfidfTransformer()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A partir de este punto realizaremos el modelo capaz de predecir, a través de los tokens que conforman un documento, si el sentimiento al respecto de la review es bueno o malo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoWITECrGhus"
      },
      "source": [
        "## **3. Sentiment Analysis with BoW and TF-IDF representations**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnFHjJ6qbZuc"
      },
      "source": [
        "Let's start by loading the problems labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A su vez los sentimientos positivos o negativos de cada review (las etiquetas), las pasamos a binario, respectivamente \"1\" y \"0\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wcbywHiDbdu-",
        "outputId": "3c681a18-c386-4e93-c29c-725921e6ba24"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 0 0 ... 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "def get_binary_label(sentiment):\n",
        "  return 1 if sentiment == \"positive\" else 0\n",
        "\n",
        "corpus_df['binary_sentiment'] = corpus_df['sentiment'].apply(get_binary_label)\n",
        "\n",
        "Y = corpus_df['binary_sentiment'].values\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>clean_review</th>\n",
              "      <th>binary_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33003</th>\n",
              "      <td>[set, paris, year, 1910, retired, old, rich, o...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[set, paris, retired, old, rich, opera_singer,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12172</th>\n",
              "      <td>[basic, structure, story, beginning, middle, e...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[basic, structure, story, beginning_middle, en...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5192</th>\n",
              "      <td>[odd, willfully, skewed, biopic, dyan, thomas,...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[odd, willfully, skewed, biopic, dyan, thomas,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32511</th>\n",
              "      <td>[okay, penelope, keith, miss, herringbone, twe...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[okay, miss, b, b, e, backbone, england, kille...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43723</th>\n",
              "      <td>[larger, life, figure, wyatt, earp, bat, maste...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[larger_life, figure, wyatt_earp, bat, masters...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16645</th>\n",
              "      <td>[6, 10, acting, great, good, acting, 4, 10, di...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[6, 10, acting, great, good, acting, 4_10, dir...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14615</th>\n",
              "      <td>[perhaps, biggest, waste, production, time, mo...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[perhaps, biggest, waste, production, time, mo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36865</th>\n",
              "      <td>[hilarious, would, sworn, ed, wood, wrote, ter...</td>\n",
              "      <td>negative</td>\n",
              "      <td>[hilarious, would, sworn, ed_wood, wrote, terr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20865</th>\n",
              "      <td>[unsung, quiet, gem, tell, true, story, pow, e...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[unsung, quiet, gem, tell, true, story, pow, e...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36511</th>\n",
              "      <td>[spent, many, sleepless, night, watching, 2001...</td>\n",
              "      <td>positive</td>\n",
              "      <td>[spent, many, sleepless_night, watching, 2001,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment  \\\n",
              "33003  [set, paris, year, 1910, retired, old, rich, o...  positive   \n",
              "12172  [basic, structure, story, beginning, middle, e...  negative   \n",
              "5192   [odd, willfully, skewed, biopic, dyan, thomas,...  negative   \n",
              "32511  [okay, penelope, keith, miss, herringbone, twe...  negative   \n",
              "43723  [larger, life, figure, wyatt, earp, bat, maste...  positive   \n",
              "...                                                  ...       ...   \n",
              "16645  [6, 10, acting, great, good, acting, 4, 10, di...  negative   \n",
              "14615  [perhaps, biggest, waste, production, time, mo...  negative   \n",
              "36865  [hilarious, would, sworn, ed, wood, wrote, ter...  negative   \n",
              "20865  [unsung, quiet, gem, tell, true, story, pow, e...  positive   \n",
              "36511  [spent, many, sleepless, night, watching, 2001...  positive   \n",
              "\n",
              "                                            clean_review  binary_sentiment  \n",
              "33003  [set, paris, retired, old, rich, opera_singer,...                 1  \n",
              "12172  [basic, structure, story, beginning_middle, en...                 0  \n",
              "5192   [odd, willfully, skewed, biopic, dyan, thomas,...                 0  \n",
              "32511  [okay, miss, b, b, e, backbone, england, kille...                 0  \n",
              "43723  [larger_life, figure, wyatt_earp, bat, masters...                 1  \n",
              "...                                                  ...               ...  \n",
              "16645  [6, 10, acting, great, good, acting, 4_10, dir...                 0  \n",
              "14615  [perhaps, biggest, waste, production, time, mo...                 0  \n",
              "36865  [hilarious, would, sworn, ed_wood, wrote, terr...                 0  \n",
              "20865  [unsung, quiet, gem, tell, true, story, pow, e...                 1  \n",
              "36511  [spent, many, sleepless_night, watching, 2001,...                 1  \n",
              "\n",
              "[15000 rows x 4 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Veamos que tenemos tanto los tokens limpios asociados a su clasificación binaria\n",
        "\n",
        "corpus_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNCsJPzsj9mc"
      },
      "source": [
        "And save all the changes we have made in ``corpus_df`` for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "8rWYfb-r_JXH"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "def pickler(file: str, ob):\n",
        "    \"\"\"Pickle object to file\"\"\"\n",
        "    with open(file, 'wb') as f:\n",
        "        pickle.dump(ob, f)\n",
        "    return 0\n",
        "\n",
        "pickler(\"corpus_df_imbdb.pickle\",corpus_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXBeXFgYcTeE"
      },
      "source": [
        "Since we have carried out the vectorization with Gensim,  we have to convert our vector representation into NumPy arrays so we can use Sklearn's classifiers. To do this, Gensim includes two functions: [corpus2dense](https://tedboy.github.io/nlps/generated/generated/gensim.matutils.corpus2dense.html), [corpus2csc](https://tedboy.github.io/nlps/generated/generated/gensim.matutils.corpus2csc.html). In general, when dealing with huge corpora, we will be interested in managing the sparse form of the data to save on computational costs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "vGrhOlDIc7oc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Obtenemos tanto de la representación BoW como TFIDF, su forma dense/sparse en array de numpy (donde cada posición es una lista con los valores de cada token en dicho documento) (es decir, tendremos 15k muestras con 20 y pico k atributos asociados al valor de cada token en el diccionario)\n",
        "# para dicho documento en BoW (número de veces que aparece en el documento) o TFIDF (valor TF en dicho documento)\n",
        "\n",
        "from gensim.matutils import corpus2dense, corpus2csc\n",
        "\n",
        "n_tokens = len(D)\n",
        "num_docs = len(reviews_bow)\n",
        "\n",
        "# Convert BoW representacion\n",
        "corpus_bow_dense = corpus2dense(reviews_bow, num_terms=n_tokens, num_docs=num_docs).T\n",
        "corpus_bow_sparse = corpus2csc(reviews_bow, num_terms=n_tokens, num_docs=num_docs).T\n",
        "\n",
        "# Convert TFIDF representacion\n",
        "corpus_tfidf_dense = corpus2dense(reviews_tfidf, num_terms=n_tokens, num_docs=num_docs).T\n",
        "corpus_tfidf_sparse = corpus2csc(reviews_tfidf, num_terms=n_tokens, num_docs=num_docs).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDkvoVRsmdak"
      },
      "source": [
        "##### **Exercise 9**\n",
        "\n",
        "Train an SVM classifier with the BoW representation of the IMDB dataset. Use the Sklearn function ``train_test_split`` to split the BOW representation of the reviews with a $70/30$ ratio each and a random state of $42$. Find the best hyperparameters for the SVM (``C`` and ``kernel``) via cross-validation with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). Evaluate the performance of the classifier based on the [R2 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos el SVM clasiffier para la representación BoW de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Obtenemos conjunto de entrenamiento y test en una relación 70/30\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus_bow_sparse, Y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El accuracy usando SVM en el dataset BoW sin analizar los mejores parámetros es de: 0.8675555555555555\n",
            "\n",
            "La respectiva matriz de confusión: \n",
            "[[1915  349]\n",
            " [ 247 1989]]\n"
          ]
        }
      ],
      "source": [
        "# Analizamos el SVM con el dataset BoW dejando los parámetros por defecto\n",
        "\n",
        "clf_SVM = SVC()\n",
        "BoW_SVM = clf_SVM.fit(X_train,y_train)\n",
        "y_pred = BoW_SVM.predict(X_test)\n",
        "print('El accuracy usando SVM en el dataset BoW sin analizar los mejores parámetros es de: ' + str(accuracy_score(y_test, y_pred)))\n",
        "print('')\n",
        "print('La respectiva matriz de confusión: ')\n",
        "print(confusion_matrix(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El mejor estimador a usar: SVC(C=10.0)\n",
            "El mejor parámetro a usar: {'C': 10.0, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "# Realizamos el estudio paramétrico de los hiperparámetros del SVM\n",
        "\n",
        "param_grid = {\n",
        "    \"C\": [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0],\n",
        "    \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "    }\n",
        "\n",
        "grid_cv = GridSearchCV(SVC(), param_grid, cv = 2, n_jobs = -1)\n",
        "grid_cv.fit(X_train,y_train)\n",
        "print('El mejor estimador a usar: ' + str(grid_cv.best_estimator_))\n",
        "print('El mejor parámetro a usar: ' + str(grid_cv.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El accuracy usando SVM en el dataset BoW con los mejores parámetros es de: 0.8837777777777778\n",
            "\n",
            "La respectiva matriz de confusión: \n",
            "[[1973  291]\n",
            " [ 232 2004]]\n"
          ]
        }
      ],
      "source": [
        "# Creamos el modelo con los mejores hiperparámetros\n",
        "\n",
        "clf_SVM_BoW = SVC(C=10.0, kernel = 'rbf')\n",
        "clf_SVM_BoW.fit(X_train,y_train)\n",
        "y_pred = clf_SVM_BoW.predict(X_test)\n",
        "print('El accuracy usando SVM en el dataset BoW con los mejores parámetros es de: ' + str(accuracy_score(y_test, y_pred)))\n",
        "print('')\n",
        "print('La respectiva matriz de confusión: ')\n",
        "print(confusion_matrix(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos una cierta mejora en el accuracy al utilizar hiperparámetros algo más óptimos. Veamos los resultados usando la representación TF-IDF de los token en los documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RN0zvaZMqZL"
      },
      "source": [
        "##### **Exercise 10**\n",
        "\n",
        "Mimic the steps from Exercise 9 to train an SVM classifier with the TF-IDF representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos el SVM clasiffier para la representación TF-IDF de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Obtenemos conjunto de entrenamiento y test en una relación 70/30\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus_tfidf_sparse, Y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "qwZn_P8rdLOk"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El accuracy usando SVM en el dataset TFIDF sin analizar los mejores parámetros es de: 0.9017777777777778\n",
            "\n",
            "La respectiva matriz de confusión: \n",
            "[[2018  246]\n",
            " [ 196 2040]]\n"
          ]
        }
      ],
      "source": [
        "# Analizamos el SVM con el dataset TD-IDF dejando los parámetros por defecto\n",
        "\n",
        "clf_SVM = SVC()\n",
        "TFIDf_SVM = clf_SVM.fit(X_train,y_train)\n",
        "y_pred = TFIDf_SVM.predict(X_test)\n",
        "print('El accuracy usando SVM en el dataset TFIDF sin analizar los mejores parámetros es de: ' + str(accuracy_score(y_test, y_pred)))\n",
        "print('')\n",
        "print('La respectiva matriz de confusión: ')\n",
        "print(confusion_matrix(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ya de base supera l mejor modelo usando la representación BoW de los datos. Es lógico ya que tal y como vimos en teoría, la representación TF-IDF proporciona más información de la frase al referirse también al resto del corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El mejor estimador a usar: SVC(C=3.0)\n",
            "El mejor parámetro a usar: {'C': 3.0, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "# Realizamos el estudio paramétrico de los hiperparámetros del SVM\n",
        "\n",
        "param_grid = {\n",
        "    \"C\": [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0],\n",
        "    \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "    }\n",
        "\n",
        "grid_cv = GridSearchCV(SVC(), param_grid, cv = 2, n_jobs = -1)\n",
        "grid_cv.fit(X_train,y_train)\n",
        "print('El mejor estimador a usar: ' + str(grid_cv.best_estimator_))\n",
        "print('El mejor parámetro a usar: ' + str(grid_cv.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El accuracy usando SVM en el dataset TFIDF usando los mejores parámetros es de: 0.9051111111111111\n",
            "\n",
            "La respectiva matriz de confusión: \n",
            "[[2036  228]\n",
            " [ 199 2037]]\n"
          ]
        }
      ],
      "source": [
        "# Creamos el modelo con los mejores hiperparámetros\n",
        "\n",
        "clf_SVM_TFIDF = SVC(C=3.0, kernel = 'rbf')\n",
        "clf_SVM_TFIDF.fit(X_train,y_train)\n",
        "y_pred = clf_SVM_TFIDF.predict(X_test)\n",
        "print('El accuracy usando SVM en el dataset TFIDF usando los mejores parámetros es de: ' + str(accuracy_score(y_test, y_pred)))\n",
        "print('')\n",
        "print('La respectiva matriz de confusión: ')\n",
        "print(confusion_matrix(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si bien hemos aprendido cada uno de los pasos necesarios para un buen preprocesado de datos en forma de strings, vemos como las representaciónes BoW y TFIDF de los tokens para un problema de análisis dfe sentimientos pueden dar resultados bastante buenos aproximando a un accuracy de un 88.3% y 90.5% respectivamente.\n",
        "\n",
        "Los resultados son normales dada la naturaleza del problema. En el análisis de sentimientos no necesitamos una profundización exhaustiva en la correlación y orden de las palabras sino que existen muchas palabras que se relacionan directamente con un sentimiento bueno o malo por parte del escritor de la \"review\". Si bien BoW o TFIDF no dan excesiva información del segmento de texto, si plasman bien la frecuencia de aparición de las palabras en el mismo, donde, según la cantidad de veces que aparecen ciertas palabras (directamente ligadas a un sentimiento) en un texto, puede predecir con bastante exactitud el sentimiento/etiqueta asociada al mismo.\n",
        "\n",
        "Finalmente, cabe destacar que no contamos con un desbalanceo en las clases positivo/negativo y es por ello que el modelo, aparte de predecir bien en general, tiene muy buena capacidad de predicción para ambas clases (podemos ver esto en las diagonales de las matrices de confusión las cuales se relacionan directamente con las etiquetas clasificadas correctamente para cada clase)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LriaFb8wbEyP"
      },
      "source": [
        "---\n",
        "In this first laboratory, we have covered the necessary preprocessing steps that need to be applied to a text corpus previous to its vectorization using several state-of-the-art Python libraries. We have then seen how to obtain BoW and TFIDF representations based on the Gensim library and how to use them for a Sentiment Analysis problem. \n",
        "\n",
        "While we will see that Bag-of-Words and TF-IDF as they neither capture the context of words nor allow for similarity comparison, it is still important to know how they work and how to use them, since they still provide quite good results in some tasks, as we have seen in this notebook. \n",
        "\n",
        "To finish, it is also important that you get confident with the Genism library as you will be using it a lot in this course!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ids')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6 | packaged by conda-forge | (main, Oct 24 2022, 16:02:16) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "4f4b3e375183431ea8a402488496d3aca3ba53f2cb8b44eeb3f5fe53d953a410"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
